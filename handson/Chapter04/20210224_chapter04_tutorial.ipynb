{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210224_chapter04_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1gVSlbXkpLB"
      },
      "source": [
        "%%HTML\n",
        "<style>\n",
        "    div#notebook-container    { width: 100%; }\n",
        "    div#menubar-container     { width: 65%; }\n",
        "    div#maintoolbar-container { width: 99%; }\n",
        "</style>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BDU71sDkpLM"
      },
      "source": [
        "# Google Colab環境ではGoogle Driveをマウントしてアクセスできるようにします。\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Google Drive をマウントします\n",
        "    from google.colab import drive\n",
        "    mount_dir = \"/content/drive\"\n",
        "    drive.mount(mount_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd2x9UT-kpLO"
      },
      "source": [
        "# 「ファンダメンタルズ分析チャレンジ」で作成したモデルを使用してポートフォリオを構築しよう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj0bpDsYkpLO"
      },
      "source": [
        "```\n",
        "ここからはChapter02で作成したモデルをベースにポートフォリオを組成して、バックテストで評価します。\n",
        "\n",
        "本セクションを完了すると以下のことを習得可能です。\n",
        "- 本コンペティション用の特徴量の生成方法\n",
        "- Chapter02で作成したモデルからの出力を組み合わせた銘柄の選択方法\n",
        "\n",
        "以下のステップで進めていきます。\n",
        "\n",
        "\n",
        "[source]\n",
        "----\n",
        "1. 環境設定\n",
        "2. ライブラリのロード\n",
        "3. データセットの読み込み\n",
        "4. 2章で作成したモデルの配置\n",
        "5. 2章のpredictor.pyの変更\n",
        "  5.1 get_inputsの変更 (stock_fin_priceを読み込む)\n",
        "  5.2 get_datasetの変更 (必要なデータのみ読み込む)\n",
        "  5.3 get_codesの変更 (ユニバースの変更)\n",
        "  5.4 get_features_for_predict の変更 (stock_fin_priceを使用する)\n",
        "  5.5 predictの変更 (銘柄選択、出力を変更)\n",
        "6. バックテストの実行\n",
        "7. バックテストの評価\n",
        "8. 適時開示情報を使用して特別損失銘柄を除外\n",
        "9. パッケージの作成\n",
        "----\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSy1FLF9do1T"
      },
      "source": [
        "## 1. 環境設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faRabxGtdo1T"
      },
      "source": [
        "```\n",
        "　基本的に4章の環境設定方法は、3章に準拠しています。本章のnotebookはlink:https://github.com/JapanExchangeGroup/J-Quants-Tutorial/tree/main/handson/Chapter04/20210224-chapter04-tutorial.ipynb[こちら]からダウンロードしてご利用ください。なお、2章のnotebookを実行した際に最後に保存されるモデルを配置したディレクトリを `ScoringService.get_model` のパラメーターとして設定する必要があります。本チュートリアルの2章をそのまま実行した際の学習済みのモデルは、link:https://signate.jp/competitions/443/data[こちら]からダウンロード可能です。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTFTO2SwkpLO"
      },
      "source": [
        "## 2. ライブラリのロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tso_MBcddo1U"
      },
      "source": [
        "　ランタイム環境とGoogle Colaboratory環境の両者で共通のライブラリを使用するためにバージョンを調整します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm12LzmfkpLP"
      },
      "source": [
        "# ライブラリのバージョンを調整します\n",
        "!pip install --no-cache-dir joblib==1.0.1 numpy==1.19.5 pandas==1.1.5 scikit-learn==0.20.3 scipy==1.2.1 seaborn==0.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgOsUdckpLP"
      },
      "source": [
        "　次に、以下のライブラリを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF550hj5kpLP"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qH0W09fkpLQ"
      },
      "source": [
        "```\n",
        "　次に、本コンペティションの評価検証用のバックテストモジュールを読み込みます。バックテストモジュールの使用方法については「3.6. バックテスト環境の構築」をご参照ください。インポート時にエラーが出た場合は、sys.path に backtest.py ファイルを配置したディレクトリを追加してから再度インポートしてください。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH6Ti3fukpLQ"
      },
      "source": [
        "# インポート時にエラーが出た場合は、以下のmodule_dirをbacktest.pyを配置したディレクトリに変更してください。\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  # Backtestを配置したディレクトリへのフルパスを指定します\n",
        "  module_dir = f\"{mount_dir}/MyDrive/JPX_competition/Chapter03/backtest\"\n",
        "else:\n",
        "  # Backtestを配置したディレクトリへのフルパスを指定します\n",
        "  module_dir = \"/notebook/Chapter03/backtest\" \n",
        "sys.path.append(module_dir)\n",
        "\n",
        "from backtest import Backtest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kebZtb8kpLQ"
      },
      "source": [
        "　Pandasのデータを表示する際に省略されないように設定を変更します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLQXo9t9kpLQ"
      },
      "source": [
        "# 表示用の設定を変更します\n",
        "%matplotlib inline\n",
        "pd.options.display.max_rows = 100\n",
        "pd.options.display.max_columns = 100\n",
        "pd.options.display.width = 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCAorgO_kpLR"
      },
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVYrTWGSkpLR"
      },
      "source": [
        "## 3. データセットの読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKdq2hajkpLR"
      },
      "source": [
        "```\n",
        "　データセットを配置したディレクトリのパスを設定します。Google Colabをご使用の場合は Google Drive にデータセットをアップロードし、そのディレクトリを指定してください。なお、データセットの取得方法および内容については「3.4. データセットの説明」をご参照ください。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1vZvY8dkpLR"
      },
      "source": [
        "# データセットを配置したディレクトリのパスを設定\n",
        "if 'google.colab' in sys.modules:\n",
        "    dataset_dir = f\"{mount_dir}/MyDrive/JPX_competition/data_dir_comp2\"\n",
        "else:\n",
        "    dataset_dir = \"/notebook/data_dir_comp2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsaqyp8TkpLS"
      },
      "source": [
        "```\n",
        "　本コンペティションのランタイム環境におけるデータセットへのアクセスは、 `ScoringService.predict()` メソッドに渡されるinputsパラメーターを通して行う必要があります。そのため、以下のように本notebook環境でもランタイム環境と共通の方法でデータセットにアクセスすることで、コードが複雑になったり投稿用にコードを編集したりしなくても済むようにしています。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwUit8eLkpLS"
      },
      "source": [
        "# 入力パラメーターを設定します。ランタイム環境での実行時と同一フォーマットにします\n",
        "inputs = {\n",
        "    \"stock_list\": f\"{dataset_dir}/stock_list.csv.gz\",\n",
        "    \"stock_price\": f\"{dataset_dir}/stock_price.csv.gz\",\n",
        "    \"stock_fin\": f\"{dataset_dir}/stock_fin.csv.gz\",\n",
        "    \"stock_fin_price\": f\"{dataset_dir}/stock_fin_price.csv.gz\",\n",
        "    # ニュースデータ\n",
        "    \"tdnet\": f\"{dataset_dir}/tdnet.csv.gz\",\n",
        "    \"disclosureItems\": f\"{dataset_dir}/disclosureItems.csv.gz\",\n",
        "    \"nikkei_article\": f\"{dataset_dir}/nikkei_article.csv.gz\",\n",
        "    \"article\": f\"{dataset_dir}/article.csv.gz\",\n",
        "    \"industry\": f\"{dataset_dir}/industry.csv.gz\",\n",
        "    \"industry2\": f\"{dataset_dir}/industry2.csv.gz\",\n",
        "    \"region\": f\"{dataset_dir}/region.csv.gz\",\n",
        "    \"theme\": f\"{dataset_dir}/theme.csv.gz\",\n",
        "    # 目的変数データ\n",
        "    \"stock_labels\": f\"{dataset_dir}/stock_labels.csv.gz\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJUcKe--kpLS"
      },
      "source": [
        "## 3. 2章で作成したモデルの配置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZUBgxLkpLT"
      },
      "source": [
        "```\n",
        "　2章で作成したモデルである `my_label_high_20.pkl` と `my_label_low_20.pkl` をarchive/modelディレクトリに配置します。これらのモデルはlink:https://signate.jp/competitions/443/data[こちら]からダウンロードすることができます。次に、これらのモデルを保存したディレクトリへのパスを `ScoringService.get_model` メソッドへのパラメーターとして入力してください。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUkfBrfHkpLT"
      },
      "source": [
        "　モデルを配置したディレクトリを変数に設定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_n9hSFo3aI"
      },
      "source": [
        "# モデル配置ディレクトリのパスを設定\n",
        "if 'google.colab' in sys.modules:\n",
        "    model_dir = f\"{mount_dir}/MyDrive/JPX_competition/Chapter04/archive/model\"\n",
        "else:\n",
        "    model_dir = \"archive/model\"\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPoea1Mtdo1Y"
      },
      "source": [
        "モデルが存在することを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AXxUF8pkpLU"
      },
      "source": [
        "!ls -lh $model_dir/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkn0JHzkpLU"
      },
      "source": [
        "## 5. 2章のpredictor.pyの変更"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFbbBcHokpLU"
      },
      "source": [
        "```\n",
        "　ここからは2章で作成した `predictor.py` をベースに変更していきます。なお、2章の `predictor.py` はlink:https://raw.githubusercontent.com/JapanExchangeGroup/J-Quants-Tutorial/main/handson/Chapter02/archive/src/predictor.py[こちら]から取得可能です。\n",
        "\n",
        "　ここでは、predictor.pyに以下の変更を実施します。ある程度規模の大きな変更となりますが、ポートフォリオを組成するために必要な変更です。\n",
        "\n",
        " - get_datasetの変更 (必要なデータのみ読み込む)\n",
        " - get_codesの変更 (ユニバースの変更)\n",
        " - get_features_for_predict の変更 (stock_fin_priceを使用する)\n",
        " - get_exclude の追加 (適時開示情報を使用した銘柄選択)\n",
        " - strategy の追加 (銘柄選択)\n",
        " - predict の変更 (銘柄選択、出力を変更)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmr8qJhrdo1Z"
      },
      "source": [
        " 編集結果は以下になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb71_mfMkpLU"
      },
      "source": [
        "class ScoringService(object):\n",
        "    # テスト期間開始日\n",
        "    TEST_START = \"2021-02-01\"\n",
        "    # 目的変数\n",
        "    TARGET_LABELS = [\"label_high_20\", \"label_low_20\"]\n",
        "    # データをこの変数に読み込む\n",
        "    dfs = None\n",
        "    # モデルをこの変数に読み込む\n",
        "    models = None\n",
        "    # 対象の銘柄コードをこの変数に読み込む\n",
        "    codes = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_dataset(cls, inputs, load_data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (list[str]): path to dataset files\n",
        "            load_data (list[str]): specify loading data\n",
        "        Returns:\n",
        "            dict[pd.DataFrame]: loaded data\n",
        "        \"\"\"\n",
        "        if cls.dfs is None:\n",
        "            cls.dfs = {}\n",
        "        for k, v in inputs.items():\n",
        "            # 必要なデータのみ読み込みます\n",
        "            if k not in load_data:\n",
        "                continue\n",
        "            cls.dfs[k] = pd.read_csv(v)\n",
        "            # DataFrameのindexを設定します。\n",
        "            if k == \"stock_price\":\n",
        "                cls.dfs[k].loc[:, \"datetime\"] = pd.to_datetime(\n",
        "                    cls.dfs[k].loc[:, \"EndOfDayQuote Date\"]\n",
        "                )\n",
        "                cls.dfs[k].set_index(\"datetime\", inplace=True)\n",
        "            elif k in [\"stock_fin\", \"stock_fin_price\", \"stock_labels\"]:\n",
        "                cls.dfs[k].loc[:, \"datetime\"] = pd.to_datetime(\n",
        "                    cls.dfs[k].loc[:, \"base_date\"]\n",
        "                )\n",
        "                cls.dfs[k].set_index(\"datetime\", inplace=True)\n",
        "        return cls.dfs\n",
        "\n",
        "    @classmethod\n",
        "    def get_codes(cls, dfs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dfs (dict[pd.DataFrame]): loaded data\n",
        "        Returns:\n",
        "            array: list of stock codes\n",
        "        \"\"\"\n",
        "        stock_list = dfs[\"stock_list\"].copy()\n",
        "        # 予測対象の銘柄コードを取得\n",
        "        cls.codes = stock_list[stock_list[\"universe_comp2\"] == True][\n",
        "            \"Local Code\"\n",
        "        ].values\n",
        "        return cls.codes\n",
        "\n",
        "    @classmethod\n",
        "    def get_features_for_predict2(cls, dfs, code, fin_columns, start_dt=TEST_START):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dfs (dict)  : dict of pd.DataFrame include stock_fin, stock_price\n",
        "            code (int)  : A local code for a listed company\n",
        "            fin_columns(list[str]): list of columns\n",
        "            start_dt (str): specify date range\n",
        "        Returns:\n",
        "            feature DataFrame (pd.DataFrame)\n",
        "        \"\"\"\n",
        "        # stock_fin_priceデータを読み込み\n",
        "        stock_fin_price = dfs[\"stock_fin_price\"]\n",
        "\n",
        "        # 特定の銘柄コードのデータに絞る\n",
        "        feats = stock_fin_price[stock_fin_price[\"Local Code\"] == code]\n",
        "        # 2章で作成したモデルの特徴量では過去60営業日のデータを使用しているため、\n",
        "        # 予測対象日からバッファ含めて土日を除く過去90日遡った時点から特徴量を生成します。\n",
        "        n = 90\n",
        "        # 特徴量の生成対象期間を指定\n",
        "        feats = feats.loc[pd.Timestamp(start_dt) - pd.offsets.BDay(n) :]\n",
        "        # 指定されたカラムおよびExchangeOfficialCloseに絞り込み\n",
        "        feats = feats.loc[\n",
        "            :, fin_columns + [\"EndOfDayQuote ExchangeOfficialClose\"]\n",
        "        ].copy()\n",
        "        # 欠損値処理\n",
        "        feats = feats.fillna(0)\n",
        "\n",
        "        # 終値の20営業日リターン\n",
        "        feats[\"return_1month\"] = feats[\n",
        "            \"EndOfDayQuote ExchangeOfficialClose\"\n",
        "        ].pct_change(20)\n",
        "        # 終値の40営業日リターン\n",
        "        feats[\"return_2month\"] = feats[\n",
        "            \"EndOfDayQuote ExchangeOfficialClose\"\n",
        "        ].pct_change(40)\n",
        "        # 終値の60営業日リターン\n",
        "        feats[\"return_3month\"] = feats[\n",
        "            \"EndOfDayQuote ExchangeOfficialClose\"\n",
        "        ].pct_change(60)\n",
        "        # 終値の20営業日ボラティリティ\n",
        "        feats[\"volatility_1month\"] = (\n",
        "            np.log(feats[\"EndOfDayQuote ExchangeOfficialClose\"])\n",
        "            .diff()\n",
        "            .rolling(20)\n",
        "            .std()\n",
        "        )\n",
        "        # 終値の40営業日ボラティリティ\n",
        "        feats[\"volatility_2month\"] = (\n",
        "            np.log(feats[\"EndOfDayQuote ExchangeOfficialClose\"])\n",
        "            .diff()\n",
        "            .rolling(40)\n",
        "            .std()\n",
        "        )\n",
        "        # 終値の60営業日ボラティリティ\n",
        "        feats[\"volatility_3month\"] = (\n",
        "            np.log(feats[\"EndOfDayQuote ExchangeOfficialClose\"])\n",
        "            .diff()\n",
        "            .rolling(60)\n",
        "            .std()\n",
        "        )\n",
        "        # 終値と20営業日の単純移動平均線の乖離\n",
        "        feats[\"MA_gap_1month\"] = feats[\"EndOfDayQuote ExchangeOfficialClose\"] / (\n",
        "            feats[\"EndOfDayQuote ExchangeOfficialClose\"].rolling(20).mean()\n",
        "        )\n",
        "        # 終値と40営業日の単純移動平均線の乖離\n",
        "        feats[\"MA_gap_2month\"] = feats[\"EndOfDayQuote ExchangeOfficialClose\"] / (\n",
        "            feats[\"EndOfDayQuote ExchangeOfficialClose\"].rolling(40).mean()\n",
        "        )\n",
        "        # 終値と60営業日の単純移動平均線の乖離\n",
        "        feats[\"MA_gap_3month\"] = feats[\"EndOfDayQuote ExchangeOfficialClose\"] / (\n",
        "            feats[\"EndOfDayQuote ExchangeOfficialClose\"].rolling(60).mean()\n",
        "        )\n",
        "        # 欠損値処理\n",
        "        feats = feats.fillna(0)\n",
        "        # 元データのカラムを削除\n",
        "        feats = feats.drop([\"EndOfDayQuote ExchangeOfficialClose\"], axis=1)\n",
        "\n",
        "        # 1B resample + ffill で金曜日に必ずレコードが存在するようにする\n",
        "        feats = feats.resample(\"B\").ffill()\n",
        "        # 特徴量を金曜日日付のみに絞り込む\n",
        "        FRIDAY = 4\n",
        "        feats = feats.loc[feats.index.dayofweek == FRIDAY]\n",
        "\n",
        "        # 欠損値処理を行います。\n",
        "        feats = feats.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        # 銘柄コードを設定\n",
        "        feats[\"code\"] = code\n",
        "\n",
        "        # 生成対象日以降の特徴量に絞る\n",
        "        feats = feats.loc[pd.Timestamp(start_dt) :]\n",
        "\n",
        "        return feats\n",
        "\n",
        "    @classmethod\n",
        "    def get_feature_columns(cls, dfs, train_X, column_group=\"fundamental+technical\"):\n",
        "        # 特徴量グループを定義\n",
        "        # ファンダメンタル\n",
        "        fundamental_cols = dfs[\"stock_fin\"].select_dtypes(\"float64\").columns\n",
        "        fundamental_cols = fundamental_cols[\n",
        "            fundamental_cols != \"Result_Dividend DividendPayableDate\"\n",
        "        ]\n",
        "        fundamental_cols = fundamental_cols[fundamental_cols != \"Local Code\"]\n",
        "        # 価格変化率\n",
        "        returns_cols = [x for x in train_X.columns if \"return\" in x]\n",
        "        # テクニカル\n",
        "        technical_cols = [\n",
        "            x for x in train_X.columns if (x not in fundamental_cols) and (x != \"code\")\n",
        "        ]\n",
        "        columns = {\n",
        "            \"fundamental_only\": fundamental_cols,\n",
        "            \"return_only\": returns_cols,\n",
        "            \"technical_only\": technical_cols,\n",
        "            \"fundamental+technical\": list(fundamental_cols) + list(technical_cols),\n",
        "        }\n",
        "        return columns[column_group]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model(cls, model_path=\"../model\", labels=None):\n",
        "        \"\"\"Get model method\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the trained model directory.\n",
        "            labels (arrayt): list of prediction target labels\n",
        "\n",
        "        Returns:\n",
        "            bool: The return value. True for success, False otherwise.\n",
        "\n",
        "        \"\"\"\n",
        "        if cls.models is None:\n",
        "            cls.models = {}\n",
        "        if labels is None:\n",
        "            labels = cls.TARGET_LABELS\n",
        "        for label in labels:\n",
        "            m = os.path.join(model_path, f\"my_model_{label}.pkl\")\n",
        "            with open(m, \"rb\") as f:\n",
        "                # pickle形式で保存されているモデルを読み込み\n",
        "                cls.models[label] = pickle.load(f)\n",
        "\n",
        "        return True\n",
        "\n",
        "    @classmethod\n",
        "    def get_exclude(\n",
        "        cls,\n",
        "        df_tdnet,  # tdnetのデータ\n",
        "        start_dt=None,  # データ取得対象の開始日、Noneの場合は制限なし\n",
        "        end_dt=None,  # データ取得対象の終了日、Noneの場合は制限なし\n",
        "        lookback=7,  # 除外考慮期間 (days)\n",
        "        target_day_of_week=4,  # 起点となる曜日\n",
        "    ):\n",
        "        # 特別損失のレコードを取得\n",
        "        special_loss = df_tdnet[df_tdnet[\"disclosureItems\"].str.contains('201\"')].copy()\n",
        "        # 日付型を調整\n",
        "        special_loss[\"date\"] = pd.to_datetime(special_loss[\"disclosedDate\"])\n",
        "        # 処理対象開始日が設定されていない場合はデータの最初の日付を取得\n",
        "        if start_dt is None:\n",
        "            start_dt = special_loss[\"date\"].iloc[0]\n",
        "        # 処理対象終了日が設定されていない場合はデータの最後の日付を取得\n",
        "        if end_dt is None:\n",
        "            end_dt = special_loss[\"date\"].iloc[-1]\n",
        "        #  処理対象日で絞り込み\n",
        "        special_loss = special_loss[\n",
        "            (start_dt <= special_loss[\"date\"]) & (special_loss[\"date\"] <= end_dt)\n",
        "        ]\n",
        "        # 出力用にカラムを調整\n",
        "        res = special_loss[[\"code\", \"disclosedDate\", \"date\"]].copy()\n",
        "        # 銘柄コードを4桁にする\n",
        "        res[\"code\"] = res[\"code\"].astype(str).str[:-1]\n",
        "        # 予測の基準となる金曜日の日付にするために調整\n",
        "        res[\"remain\"] = (target_day_of_week - res[\"date\"].dt.dayofweek) % 7\n",
        "        res[\"start_dt\"] = res[\"date\"] + pd.to_timedelta(res[\"remain\"], unit=\"d\")\n",
        "        res[\"end_dt\"] = res[\"start_dt\"] + pd.Timedelta(days=lookback)\n",
        "        columns = [\"code\", \"date\", \"start_dt\", \"end_dt\"]\n",
        "        return res[columns].reset_index(drop=True)\n",
        "\n",
        "    @classmethod\n",
        "    def strategy(cls, strategy_id, df, df_tdnet):\n",
        "        df = df.copy()\n",
        "        # 銘柄選択方法選択\n",
        "        if strategy_id in [1, 4]:\n",
        "            # 最高値モデル +　最安値モデル\n",
        "            df.loc[:, \"pred\"] = df.loc[:, \"label_high_20\"] + df.loc[:, \"label_low_20\"]\n",
        "        elif strategy_id in [2, 5]:\n",
        "            # 最高値モデル\n",
        "            df.loc[:, \"pred\"] = df.loc[:, \"label_high_20\"]\n",
        "        elif strategy_id in [3, 6]:\n",
        "            # 最高値モデル\n",
        "            df.loc[:, \"pred\"] = df.loc[:, \"label_low_20\"]\n",
        "        else:\n",
        "            raise ValueError(\"no strategy_id selected\")\n",
        "\n",
        "        # 特別損失を除外する場合\n",
        "        if strategy_id in [4, 5, 6]:\n",
        "            # 特別損失が発生した銘柄一覧を取得\n",
        "            df_exclude = cls.get_exclude(df_tdnet)\n",
        "            # 除外用にユニークな列を作成します。\n",
        "            df_exclude.loc[:, \"date-code_lastweek\"] = df_exclude.loc[:, \"start_dt\"].dt.strftime(\"%Y-%m-%d-\") + df_exclude.loc[:, \"code\"]\n",
        "            df_exclude.loc[:, \"date-code_thisweek\"] = df_exclude.loc[:, \"end_dt\"].dt.strftime(\"%Y-%m-%d-\") + df_exclude.loc[:, \"code\"]\n",
        "            df.loc[:, \"date-code_lastweek\"] = (df.index - pd.Timedelta(\"7D\")).strftime(\"%Y-%m-%d-\") + df.loc[:, \"code\"].astype(str)\n",
        "            df.loc[:, \"date-code_thisweek\"] = df.index.strftime(\"%Y-%m-%d-\") + df.loc[:, \"code\"].astype(str)\n",
        "            # 特別損失銘柄を除外\n",
        "            df = df.loc[~df.loc[:, \"date-code_lastweek\"].isin(df_exclude.loc[:, \"date-code_lastweek\"])]\n",
        "            df = df.loc[~df.loc[:, \"date-code_thisweek\"].isin(df_exclude.loc[:, \"date-code_thisweek\"])]\n",
        "\n",
        "        # 予測出力を降順に並び替え\n",
        "        df = df.sort_values(\"pred\", ascending=False)\n",
        "        # 予測出力の大きいものを取得\n",
        "        df = df.groupby(\"datetime\").head(30)\n",
        "\n",
        "        return df\n",
        "\n",
        "    @classmethod\n",
        "    def predict(\n",
        "        cls,\n",
        "        inputs,\n",
        "        labels=None,\n",
        "        codes=None,\n",
        "        start_dt=TEST_START,\n",
        "        load_data=[\"stock_list\", \"stock_fin\", \"stock_fin_price\", \"stock_price\", \"tdnet\"],\n",
        "        fin_columns=None,\n",
        "        strategy_id=1,\n",
        "    ):\n",
        "        \"\"\"Predict method\n",
        "\n",
        "        Args:\n",
        "            inputs (dict[str]): paths to the dataset files\n",
        "            labels (list[str]): target label names\n",
        "            codes (list[int]): traget codes\n",
        "            start_dt (str): specify date range\n",
        "            load_data (list[str]): specify loading data\n",
        "            fin_columns (list[str]): specify feature columns\n",
        "            strategy_id (int): specify strategy\n",
        "        Returns:\n",
        "            str: Inference for the given input.\n",
        "        \"\"\"\n",
        "        if fin_columns is None:\n",
        "            fin_columns = [\n",
        "                \"Result_FinancialStatement FiscalYear\",\n",
        "                \"Result_FinancialStatement NetSales\",\n",
        "                \"Result_FinancialStatement OperatingIncome\",\n",
        "                \"Result_FinancialStatement OrdinaryIncome\",\n",
        "                \"Result_FinancialStatement NetIncome\",\n",
        "                \"Result_FinancialStatement TotalAssets\",\n",
        "                \"Result_FinancialStatement NetAssets\",\n",
        "                \"Result_FinancialStatement CashFlowsFromOperatingActivities\",\n",
        "                \"Result_FinancialStatement CashFlowsFromFinancingActivities\",\n",
        "                \"Result_FinancialStatement CashFlowsFromInvestingActivities\",\n",
        "                \"Forecast_FinancialStatement FiscalYear\",\n",
        "                \"Forecast_FinancialStatement NetSales\",\n",
        "                \"Forecast_FinancialStatement OperatingIncome\",\n",
        "                \"Forecast_FinancialStatement OrdinaryIncome\",\n",
        "                \"Forecast_FinancialStatement NetIncome\",\n",
        "                \"Result_Dividend FiscalYear\",\n",
        "                \"Result_Dividend QuarterlyDividendPerShare\",\n",
        "                \"Result_Dividend AnnualDividendPerShare\",\n",
        "                \"Forecast_Dividend FiscalYear\",\n",
        "                \"Forecast_Dividend QuarterlyDividendPerShare\",\n",
        "                \"Forecast_Dividend AnnualDividendPerShare\",\n",
        "            ]\n",
        "\n",
        "        # データ読み込み\n",
        "        if cls.dfs is None:\n",
        "            print(\"[+] load data\")\n",
        "            cls.get_dataset(inputs, load_data)\n",
        "            cls.get_codes(cls.dfs)\n",
        "\n",
        "        # 予測対象の銘柄コードと目的変数を設定\n",
        "        if codes is None:\n",
        "            codes = cls.codes\n",
        "        if labels is None:\n",
        "            labels = cls.TARGET_LABELS\n",
        "\n",
        "        if \"purchase_date\" in inputs.keys():\n",
        "            # ランタイム環境では指定された投資対象日付を使用します\n",
        "            # purchase_dateを読み込み\n",
        "            df_purchase_date = pd.read_csv(inputs[\"purchase_date\"])\n",
        "            # purchase_dateの最も古い日付を設定\n",
        "            start_dt = pd.Timestamp(\n",
        "                df_purchase_date.sort_values(\"Purchase Date\").iloc[0, 0]\n",
        "            )\n",
        "\n",
        "        # 予測対象日を調整\n",
        "        # start_dtにはポートフォリオの購入日を指定しているため、\n",
        "        # 予測に使用する前週の金曜日を指定します。\n",
        "        start_dt = pd.Timestamp(start_dt) - pd.Timedelta(\"3D\")\n",
        "\n",
        "        # 特徴量を作成\n",
        "        print(\"[+] generate feature\")\n",
        "        buff = []\n",
        "        for code in tqdm(codes):\n",
        "            buff.append(\n",
        "                cls.get_features_for_predict2(cls.dfs, code, fin_columns, start_dt)\n",
        "            )\n",
        "        feats = pd.concat(buff)\n",
        "\n",
        "        # 結果を以下のcsv形式で出力する\n",
        "        # 1列目:date\n",
        "        # 2列目:Local Code\n",
        "        # 3列目:budget\n",
        "        # headerあり、2列目3列目はint64\n",
        "\n",
        "        # 日付と銘柄コードに絞り込み\n",
        "        df = feats.loc[:, [\"code\"]].copy()\n",
        "        # 購入金額を設定 (ここでは一律50000とする)\n",
        "        df.loc[:, \"budget\"] = 50000\n",
        "\n",
        "        # 特徴量カラムを指定\n",
        "        feature_columns = ScoringService.get_feature_columns(cls.dfs, feats)\n",
        "\n",
        "        # 目的変数毎に予測\n",
        "        print(\"[+] predict\")\n",
        "        for label in tqdm(labels):\n",
        "            # 予測実施\n",
        "            df[label] = ScoringService.models[label].predict(feats[feature_columns])\n",
        "            # 出力対象列に追加\n",
        "\n",
        "        # 銘柄選択方法選択\n",
        "        df = cls.strategy(strategy_id, df, cls.dfs[\"tdnet\"])\n",
        "\n",
        "        # 日付順に並び替え\n",
        "        df.sort_index(kind=\"mergesort\", inplace=True)\n",
        "        # 月曜日日付に変更\n",
        "        df.index = df.index + pd.Timedelta(\"3D\")\n",
        "        # 出力用に調整\n",
        "        df.index.name = \"date\"\n",
        "        df.rename(columns={\"code\": \"Local Code\"}, inplace=True)\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "        # 出力対象列を定義\n",
        "        output_columns = [\"date\", \"Local Code\", \"budget\"]\n",
        "\n",
        "        out = io.StringIO()\n",
        "        df.to_csv(out, header=True, index=False, columns=output_columns)\n",
        "\n",
        "        return out.getvalue()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGIKdBcfkpLd"
      },
      "source": [
        "## 予測の実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ic1dkIqdo1m"
      },
      "source": [
        "　ここまででソースコードの変更が完了したら、いよいよモデルを実行します。まず、対象期間を設定します。2020-01-01を指定していますが、予測の出力される日はここまでの実装により月曜日の日付のみになるはずです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvRTcL8ckpLk"
      },
      "source": [
        "# 対象期間を設定\n",
        "start_dt = pd.Timestamp(\"2020-01-01\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYmZpOujkpLk"
      },
      "source": [
        "# 学習済みモデルを読み込みます\n",
        "ScoringService.get_model(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbL6W2JekpLk",
        "scrolled": true
      },
      "source": [
        "# 予測を実行します\n",
        "str_ret = ScoringService.predict(inputs, start_dt=start_dt, strategy_id=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhPX4t2MkpLk",
        "scrolled": false
      },
      "source": [
        "# 出力を確認\n",
        "print(\"\\n\".join(str_ret.split(\"\\n\")[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwcVEaw8do1n"
      },
      "source": [
        "次で実施するバックテストのために結果をCSVファイルに保存しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZctCgcnkpLl"
      },
      "source": [
        "# 出力を保存\n",
        "with open(\"chapter03-tutorial-02-backtest-1.csv\", mode=\"w\") as f:\n",
        "    f.write(str_ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RO4AFHKkpLl"
      },
      "source": [
        "## 6. バックテストの実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66p-BmVkkpLl"
      },
      "source": [
        "```\n",
        "　本コンペティションの Public LB および Private LB と同等の評価ロジックを実装したバックテスト用の `backtest.py` ファイルを使用して、作成したポートフォリオを評価します。バックテストの使用法などの詳細は「3.6. バックテスト」をご参照ください。\n",
        "\n",
        "　初めにバックテストを使用する際に必要なデータを準備します。バックテストの実行には以下の3つのデータが必要になります。\n",
        "\n",
        "1. ユニバース (stock_list.csv.gz)\n",
        "2. 株価 (stock_price.csv.gz)\n",
        "3. テスト対象のポートフォリオ\n",
        "\n",
        "　Backtest.prepare_data に1および2のデータを保存しているディレクトリへのパスを指定して、必要なデータをロードします。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvKDJoX5kpLl",
        "scrolled": true
      },
      "source": [
        "# データを保存しているディレクトリを指定します。\n",
        "backtest_dataset_dir = dataset_dir\n",
        "# バックテストに必要なデータを読み込みます。\n",
        "backtest_codes, backtest_price = Backtest.prepare_data(backtest_dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QmR8g3kpLl"
      },
      "source": [
        "　Backtest.load_submit にバックテスト対象のポートフォリオを保存したファイルへのパスを指定して読み込みます。`load_submit` ではデータを読み込み時にフォーマットのチェックをしたり、レコード順を付与する等、バックテスト実行のための前処理を実施しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ5_sHIRkpLl"
      },
      "source": [
        "# 先ほど出力したポートフィリオデータを読み込みます\n",
        "df_submit = Backtest.load_submit(\"chapter03-tutorial-02-backtest-1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGo-saHykpLm"
      },
      "source": [
        "3つのデータを指定してバックテストを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOxoJUIckpLm"
      },
      "source": [
        "# バックテスト結果保存用\n",
        "results = {}\n",
        "stocks = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1hrLnZfkpLm"
      },
      "source": [
        "results[1], stocks[1] = Backtest.run(df_submit.loc[start_dt:], backtest_codes, backtest_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3diElBy2kpLn"
      },
      "source": [
        "結果を確認します。うまく結果が取得できていることが確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gNHhNjVkpLn",
        "scrolled": true
      },
      "source": [
        "# バックテスト結果のサマリー\n",
        "results[1].head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDWlj2NFkpLn"
      },
      "source": [
        "# 銘柄毎の情報\n",
        "stocks[1].head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLZiwaTokpLo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkbhq_akpLo"
      },
      "source": [
        "### ポートフォリオの銘柄選択方法を変更してバックテストを比較します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUp8oGBmkpLo"
      },
      "source": [
        "　次に、銘柄選択の方法を変えてポートフォリオを組成します。1はすでに取得できていますので2、3も取得します。\n",
        "\n",
        "1. 最高値モデル出力に最安値モデル出力を足して使用\n",
        "2. 最高値モデルの出力を使用\n",
        "3. 最安値モデルの出力を使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRPEEEKYkpLo",
        "scrolled": false
      },
      "source": [
        "# 戦略毎に処理\n",
        "for strategy_id in tqdm([2, 3]):\n",
        "    # ポートフォリオ組成\n",
        "    str_ret = ScoringService.predict(inputs, start_dt=start_dt, strategy_id=strategy_id)\n",
        "    # ポートフォリオを保存\n",
        "    with open(f\"chapter03-tutorial-02-backtest-{strategy_id}.csv\", mode=\"w\") as f:\n",
        "        f.write(str_ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k8_dNXgkpLo"
      },
      "source": [
        "バックテストを実施します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb6hEe2NkpLp"
      },
      "source": [
        "# 戦略毎に処理\n",
        "for strategy_id in tqdm([2, 3]):\n",
        "    # ポートフォリオを読み込み\n",
        "    df_submit = Backtest.load_submit(f\"chapter03-tutorial-02-backtest-{strategy_id}.csv\")\n",
        "    # バックテスト実行\n",
        "    results[strategy_id], stocks[strategy_id] = Backtest.run(df_submit.loc[start_dt:], backtest_codes, backtest_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS47kHbIkpLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZymBSlY9kpLp"
      },
      "source": [
        "## 7. バックテストの評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc29XoH_kpLp"
      },
      "source": [
        "```\n",
        "上記のバックテストを実行して取得した、週毎のリターン情報と各銘柄毎の購入結果数の情報を評価します。各項目の定義については「3.6.4. バックテストの評価軸と取引戦略」をご参照ください。\n",
        "\n",
        "以下では、結果の評価として以下を実施します。\n",
        "\n",
        "===== 週毎のリターンデータ\n",
        "\n",
        "1. 週毎の運用実績の分布をプロット (本コンペティションの評価項目)\n",
        "2. 週毎の運用実績の統計量\n",
        "3. 週毎の勝率・ペイオフレシオ・シャープレシオ\n",
        "4. 週毎に曜日別のリターンをプロット\n",
        "5. 週毎のリターンの累積プロット\n",
        "6. ユニバースとの散布図\n",
        "7. ユニバースに対するベータを計算\n",
        "\n",
        "===== 各銘柄毎のデータ\n",
        "\n",
        "1. 銘柄毎の運用実績の分布をプロット\n",
        "2. 銘柄毎のreturnの分布をプロット\n",
        "3. 週毎の勝ち銘柄率をプロット\n",
        "4. 週毎の勝ち銘柄率の統計量\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj0p8JgLdo1w"
      },
      "source": [
        "### 1. 週毎の運用実績の分布をプロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1qGAYzXkpLp"
      },
      "source": [
        "　まず、週毎の運用実績の分布をヒストグラムで確認します。ここから先の解析は、「1.最高値モデル出力に最安値モデル出力を足して使用」、「2.最高値モデルの出力を使用」、「3.最安値モデルの出力を使用」の3種類を解析しており、特別損失銘柄を除外したポートフォリオ（4〜6）の解析は最後に実施します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5SP-UCIkpLp"
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(1, len(results), figsize=(8 * len(results), 8), sharex=True, sharey=True)\n",
        "\n",
        "# 戦略毎に処理\n",
        "for i, k in enumerate(results.keys()):\n",
        "    # 描画位置を指定\n",
        "    ax = axes[i]\n",
        "    # 分布をプロット\n",
        "    results[k].week_pl.hist(bins=25, ax=ax)\n",
        "    # タイトルを設定\n",
        "    ax.set_title(f\"{k}: week pl distribution\")\n",
        "#　描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO0ov32_do1w"
      },
      "source": [
        "　1及び2の戦略は似たような分布をしていることが確認できます。最安値モデルを利用している3は、1や2とは大きく分布が異なっており、分布が狭く運用実績が0近辺に集中していることがわかります。すなわち、3の戦略では、ボラティリティが低い銘柄を中心に採用している可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-aPXBs1do1w"
      },
      "source": [
        "### 2. 週毎の運用実績の統計量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgIWrLigdo1w"
      },
      "source": [
        "　次に、週毎の運用実績の統計量を算出します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u94HpibvkpLq",
        "scrolled": true
      },
      "source": [
        "# week_plの分布の統計量\n",
        "\n",
        "# 結合用データ保存\n",
        "buff = []\n",
        "# ストラテジー毎に処理\n",
        "for k in results.keys():\n",
        "    # week_plの統計量を取得します。\n",
        "    df = results[k].loc[:, [\"week_pl\"]].describe().T\n",
        "    # インデックスを編集してストラテジーのIDにする\n",
        "    df.index = [k]\n",
        "    # インデックス名変更\n",
        "    df.index.name = \"strategy_id\"\n",
        "    # 結合用に保存\n",
        "    buff.append(df)\n",
        "# 結合して表示\n",
        "pd.concat(buff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q74RJeqHdo1x"
      },
      "source": [
        "　分布情報を数値で確認すると、先程と同様に1及び2の分布情報が似ていることが確認できます。異なる点として25%分位点を比較すると、1の戦略は2の戦略よりも負けを若干抑えることができている可能性が示唆されます。2の戦略では、1の戦略よりも平均が大きくなっています。結果として、1の戦略は2の戦略よりも収益性は劣るものの、リスクコントロールができている可能性が示唆されています。また、3の戦略の結果は平均が1及び2の戦略より低く、25%及び75%分位点も1及び2の戦略よりも狭い分布となっていることがわかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq-z2ctGdo1x"
      },
      "source": [
        "### 3. 週毎の勝率・ペイオフレシオ・シャープレシオ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk_9X4ZwkpLq"
      },
      "source": [
        "　他のメトリクスも確認していきましょう。週毎の勝率、ペイオフレシオ、シャープレシオを算出します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qiz4o7pukpLq",
        "scrolled": false
      },
      "source": [
        "# 結合用データ保存\n",
        "buff = []\n",
        "# 戦略毎に処理\n",
        "for k in results.keys():\n",
        "    df_return = results[k]\n",
        "    # 計算結果保存用\n",
        "    d = {}\n",
        "    # 件数\n",
        "    d[\"count\"] = df_return.shape[0]\n",
        "    # 勝率\n",
        "    d[\"win_ratio\"] = (\n",
        "        df_return.loc[df_return.loc[:, \"week_return\"] > 0].shape[0] / d[\"count\"]\n",
        "    )\n",
        "    # ペイオフレシオ\n",
        "    d[\"payoff_ratio\"] = df_return.loc[\n",
        "        df_return.loc[:, \"week_return\"] > 0, \"week_return\"\n",
        "    ].mean() / (\n",
        "        -1 * df_return.loc[df_return.loc[:, \"week_return\"] <= 0, \"week_return\"].mean()\n",
        "    )\n",
        "    # シャープレシオ\n",
        "    d[\"sharp\"] = (\n",
        "        df_return.loc[:, \"week_return\"].mean() / df_return.loc[:, \"week_return\"].std()\n",
        "    )\n",
        "    # 平均PL\n",
        "    d[\"avgPL\"] = df_return.loc[:, \"week_pl\"].mean()\n",
        "    # week_plの合計\n",
        "    d[\"PL\"] = df_return.loc[:, \"week_pl\"].sum()\n",
        "    # strategy_idを設定\n",
        "    df = pd.DataFrame([d], index=[k])\n",
        "    # インデックス名を指定\n",
        "    df.index.name = \"strategy_id\"\n",
        "    # 結合用に保存\n",
        "    buff.append(df)\n",
        "# 結合して表示\n",
        "pd.concat(buff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW_d020Vdo1x"
      },
      "source": [
        "　1及び2の戦略の結果はこちらでも似通っています。3の戦略は勝率も低く、運用実績も低いため、今のところ最安値を単体で利用する手法が機能しているようには見えません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjIuoHpzdo1y"
      },
      "source": [
        "### 4. 週毎に曜日別のリターンをプロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5dWW02Rdo1y"
      },
      "source": [
        "```\n",
        "　週毎の1日目から5日目までのリターンの推移をプロットし、曜日毎に勝ち負けの分布に差異が無いかを確認しています。3章と同じようにseabornのバイオリンプロット(link:https://seaborn.pydata.org/generated/seaborn.violinplot.html[リンク])を利用します。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obSurv9vkpLq",
        "scrolled": false
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(\n",
        "    len(results), 1, figsize=(20, 4 * len(results)), sharex=True, sharey=True\n",
        ")\n",
        "\n",
        "# 描画用データ保存用\n",
        "dfs_plot = {}\n",
        "\n",
        "# 戦略毎に処理\n",
        "for i, k in enumerate(results.keys()):\n",
        "    # 描画位置を指定\n",
        "    ax = axes[i]\n",
        "    # 列を行に変換\n",
        "    dfs_plot[k] = (\n",
        "        results[k]\n",
        "        .set_index(\"date\")\n",
        "        .loc[\n",
        "            :,\n",
        "            [\n",
        "                \"day_1_return\",\n",
        "                \"day_2_return\",\n",
        "                \"day_3_return\",\n",
        "                \"day_4_return\",\n",
        "                \"day_5_return\",\n",
        "            ],\n",
        "        ]\n",
        "        .stack()\n",
        "        .to_frame()\n",
        "        .reset_index()\n",
        "        .rename(columns={0: \"return\"})\n",
        "    )\n",
        "    # 作業用に変数設定\n",
        "    df_plot = dfs_plot[k]\n",
        "    # 各週を列に変換して曜日毎のreturnをプロット\n",
        "    df_plot.groupby([\"level_1\", \"date\"]).first().unstack().plot(ax=ax, legend=False)\n",
        "    # タイトルを設定\n",
        "    ax.set_title(f\"{k}: Daily returns\")\n",
        "    # リターンが0の位置に基準線を描画\n",
        "    ax.axhline(y=0, color=\"black\")\n",
        "    # グリッドを表示\n",
        "    ax.grid(True)\n",
        "# 描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3FafVVXdo1y"
      },
      "source": [
        "### バイオリンプロット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO7KpdELkpLr",
        "scrolled": false
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(len(results), 1, figsize=(15, 4 * len(results)), sharex=True, sharey=True)\n",
        "\n",
        "# 戦略毎に処理\n",
        "for i, k in enumerate(results.keys()):\n",
        "    # 描画位置を指定\n",
        "    ax = axes[i]\n",
        "    # 箱が見やすいように横方向を指定してプロット\n",
        "    sns.violinplot(x=\"return\", y=\"level_1\", data=dfs_plot[k], ax=ax, orient=\"h\")\n",
        "    # タイトルを設定\n",
        "    ax.set_title(f\"{k}: daily return\")\n",
        "    # グリッドを表示\n",
        "    ax.grid(True)\n",
        "# 文字が重なって読みにくいので間隔調整\n",
        "fig.tight_layout(pad=2.0)\n",
        "# 描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHxaUmzCdo1y"
      },
      "source": [
        "　この結果から1及び2の戦略は、木曜日・金曜日の負けが大きそうなことがわかります。予測モデルを利用する場合、週の後半に負けているのはその予測モデルの効果が後半で薄れている可能性を示唆していることもあるので注意が必要です。このグラフにも若干その傾向が観測されるため、何らかの改善を実施した場合に、この結果がどうなっているかを確認する価値があります。3の戦略は1及び2の戦略と比較すると分布が狭いことが確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oAY8pwRdo1z"
      },
      "source": [
        "### 6. 週毎のリターンの累積プロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16CJ4KbTkpLr"
      },
      "source": [
        "　次に、取得した収益率の時系列を累積プロットします。まず、比較対象であるベンチマークとして取引対象の全銘柄の平均週次リターンを計算します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl2ML8iekpLr"
      },
      "source": [
        "# 変数名を調整します。\n",
        "# backtest_priceはユニバースで絞り込み済みです\n",
        "df_price = backtest_price"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VijWFrhxkpLr"
      },
      "source": [
        "# 週毎に始値と終値を取得\n",
        "df_wp = (\n",
        "    # start_dt以降の日付のみ計算\n",
        "    df_price.loc[df_price.index >= start_dt].sort_values([\"Local Code\", \"EndOfDayQuote Date\"])\n",
        "    # 銘柄コード毎に処理\n",
        "    .groupby(\"Local Code\")\n",
        "    # 月曜日スタートで週にリサンプル\n",
        "    .resample(\"W-MON\", label=\"left\", closed=\"left\")\n",
        "    # 始値は最初のレコード、終値は最後のレコードを取得\n",
        "    .agg({\"EndOfDayQuote Open\": \"first\", \"EndOfDayQuote ExchangeOfficialClose\": \"last\"})\n",
        "    # マルチインデックスを解除\n",
        "    .reset_index(level=[0])\n",
        ")\n",
        "# Open が 0.0 の銘柄は値段が付かなかった銘柄で、バックテストでは購入対象外であるため除外する\n",
        "df_wp = df_wp.loc[df_wp.loc[:, \"EndOfDayQuote Open\"] != 0.0]\n",
        "# 銘柄毎の週次リターンを計算\n",
        "df_wp.loc[:, \"universe\"] = (\n",
        "    (\n",
        "        (\n",
        "            df_wp.loc[:, \"EndOfDayQuote ExchangeOfficialClose\"]\n",
        "            / df_wp.loc[:, \"EndOfDayQuote Open\"]\n",
        "        )\n",
        "        - 1\n",
        "    )\n",
        "    * 100\n",
        ")\n",
        "# ユニバースの週毎のリターンを計算します。\n",
        "df_universe_return = df_wp.groupby(df_wp.index)[\"universe\"].mean().to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM6cCanUdo1z"
      },
      "source": [
        "　ベンチマークとして取引対象の全銘柄の平均週次リターンが準備できたら、今回の取引戦略の結果と一緒にプロットしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7XxyNlOkpLs",
        "scrolled": false
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(1, 1, figsize=(20, 8), sharex=True, sharey=True)\n",
        "\n",
        "# 戦略毎に処理\n",
        "for k in results.keys():\n",
        "    # 描画位置を指定\n",
        "    ax = axes\n",
        "    # 戦略別の累積リターンを描画\n",
        "    results[k].set_index(\"date\").loc[:, [\"week_return\"]].rename(\n",
        "        columns={\"week_return\": f\"{k}: week_return\"}\n",
        "    ).cumsum().plot(ax=ax)\n",
        "\n",
        "# ユニバースの週次リターンの累積をプロット\n",
        "df_universe_return.cumsum().plot(ax=ax, color=\"black\", label=\"universe\")\n",
        "\n",
        "# 表示を調整\n",
        "ax.set_title(\"Cumulative week_return\")\n",
        "# グリッドを表示\n",
        "ax.grid(True)\n",
        "# 描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaQ3cFFKdo10"
      },
      "source": [
        "　このプロットからは複数の事がわかります。まず1及び2の戦略は想定通り、極めて似通った結果となっています。1の戦略は最安値モデルの結果も足しているため、2の最高値モデルのみを用いたポートフォリオよりもドローダウンが小さいことを期待していましたが、3月の暴落時の結果を見る限り、その効果は観測されていません。これは、3の最安値モデルでも暴落時に下落低減効果が観測できないため、3の戦略では残念ながら3月の暴落の対処にはならなかったようです。\n",
        "\n",
        "　1及び2の戦略は3月以降の反発で大きく収益を上げています。特にユニバースがほぼフラットになった6月以降も1及び2の戦略は高い収益をあげており、マーケットがフラットな状況でも銘柄選択により収益を上げる力がある可能性が示唆されます。\n",
        "\n",
        "　また、3の戦略は一見するとユニバースとほぼ同じ収益に見えますが、3月以降ほぼドローダウンが発生しないことは注目に値します。これは低いボラティリティの銘柄を優先的に採用したときなどに見られる現象で、実際に収益曲線が安定していることからも、3の戦略自体もアンサンブルなどのデータとしては有望な可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6XDa0VVdo10"
      },
      "source": [
        "### 7. ユニバースとの散布図"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqCWKsG6do10"
      },
      "source": [
        "　ユニバースとリターンの散布図をチェックします。ユニバースとリターンの散布図は、マーケットの動きに対してポートフォリオの運用実績がどのように分布するかを確認するために利用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqSKEtP8kpLs",
        "scrolled": false
      },
      "source": [
        "# 戦略毎に処理\n",
        "for k in results.keys():\n",
        "    # 散布図をプロット\n",
        "    p = sns.jointplot(\n",
        "        x=df_universe_return.iloc[:, 0],\n",
        "        y=results[k].loc[:, \"week_return\"],\n",
        "        kind=\"reg\",\n",
        "        height=5,\n",
        "        stat_func=stats.pearsonr,\n",
        "    )\n",
        "    # タイトルを設定\n",
        "    p.fig.suptitle(f\"{k}: week_return vs universe\")\n",
        "    # タイトル表示用に位置を調整\n",
        "    p.fig.subplots_adjust(top=0.95)\n",
        "    # 描画\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu5v0LN8do10"
      },
      "source": [
        "　1の戦略からはユニバースが0のときも収益を上げる力がある傾向が観測できます。2の戦略の分布は1よりも広く、これは高いボラティリティの銘柄を利用している可能性が示唆されます。3の戦略はユニバースの下落のときは一緒に負けていることが観測できます。もしユニバースが下落しても、下落を抑えることができていれば、3の戦略はリスク回避用のモデルとして理想的な結果でしたが、そのような効果までは観測できませんでした。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B3XSEcZdo10"
      },
      "source": [
        "### 8. ユニバースに対するベータを計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og9LXz9Ldo11"
      },
      "source": [
        "　最後にベータを計算しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIRfd6XpkpLs"
      },
      "source": [
        "# 結合用に保存\n",
        "buff = []\n",
        "# 戦略毎に処理\n",
        "for k in results.keys():\n",
        "    # ベータを計算\n",
        "    res = stats.linregress(df_universe_return.iloc[:,0], results[k].loc[:, \"week_return\"])\n",
        "    # 一覧表示用にデータフレームを作成\n",
        "    df_beta = pd.DataFrame([res.slope], index=[k], columns=[\"beta\"])\n",
        "    # インデックス名を設定\n",
        "    df_beta.index.name = \"storategy_id\"\n",
        "    # 保存\n",
        "    buff.append(df_beta)\n",
        "# 結合して表示\n",
        "pd.concat(buff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVBCYWhTkpLs"
      },
      "source": [
        "　1及び2の戦略は、1.2近辺の高ベータなストラテジーであることがわかります。3の戦略は低いボラティリティの銘柄をポートフォリオに採用するストラテジー特有の傾向である低いベータ値が観測できています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ5WOzHwkpLs"
      },
      "source": [
        "### 銘柄毎のデータを使用して分析していきます"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ_7P-v1kpLs"
      },
      "source": [
        "　次に、銘柄毎のデータを使用して分析します。ここでは、銘柄毎のデータを使用して分析するために必要な計算を実施しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cpa1n_ekpLt"
      },
      "source": [
        "# 分析用データ保存用\n",
        "dfs_analyze = {}\n",
        "# 戦略毎に処理\n",
        "for i in stocks.keys():\n",
        "    # 分析用にデータをコピー\n",
        "    df_analyze = stocks[i].copy()\n",
        "    # day5に必ず値が存在するように調整します\n",
        "    df_analyze.loc[:, [\"day_1\", \"day_2\", \"day_3\", \"day_4\", \"day_5\"]] = (\n",
        "        df_analyze.loc[:, [\"day_1\", \"day_2\", \"day_3\", \"day_4\", \"day_5\"]]\n",
        "        .replace(0.0, np.nan)\n",
        "        .ffill(axis=1)\n",
        "    )\n",
        "    # 終値とエントリーの差分を計算\n",
        "    df_analyze.loc[:, \"diff\"] = df_analyze.loc[:, [\"entry\", \"day_5\"]].diff(axis=1)[\n",
        "        \"day_5\"\n",
        "    ]\n",
        "    # 損益を計算します\n",
        "    df_analyze.loc[:, \"pl\"] = df_analyze.loc[:, \"diff\"] * df_analyze.loc[:, \"actual\"]\n",
        "    # リターンを計算します\n",
        "    df_analyze.loc[:, \"return\"] = (\n",
        "        (df_analyze.loc[:, \"day_5\"] / df_analyze.loc[:, \"entry\"]) - 1\n",
        "    ) * 100\n",
        "    # infを0.0に変換\n",
        "    df_analyze = df_analyze.replace(np.inf, 0.0)\n",
        "    # 処理結果を保存\n",
        "    dfs_analyze[i] = df_analyze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAFSesr9kpLt"
      },
      "source": [
        "　分析用データを表示して、うまく計算ができているかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRBTEocqkpLt"
      },
      "source": [
        "dfs_analyze[1].head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_jMB_Edo12"
      },
      "source": [
        "### 1. 銘柄毎の運用実績の分布をプロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsynnGJ3kpLt"
      },
      "source": [
        "　銘柄ごとのリターンの分布をヒストグラムでプロットします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtrZidGQkpLt",
        "scrolled": true
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(1, len(dfs_analyze), figsize=(8 * len(dfs_analyze), 8), sharex=True, sharey=True)\n",
        "\n",
        "# 戦略毎に処理\n",
        "for i, k in enumerate(dfs_analyze.keys()):\n",
        "    # 描画位置を指定\n",
        "    ax = axes[i]\n",
        "    # ヒストグラムをプロット\n",
        "    dfs_analyze[k].groupby([\"date\", \"Local Code\"])[\"return\"].sum().hist(bins=50, log=True, ax=ax)\n",
        "    # タイトルを設定\n",
        "    ax.set_title(f\"{k}: Weekly PL distribution\")\n",
        "# 描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egjKIla0do12"
      },
      "source": [
        "　ここからはあまりはっきりとした傾向はわかりませんが、1の戦略の分布が若干右側に広く大勝ちした銘柄を選択できている可能性が示唆されます。3の戦略はやはり銘柄個別で見ても分布が狭く、ボラティリティが低い銘柄を採用しているようです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bml9h0Sudo13"
      },
      "source": [
        "### 2. 週毎の勝ち銘柄率をプロット"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXJ3cvE3do13"
      },
      "source": [
        "　最後に週毎の銘柄の勝率を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPiQWkv-kpLu",
        "scrolled": false
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 8), sharex=True, sharey=True)\n",
        "\n",
        "# 統計量表示用\n",
        "buff = []\n",
        "# 戦略毎に処理\n",
        "for k in dfs_analyze.keys():\n",
        "    # 週毎の勝ち銘柄率を計算\n",
        "    win_ratio = (\n",
        "        dfs_analyze[k]\n",
        "        .set_index(\"date\")\n",
        "        .groupby(\"date\")\n",
        "        .apply(lambda x: (x.pl > 0).sum() / x.shape[0])\n",
        "        .to_frame()\n",
        "        .rename(columns={0: f\"{k}: win_ratio\"})\n",
        "    )\n",
        "    # プロット\n",
        "    win_ratio.plot(ax=ax)\n",
        "    # 統計量を保存\n",
        "    buff.append(win_ratio.describe().T)\n",
        "# ユニバースの勝ち銘柄率をプロット\n",
        "df_wp.groupby(df_wp.index).apply(lambda x: (x.universe > 0).sum() / x.shape[0]).rename(\n",
        "    \"universe\"\n",
        ").to_frame().plot(ax=ax, color=\"black\")\n",
        "# タイトルを設定\n",
        "ax.set_title(\"win ratio of selected stocks\")\n",
        "# グリッド表示\n",
        "ax.grid(True)\n",
        "# 0.5に基準線を描画\n",
        "ax.axhline(y=0.5, color=\"red\")\n",
        "#  描画\n",
        "plt.show()\n",
        "# 週毎の勝ち銘柄率の統計量\n",
        "display(pd.concat(buff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INUibbvLdo13"
      },
      "source": [
        "このプロットからいくつかおもしろいことがわかります。まず、1/2のストラテジーは6月から10月にかけて収益を上げていましたが、この時期に勝率がユニバースよりも高かったわけではないことがわかります。むしろ銘柄単位で見ると勝率はユニバースの平均よりも若干低い事がわかります。つまり、この期間に1/2のストラテジーは勝率で稼いだわけではなく、銘柄ごとのリターンが大きかったことが推測されます。 +\n",
        "これはどのような銘柄を選択していたかを調査することによって、どのあたりに収益性の厳選があるかを考えるヒントになります。このような傾向が観測されたらファクター分析などを利用して収益の厳選を確認する作業を実施するなどの発展が考えられます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yy4FE22do13"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPv2MbWhkpLu"
      },
      "source": [
        "## 8. 適時開示情報を使用して特別損失銘柄を除外"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKVClj1kpLu"
      },
      "source": [
        "```\n",
        "　strategy_idの4/5/6にtdnet.csv.gzを使用して特別損失を発表した銘柄を除外する戦略を実装しています。strategy_idに4/5/6を指定してバックテストを実施し、設定した仮説（特別損失開示による株価の下落を回避し、ポートフォリオの利益を向上できるとの仮説）が有効なのかどうかを検証します。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGODeyskpLv"
      },
      "source": [
        "　特別損失を発表した銘柄を除外してポートフォリオを組成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQhrRnd8kpLv"
      },
      "source": [
        "# 戦略毎に処理\n",
        "for strategy_id in tqdm([4, 5, 6]):\n",
        "    # ポートフォリオ組成\n",
        "    str_ret = ScoringService.predict(inputs, start_dt=start_dt, strategy_id=strategy_id)\n",
        "    # ポートフォリオを保存\n",
        "    with open(f\"chapter03-tutorial-02-backtest-{strategy_id}.csv\", mode=\"w\") as f:\n",
        "        f.write(str_ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-83JNajkpLv"
      },
      "source": [
        "バックテストを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-arMbOZkpLv"
      },
      "source": [
        "# 戦略毎に処理\n",
        "for strategy_id in tqdm([4, 5, 6]):\n",
        "    # ポートフォリオを読み込み\n",
        "    df_submit = Backtest.load_submit(f\"chapter03-tutorial-02-backtest-{strategy_id}.csv\")\n",
        "    # バックテスト実行\n",
        "    results[strategy_id], stocks[strategy_id] = Backtest.run(df_submit.loc[start_dt:], backtest_codes, backtest_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrhAziPedo14"
      },
      "source": [
        "### 改善を試みたバックテスト結果の考察"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY58XTlrkpLv"
      },
      "source": [
        "　ここではまずweek_returnの累積をプロットして効果が出ているかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ij2N3ApkpLv"
      },
      "source": [
        "# 描画領域を定義\n",
        "fig, axes = plt.subplots(1, 1, figsize=(20, 8), sharex=True, sharey=True)\n",
        "\n",
        "# 戦略毎に処理\n",
        "for k in results.keys():\n",
        "    # 描画位置を指定\n",
        "    ax = axes\n",
        "    # 戦略別の累積リターンを描画\n",
        "    results[k].set_index(\"date\").loc[:, [\"week_return\"]].rename(\n",
        "        columns={\"week_return\": f\"{k}: week_return\"}\n",
        "    ).cumsum().plot(ax=ax)\n",
        "\n",
        "# ユニバースの週次リターンの累積をプロット\n",
        "df_universe_return.cumsum().plot(ax=ax, color=\"black\", label=\"universe\")\n",
        "\n",
        "# 表示を調整\n",
        "ax.set_title(\"Cumulative week_return\")\n",
        "# グリッドを表示\n",
        "ax.grid(True)\n",
        "# 描画\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpQYNCDtkpLv"
      },
      "source": [
        "```\n",
        "　2及び5の戦略に注目すると、特別損失を発表した銘柄を除外するとポートフォリオの利益が上昇することもあることがわかります。ここでは、勝率やシャープレシオ等これまで実施してきた詳細な分析については実施していませんが、実際の戦略の決定には一つの評価指標のみを使用するのではなく、複数の評価指標を使用して総合的に判断することが重要です。前述の評価方法などを参考に各自で実施してみてください。\n",
        "\n",
        "　また、今回は適時開示情報の特別損失のみを使用していますが、それ以外にも株価に影響を与える開示項目があるかもしれません、上記のように仮説を立てて一つずつ効果を比較しながら検証することが重要になります。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dslSVNISdo15"
      },
      "source": [
        "### バックテストの総括"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5MS1xRKkpLw"
      },
      "source": [
        "```\n",
        "　まず、1及び2の戦略はユニバースと比較して高い収益性を実現できていました。最高値モデルと最安値モデルを足し合わせた1の戦略が、より安定的な収益を上げることを期待していましたが、シャープレシオなどを比較しても2の戦略の方が優れているため、最高値モデルと最安値モデルを足すよりも、現時点では最高値モデルを単体で利用するほうが良い結果が期待できそうです。\n",
        "\n",
        "　ただし、最高値モデルと最安値モデルの組み合わせ方が悪かった可能性もあります。というのも、2章で作成した予測モデルはあくまでスピアマンの順位相関を意識して作られたため、予測値の幅よりも順位を当てることを意識して設計されています。この場合、予測の幅が必ずしも実際の幅と一致する必要はありませんので、単純に足すよりもscikit-learnライブラリのStandardScaler( https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html )による標準化などを通して、一旦正規化を実施してから足すのがよいのではないかと考えられます。\n",
        "\n",
        "このようにモデルの構築と利用方法の組み合わせによって銘柄選択の戦略は決まりますので、是非様々な試行錯誤を実施してください。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQUa2VVkpLw"
      },
      "source": [
        "## 9. パッケージの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI08NwvWkpLw"
      },
      "source": [
        "```\n",
        "　ここまで、モデルの作成及び評価をしてきました。ここからは、投稿用のパッケージを作成します。ランタイム環境用のモデルは以下の構成である必要がありますので、まずは必要なディレクトリを作成していきます。また、適宜「3.8. 投稿用パッケージの作成」についてもご参照ください。\n",
        "\n",
        "[source]\n",
        "----\n",
        ".\n",
        "├── model                  必須 学習済モデルを置くディレクトリ\n",
        "│   └── ...\n",
        "├── src                    必須 Python のプログラムを置くディレクトリ\n",
        "│   ├── predictor.py       必須 最初のプログラムが呼び出すファイル\n",
        "│   └── ...                その他のファイル (ディレクトリ作成可能)\n",
        "└── requirements.txt       任意\n",
        "----\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iauvv96kdo16"
      },
      "source": [
        "### ライブラリバージョン調整"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnxq8needo16"
      },
      "source": [
        "```\n",
        "　本章で利用したモジュールのバージョンを調整するために、requirements.txtに以下を記載して保存します。\n",
        "\n",
        "[source]\n",
        "----\n",
        "joblib==1.0.1\n",
        "numpy==1.19.5\n",
        "pandas==1.1.5\n",
        "scikit-learn==0.20.3\n",
        "scipy==1.2.1\n",
        "seaborn==0.9.0\n",
        "----\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNMDzfHdo16"
      },
      "source": [
        "### ランタイム実行用クラス及び提出ファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtGtPKZKdo16"
      },
      "source": [
        "```\n",
        "　最後に作成したScoringServiceクラスをpredictor.pyに書き込み、Zip形式で圧縮することで提出可能なzipファイルが作成されます。\n",
        "\n",
        "Zipファイル作成例\n",
        "[source,bash]\n",
        "----\n",
        "$ ls\n",
        "model  requirements.txt  src\n",
        "$ zip -v submit.zip requirements.txt src/*.py model/*.pkl\n",
        "updating: requirements.txt\t(in=0) (out=0) (stored 0%)\n",
        "updating: src/predictor.py\t(in=11408) (out=2417) (deflated 79%)\n",
        "updating: model/my_model_label_high_20.pkl .\t(in=18919345) (out=5071005) (deflated 73%)\n",
        "updating: model/my_model_label_low_20.pkl .\t(in=18704305) (out=5006613) (deflated 73%)\n",
        "total bytes=37635058, compressed=10080035 -> 73% savings\n",
        "----\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhZPDSNMdo16"
      },
      "source": [
        "# 提出ファイルの保存先ディレクトリを指定します。\n",
        "if 'google.colab' in sys.modules:\n",
        "    output_path = f\"{mount_dir}/MyDrive/JPX_competition/Chapter04/\"\n",
        "else:\n",
        "    output_path = \".\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAlxAm75kpLw"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# 提出用パッケージ名\n",
        "package_file = \"chapter04-model.zip\"\n",
        "# パッケージファイルパス\n",
        "package_path = f\"{output_path}/{package_file}\"\n",
        "# archiveディレクトリへのパス\n",
        "archive_path = f\"{output_path}/archive\"\n",
        "# modelディレクトリへのパス\n",
        "model_path = f\"{archive_path}/model\"\n",
        "# srcディレクトリへのパス\n",
        "src_path = f\"{archive_path}/src\"\n",
        "\n",
        "# zipファイルを作成\n",
        "with zipfile.ZipFile(package_path, \"w\") as f:\n",
        "    # requirements.txt を追加\n",
        "    print(f\"[+] add {archive_path}/requirements.txt to requirements.txt\")\n",
        "    f.write(f\"{archive_path}/requirements.txt\", \"requirements.txt\")\n",
        "\n",
        "    # model/配下を追加\n",
        "    for root, dirs, files in os.walk(model_path):\n",
        "        for file in files:\n",
        "            add_path = os.path.join(root, file)\n",
        "            rel_path = os.path.relpath(\n",
        "                os.path.join(root, file),\n",
        "                os.path.join(model_path, '..')\n",
        "            )\n",
        "            print(f\"[+] add {add_path} to {rel_path}\")\n",
        "            f.write(add_path, rel_path)\n",
        "\n",
        "    # src/predictor.py を追加\n",
        "    print(f\"[+] add {src_path}/predictor.py to src/predictor.py\")\n",
        "    f.write(f\"{src_path}/predictor.py\", \"src/predictor.py\")\n",
        "\n",
        "print(f\"[+] please check {package_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ_P62ASdo17"
      },
      "source": [
        "　以上で投稿用のモデルパッケージは完成です。`chapter04-model.zip` ファイルをコンペティションページから投稿してリーダーボードに掲載されることを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRFeG4Vfdo17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}