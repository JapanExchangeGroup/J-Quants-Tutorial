{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "20210226_chapter05_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08NytJ1xL5-8"
      },
      "source": [
        "## チュートリアル説明\n",
        "\n",
        "本チュートリアルでは、最終的にニュースコーパスから特徴量を抽出することを目的とする。  \n",
        "各章では、特徴量を抽出するにあたって、使用しているデータがどのような特性を持つかを解析し、必要に応じて正規化し、可視化する方法を説明する。  \n",
        "最後の章では、自然語処理モデルであるBERT(Bidirectional Encoder Representations from Transformers)を用いて、特徴量を抽出する方法を説明する。また、抽出された特徴量をLSTMを用いて週ごとに合成する方法を説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1y20QBL5_C"
      },
      "source": [
        "## 1.3. 基本準備\n",
        "本章では、必要なライブラリーと分析用辞書などをインストールし、チュートリアルのコードを正しく利用できる環境を構築する。  \n",
        "また、本チュートリアルで利用するライブラリーを宣言し、その他、チュートリアルを進めるにあたって必要な基本的な準備を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPOT-ueXrwd"
      },
      "source": [
        "# Google Colab環境ではGoogle Driveをマウントしてアクセスできるようにします。\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Google Drive をマウントします\n",
        "    from google.colab import drive\n",
        "    mount_dir = \"/content/drive\"\n",
        "    drive.mount(mount_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME8m28W5L5_C"
      },
      "source": [
        "### 1.3.3. 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1iMGfaJL5_C",
        "scrolled": true
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential sudo mecab libmecab-dev mecab-ipadic-utf8 fonts-ipafont-gothic file\n",
        "!pip install --no-cache-dir pandas==1.1.5 numpy==1.19.5 scattertext==0.1.0.0 wordcloud==1.8.1 torch==1.7.1 torchvision==0.8.2 transformers==4.2.2 mecab-python3==0.996.6rc1 ipadic==1.0.0 neologdn==0.4 fugashi==1.0.5 japanize-matplotlib==1.1.3 gensim==3.8.3 pyLDAvis==2.1.2\n",
        "\n",
        "# mecab用の辞書をインストール\n",
        "!git clone https://github.com/neologd/mecab-ipadic-neologd.git --branch v0.0.7 --single-branch\n",
        "!yes yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdoDW0FSL5_D"
      },
      "source": [
        "### 1.3.4. ライブラリの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3IpBaeXL5_D"
      },
      "source": [
        "# 基本ライブラリ\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import string\n",
        "from copy import copy\n",
        "from glob import glob\n",
        "from itertools import chain\n",
        "import gc\n",
        "\n",
        "# テキスト解析関連\n",
        "import MeCab\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "# 可視化関連\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display, display_markdown, IFrame\n",
        "import scattertext as st\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "#PCA関連\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# ニューラルネット関連\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset as _Dataset\n",
        "\n",
        "# ノートブック上でpyLDAvisより可視化を行う場合の設定\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf1vuCv1L5_D"
      },
      "source": [
        "### 1.3.6. 実行環境の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrXsuolTL5_E"
      },
      "source": [
        "print(sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3v2Sc6HL5_E"
      },
      "source": [
        "### 1.3.7. ファイルパスの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QecMOQDGL5_E"
      },
      "source": [
        "# colab環境で実行する場合\n",
        "if 'google.colab' in sys.modules:\n",
        "    CONFIG = {\n",
        "        'base_path': f'{mount_dir}/MyDrive/JPX_competition/workspace',\n",
        "        'article_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/nikkei_article.csv.gz',\n",
        "        'stock_price_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/stock_price.csv.gz',\n",
        "        'stock_list_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/stock_list.csv.gz',\n",
        "        'dict_path': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
        "        'font_path': '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',\n",
        "    }\n",
        "else:\n",
        "    CONFIG = {\n",
        "        'base_path': '/notebook/workspace',\n",
        "        'article_path': '/notebook/data_dir_comp2/nikkei_article.csv.gz',\n",
        "        'stock_price_path': '/notebook/data_dir_comp2/stock_price.csv.gz',\n",
        "        'stock_list_path': '/notebook/data_dir_comp2/stock_list.csv.gz',\n",
        "        'dict_path': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
        "        'font_path': '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaymZFTwL5_F"
      },
      "source": [
        "### 1.3.8. ディレクトリ作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXoNOFDzL5_F"
      },
      "source": [
        "for store_dir in ['headline_features', 'keywords_features', 'visualizations']:\n",
        "    os.makedirs(os.path.join(CONFIG[\"base_path\"], store_dir), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVYcKf1GL5_F"
      },
      "source": [
        "### 1.3.9. テキスト解析用のtaggerをビルド"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzNP_MVFL5_F"
      },
      "source": [
        "# ニュースコーパスの分析のため、taggerをビルドする。\n",
        "# owakatiはテキストのトークナイズに使用され、ochasenは品詞情報を取得するため使用される。\n",
        "owakati = MeCab.Tagger(f\"-Owakati -d {CONFIG['dict_path']}\")\n",
        "ochasen = MeCab.Tagger(f\"-Ochasen -d {CONFIG['dict_path']}\")\n",
        "\n",
        "# taggerのparseを使うことで、各々の機能を確認することができます。\n",
        "text = 'taggerの役割を確認してみます。'\n",
        "print('owakati:\\n' + owakati.parse(text))\n",
        "print('ochasen:\\n' + ochasen.parse(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkP-cctCL5_G"
      },
      "source": [
        "### 1.3.10. 本番提出用のクラス作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWwxCX5kL5_G"
      },
      "source": [
        "# 本番提出用のクラスを作成するため、関数の持たない基本クラスを定義する\n",
        "# 下記で段階的にクラスを作り上げる\n",
        "class SentimentGenerator(object):\n",
        "    # 以下は使用時にビルドされる。\n",
        "    # 各々に関しては、以下のチュートリアルで説明する。\n",
        "    article_columns = None\n",
        "    punctuation_replace_dict = None\n",
        "    punctuation_remove_list = None\n",
        "    device = None\n",
        "    feature_extractor = None\n",
        "    headline_feature_combiner_handler = None\n",
        "    keywords_feature_combiner_handler = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTpBMV4RL5_G"
      },
      "source": [
        "## 1.4. データセットの読み込み\n",
        "本章では、使用する生データの中で利用可能な情報を把握し、それらの情報だけを取得することを目標とする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq-mf9_hL5_G"
      },
      "source": [
        "### 1.4.1. データの構成の把握"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8yuEpLtL5_H",
        "scrolled": true
      },
      "source": [
        "# 今回チュートリアルで用いる生データを確認してみる。\n",
        "articles = pd.read_csv(CONFIG['article_path'])\n",
        "display(articles.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aXEx6wGL5_H"
      },
      "source": [
        "# key: column, value: 重複を排除したデータ件数のdict型宣言\n",
        "n_unique = {}\n",
        "for column in articles.columns:\n",
        "    # column毎の重複を排除したデータ件数をn_uniqueに追加する。\n",
        "    n_unique[column] = len(articles[column].unique())\n",
        "\n",
        "display_markdown('Number of unique data', raw=True)\n",
        "\n",
        "# n_uniqueをpd.Series形式に変換する。articles.dtypesよりcolumnごとのdtypeを取得\n",
        "# これらをpd.concatによってテーブル形式に変換し、表示\n",
        "display(pd.concat([pd.Series(n_unique).rename('n_unique'), articles.dtypes.rename('dtype')], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlTPYE4fL5_I"
      },
      "source": [
        "# headlineとkeywordsはニュースの固有情報を含んでいる模様。\n",
        "# classificationsはその記事の分類、sector情報などが含まれているが、これらはコーパス分析やその特徴量として用いることに適していない。\n",
        "# company_g.stock_codeは関連する株の情報を含むが、これらはコーパス分析やその特徴量として用いることに適していない。\n",
        "# classificationsやcompany_g.stock_codeは、モデル学習時のオブジェクト設定や、学習データの生成時のフィルターなどで用いることが望ましい。\n",
        "# 上記から、本チュートリアルでは、headlineとkeywordsを用いて、分析及び特徴量抽出を行う。\n",
        "\n",
        "# 表示するcolumnを定義する。\n",
        "columns = ['headline', 'keywords', 'classifications', 'company_g.stock_code']\n",
        "for column in columns:\n",
        "    display_markdown(f'#### {column}', raw=True)\n",
        "\n",
        "    # 欠損値が含まれることがあるため、どちらかに欠損値が存在するデータを除去する。\n",
        "    # 同様のindexを持つ5個のサンプルデータを表示。\n",
        "    display(articles[columns].dropna()[column].head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Wxw15YL5_I"
      },
      "source": [
        "# 二つのデータheadline, keywordsとその時刻の情報をもつpublish_datetimeを用いて分析を進め、最終的に、それらのテキストが持つ特徴量を抽出する。\n",
        "articles = articles[['publish_datetime', 'headline', 'keywords']]\n",
        "# display(articles.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8pK_FhXL5_I",
        "scrolled": true
      },
      "source": [
        "# ニュースが公開された時刻を維持するため、'publish_datetime'をindexとしてセット\n",
        "articles = articles.set_index('publish_datetime')\n",
        "\n",
        "# str形式のdatetimeをpd.Timestamp形式に変換\n",
        "articles.index = pd.to_datetime(articles.index)\n",
        "# display(articles.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4EnyKF4L5_J"
      },
      "source": [
        "# 生データから使用するコラムを設定する\n",
        "SentimentGenerator.article_columns = ['publish_datetime', 'headline', 'keywords']\n",
        "\n",
        "# 上記のコードを用いて、本番提出用のクラスにclassmethodを追加\n",
        "@classmethod\n",
        "def load_articles(cls, path, start_dt=None, end_dt=None):\n",
        "    # csvをロードする\n",
        "    # headline、keywordsをcolumnとして使用。publish_datetimeをindexとして使用。\n",
        "    articles =  pd.read_csv(path)[cls.article_columns].set_index('publish_datetime')\n",
        "\n",
        "    # str形式のdatetimeをpd.Timestamp形式に変換\n",
        "    articles.index = pd.to_datetime(articles.index)\n",
        "\n",
        "    # 必要な場合、使用するデータの範囲を指定する\n",
        "    return articles[start_dt:end_dt]\n",
        "\n",
        "\n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.load_articles = load_articles\n",
        "\n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-XK0KLRL5_J"
      },
      "source": [
        "## 1.5. データセットの前処理\n",
        "日本語のテキストは半角と全角が混在して使われる場合がある。\n",
        "同じテキスト内において、同じ単語が半角と全角で混在して使われる場合、人はそれらは単語が同様のであることを認識できるが、モデルにおいては、異なる別の単語として認識される。 更に、半角と全角が混在する単語が学習時に観察していないOut-of-Vocabularyの単語である場合は、その意味が失われ、されにテキスト全体の意味が崩れることにもなり得る。 このような問題を防止するため、テキストの正規化が用いられる。  \n",
        "本章では、使用するテキストがどのよう文字や記号で構成されていて、どのような期待していない文字や記号などを含むかを調べる。  \n",
        "更に、それらの期待していない情報を置換や取り除くなどの正規化を行う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBKLi7HJL5_J"
      },
      "source": [
        "### 1.5.1. 欠損値除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mhTUM8ZL5_J"
      },
      "source": [
        "# 欠損値の確認は、そのデータを理解する上で一番基本的な方法である。連続したデータの中で欠損値が存在する場合は補正を行う場合があるが、こちらのデータには適していない。\n",
        "# 欠損値が存在する場合、そのことに留意し、NaN値を取り除くなどの作業が必要となる。\n",
        "# 本チュートリアルでは、マケットの推定にheadline及びkeywords両方の特徴量を同時に用いる。よって、片方が存在しない場合は、無意味なデータとみなし除去する。\n",
        "\n",
        "# keywordsはデータに欠損がある\n",
        "articles.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPBfRfEXL5_K",
        "scrolled": true
      },
      "source": [
        "# 欠損値を取り除く\n",
        "articles = articles.dropna()\n",
        "articles.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu-sSErpL5_K"
      },
      "source": [
        "### 1.5.2. 全角文字の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBQ_db3RL5_K",
        "scrolled": true
      },
      "source": [
        "# 全角スペース、全角アルファベット、全角数字が含まれているかを確認する\n",
        "for column in articles.columns:\n",
        "    for check_target in [\"\\u3000\", r\"[Ａ-Ｚａ-ｚ]\", r\"[０-９]\"]:\n",
        "        display(f'Coulmn: {column}, Contains {check_target}: {articles[column].str.contains(check_target).any()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-djnnvqYL5_K"
      },
      "source": [
        "# Escape codeが含まれているかを確認する。\n",
        "for column in articles.columns:\n",
        "    for check_target in [\"\\t\", \"\\n\"]:\n",
        "        display(f'Coulmn: {column}, Contains {check_target}: {articles[column].str.contains(check_target).any()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VUgqU9L5_K"
      },
      "source": [
        "# 全角スペースを持つケースを表示\n",
        "display(articles['headline'][articles['headline'].str.contains('\\u3000')][0])\n",
        "display(articles['keywords'][articles['keywords'].str.contains('\\u3000')][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sUa32JbL5_L"
      },
      "source": [
        "# 全角の英字が含まれケースを表示\n",
        "display(articles['headline'][articles['headline'].str.contains(r\"[Ａ-Ｚａ-ｚ]\")][0])\n",
        "display(articles['keywords'][articles['keywords'].str.contains(r\"[Ａ-Ｚａ-ｚ]\")][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXgERfu6L5_L"
      },
      "source": [
        "# 全角の数字が含まれケースを表示\n",
        "display(articles['headline'][articles['headline'].str.contains(r\"[０-９]\")][0])\n",
        "display(articles['keywords'][articles['keywords'].str.contains(r\"[０-９]\")][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdgLyMHgL5_L"
      },
      "source": [
        "### 1.5.3. neologdnによる汎用テキスト正規化\n",
        "neologdnを用いて、全角英字、数字や、半角カタカナ、一部記号などのを辞書中にマッチできるよう正規化を行う。  \n",
        "neologdnの正規化規則に関して詳しい情報が必要な場合は次を参照: https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEOSqMpCL5_M"
      },
      "source": [
        "# 以下のテキストをneologdnより正規化する。\n",
        "text = '全角アルファベット:Ａ, 全角数字:０, 全角スペース:　, 半角カナ:ｱ'\n",
        "\n",
        "# 正規化前を確認する。\n",
        "display(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-lhYVTIL5_N"
      },
      "source": [
        "# 正規化後を確認する\n",
        "display(neologdn.normalize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TCy2LWFL5_N"
      },
      "source": [
        "for column in articles.columns:\n",
        "    # スペース(半角スペースを除く、全角スペースやescape codeを含む)はneologdn正規化時に全て除去される。\n",
        "    # ここでは、スペースの情報が失われないように、スペースを全て改行に書き換え、正規化後スペースに再変換する。\n",
        "    articles[column] = articles[column].apply(lambda x: '\\n'.join(x.split()))\n",
        "\n",
        "    # neologdnを使って正規化を行う。\n",
        "    articles[column] = articles[column].apply(lambda x: neologdn.normalize(x))\n",
        "    \n",
        "    # 改行をスペースに置換する。\n",
        "    articles[column] = articles[column].str.replace('\\n', ' ')\n",
        "\n",
        "# ニュース記事の変換前を確認する。\n",
        "display(articles.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPs_wsEL5_O"
      },
      "source": [
        "# 上記のコードを用いて、本番提出用のクラスにclassmethodを追加\n",
        "@classmethod\n",
        "def normalize_articles(cls, articles):\n",
        "    articles = articles.copy()\n",
        "\n",
        "    # 欠損値を取り除く\n",
        "    articles = articles.dropna()\n",
        "\n",
        "    for column in articles.columns:\n",
        "        # スペース(全角スペースを含む)はneologdn正規化時に全て除去される。\n",
        "        # ここでは、スペースの情報が失われないように、スペースを全て改行に書き換え、正規化後スペースに再変換する。\n",
        "        articles[column] = articles[column].apply(lambda x: '\\n'.join(x.split()))\n",
        "\n",
        "        # neologdnを使って正規化を行う。\n",
        "        articles[column] = articles[column].apply(lambda x: neologdn.normalize(x))\n",
        "\n",
        "        # 改行をスペースに置換する。\n",
        "        articles[column] = articles[column].str.replace('\\n', ' ')\n",
        "        \n",
        "    return articles\n",
        "\n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.normalize_articles = normalize_articles\n",
        "\n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])\n",
        "articles = SentimentGenerator.normalize_articles(articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av5UvhxgL5_O"
      },
      "source": [
        "### 1.5.4. テキスト内の記号情報の扱い\n",
        "テキスト内には、少なからず記号が使われる場合があるが、希少な記号や正しくない記号の使い方は、モデルがそのテキストを理解する上でのノイズとなり得る。\n",
        "本章では、コーパス全体に含まれる記号を取得し、それらの記号がどのように使われているかを確認する。また、意味薄い希少な記号は取り除き、正規化の必要な記号は置換する作業を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSAzY4ZLL5_O"
      },
      "source": [
        "# 記号の情報を取得するにあたって、ochasenを用いてテキストから品詞情報を取得してみる。\n",
        "def parse_by_ochasen(tagger, text):\n",
        "    # Ochasenでmecab-ipadic-neologdの辞書を使ったときの、返り値のデータ順は以下となる。\n",
        "    columns = ['表層形', '発音', '原型', '形態素の品詞型', '活用形', '活用型']\n",
        "\n",
        "    # Ochasenよりコーパスタグ付けを行う。\n",
        "    parsed = [item.split('\\t') for item in tagger.parse(text).split(\"\\n\") if item not in ('EOS', '')]\n",
        "    return pd.DataFrame(parsed, columns=columns)\n",
        "\n",
        "# 形態素の品詞型より品詞情報を確認すると、記号が記号として正しく認識されていることがわかる。\n",
        "text = 'テストテキストです。「記号を探してみます!」'\n",
        "parsed = parse_by_ochasen(tagger=ochasen, text=text)\n",
        "display(parsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_-4tz4JL5_O"
      },
      "source": [
        "# 記号として扱う品詞型を定義する。MeCabでは、記号の一部が 'サ変接続'に含まれることがある。\n",
        "# 'サ変接続'の中には、ひらがな、カタカナを含む記号でないものもあるので、それらは、punctuationとして扱わない。\n",
        "# 以下のflagに該当する品詞型を持つものだけを取得する。\n",
        "flags = [\"記号\", \"サ変接続\"]\n",
        "\n",
        "# ひらがな、カタカナ、漢字、アルファベットを含まない、記号を全て取得。\n",
        "punctuation_candidate = parsed['表層形'][parsed['形態素の品詞型'].apply(lambda x: any([flag in x for flag in flags]))]\n",
        "punctuations = punctuation_candidate[~punctuation_candidate.str.contains(r\"[一-龯ぁ-んァ-ンA-Za-z々ゝゞヽヾヴヵヶ]\")]\n",
        "punctuations = set(punctuations)\n",
        "\n",
        "display(punctuations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RquKpm_qL5_P"
      },
      "source": [
        "# 複合記号から単一記号を抽出し、集合に追加する。\n",
        "for punctuation in punctuations:\n",
        "    punctuations = punctuations | set(punctuation)\n",
        "    \n",
        "display(set(punctuations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmH4NZ_BL5_P"
      },
      "source": [
        "# コーパス全体から記号を取得するコードを作成\n",
        "def build_punctuations(tagger, texts, flags = [\"記号\", \"サ変接続\"]):\n",
        "    gc.collect()\n",
        "    # textsがpd.Seriesでない時に、pd.Seriesに変換\n",
        "    if isinstance(texts, pd.Series) is False:\n",
        "        texts = pd.Series(texts)\n",
        "\n",
        "    punctuations = set()\n",
        "\n",
        "    for text in tqdm(texts):\n",
        "        # Ochasenより単語の品詞情報を取得\n",
        "        parsed = parse_by_ochasen(tagger=tagger, text=text)\n",
        "\n",
        "        # ひらがな、カタカナ、漢字、アルファベットを含まない、記号を全て取得\n",
        "        punctuation_candidate = parsed['表層形'][parsed['形態素の品詞型'].apply(lambda x: any([flag in x.split('-') for flag in flags]))]\n",
        "        punctuation_candidate = punctuation_candidate[~punctuation_candidate.str.contains(r\"[一-龯ぁ-んァ-ンA-Za-z々ゝゞヽヾヴヵヶ]\")]\n",
        "        punctuations = punctuations | set(punctuation_candidate.tolist())\n",
        "\n",
        "    # 複合記号から単一記号を抽出\n",
        "    for punctuation in punctuations:\n",
        "        punctuations = punctuations | set(punctuation)\n",
        "\n",
        "    return punctuations\n",
        "\n",
        "headline_punctuations = build_punctuations(tagger=ochasen, texts=articles['headline'])\n",
        "keywords_punctuations = build_punctuations(tagger=ochasen, texts=articles['keywords'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNbCQIhHL5_P",
        "scrolled": true
      },
      "source": [
        "# headlineから、これら記号を含むテキストを表示\n",
        "for punctuation in sorted(headline_punctuations, key=lambda x: len(x)):\n",
        "    print(f\"punctuation: {punctuation}\\n\", articles['headline'][articles['headline'].apply(lambda x: punctuation in x)][0], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFQaN5nTL5_Q"
      },
      "source": [
        "# keywordsから、これら記号を含むテキストを表示\n",
        "for punctuation in sorted(keywords_punctuations, key=lambda x: len(x)):\n",
        "    print(f\"punctuation: {punctuation}\\n\", articles['keywords'][articles['keywords'].apply(lambda x: punctuation in x)][0], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHnlphZML5_Q"
      },
      "source": [
        "# 上記の観察から、あまり意味の保たない星や音符などの記号は取り除く。見栄えを良くするため乱用に使われている括弧は置換するなど正規化を行う。\n",
        "# また、多くの機種依存文字が記号が観測されているが、unicode正規化ライブラリーを用いて正規化を行う。\n",
        "\n",
        "# 機種依存文字の第一水準、第二水準漢字に関しては、名前などに多く使われる一部の漢字を以下で定義し、置換を行う。\n",
        "JISx0208_replace_dict = {\n",
        "    '髙': \"高\",\n",
        "    '﨑': \"崎\",\n",
        "    '濵': \"浜\",\n",
        "    '賴': \"頼\",\n",
        "    '瀨': \"瀬\",\n",
        "    '德': \"徳\",\n",
        "    '蓜': \"配\",\n",
        "    '昻': \"昂\",\n",
        "    '桒': \"桑\",\n",
        "    '栁': \"柳\",\n",
        "    '犾': \"犹\",\n",
        "    '琪': \"棋\",\n",
        "    '裵': \"裴\",\n",
        "    '魲': \"鱸\",\n",
        "    '羽': \"羽\",\n",
        "    '焏': \"丞\",\n",
        "    '祥': \"祥\",\n",
        "    '曻': \"昇\",\n",
        "    '敎': \"教\",\n",
        "    '澈': \"徹\",\n",
        "    '曺': \"曹\",\n",
        "    '黑': \"黒\",\n",
        "    '塚': \"塚\",\n",
        "    '閒': \"間\",\n",
        "    '彅': \"薙\",\n",
        "    '匤': \"匡\",\n",
        "    '冝': \"宜\",\n",
        "    '埇': \"甬\",\n",
        "    '鮏': \"鮭\",\n",
        "    '伹': \"但\",\n",
        "    '杦': \"杉\",\n",
        "    '罇': \"樽\",\n",
        "    '柀': \"披\",\n",
        "    '﨤': \"返\",\n",
        "    '寬': \"寛\",\n",
        "    '神': \"神\",\n",
        "    '福': \"福\",\n",
        "    '礼': \"礼\",\n",
        "    '贒': \"賢\",\n",
        "    '逸': \"逸\",\n",
        "    '隆': \"隆\",\n",
        "    '靑': \"青\",\n",
        "    '飯': \"飯\",\n",
        "    '飼': \"飼\",\n",
        "    '緖': \"緒\",\n",
        "    '埈': \"峻\",\n",
        "}\n",
        "\n",
        "# 置換すべき記号のdictionaryを作成する\n",
        "punctuation_replace_dict = {\n",
        "    **JISx0208_replace_dict,\n",
        "    '《': '〈',\n",
        "     '》': '〉',\n",
        "     '『': '「',\n",
        "     '』': '」',\n",
        "     '“': '\"',\n",
        "     '!!': '!',\n",
        "     '〔': '[',\n",
        "     '〕': ']',\n",
        "     'χ': 'x'\n",
        "}\n",
        "\n",
        "# あまり意味を持たない記号のリストを作成し、これらを下記のコードで取り除く\n",
        "punctuation_remove_list = ['|', '■', '◆', '●', '★', '☆', '♪', '〃', '△', '○', '□']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NgRCQpauL5_Q"
      },
      "source": [
        "# 置換するために定義した機種依存文字の第一水準、第二水準漢字がデータの中に含まれているかチェックします。\n",
        "# 観測された漢字をstoreするためのsetを定義\n",
        "catched_replace_targets = set()\n",
        "for column in articles.columns:\n",
        "    display_markdown(f'column: {column}', raw=True)\n",
        "    \n",
        "    # 定義した置換すべき漢字がデータに含まれているかをチェック\n",
        "    for key in JISx0208_replace_dict.keys():\n",
        "        \n",
        "        # articles[column]にその漢字が含まれている場合、catched_replace_targetsに追加する。\n",
        "        if articles[column].str.contains(key).any():\n",
        "            catched_replace_targets.update(key)\n",
        "            \n",
        "    display(catched_replace_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xO_E4bQlL5_Q"
      },
      "source": [
        "# 機種依存文字の丸囲みの数字、ローマ数字、単位、省略文字などは、unicodedataライブラリーを用いたunicode正規化より置換を行う。\n",
        "# 正規化形式に関しては本章ではNFKC(Normalization Form Compatibility Composition)を用いる。\n",
        "\n",
        "# unicodedata.normalize関数より、unicode正規化を行う。\n",
        "text = '丸囲みの数字:①, ローマ数字:Ⅷ, 単位:㎜㍉, 省略文字:㈱'\n",
        "unicodedata.normalize('NFKC', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn5Pd8g8L5_R"
      },
      "source": [
        "for column in articles.columns:\n",
        "    # punctuation_remove_listに含まれる記号を除去する\n",
        "    articles[column] = articles[column].str.replace(fr\"[{''.join(punctuation_remove_list)}]\", '')\n",
        "    \n",
        "    # punctuation_replace_dictに含まれる記号を置換する\n",
        "    for replace_base, replace_target in punctuation_replace_dict.items():\n",
        "        articles[column] = articles[column].str.replace(replace_base, replace_target)\n",
        "                                                    \n",
        "    # unicode正規化を行う\n",
        "    articles[column] = articles[column].apply(lambda x: unicodedata.normalize('NFKC', x))\n",
        "\n",
        "# 精製後確認\n",
        "articles.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqSBXRu2L5_R"
      },
      "source": [
        "# 上で作成した記号置換用のdictと記号削除用のリストをhandle_punctuations_in_articles関数内で使用するため、SentimentGeneratorに追加\n",
        "SentimentGenerator.punctuation_remove_list = punctuation_remove_list\n",
        "SentimentGenerator.punctuation_replace_dict = punctuation_replace_dict\n",
        "\n",
        "# 上記のコードを用いて、本番提出用のクラスにclassmethodを追加\n",
        "@classmethod\n",
        "def handle_punctuations_in_articles(cls, articles):\n",
        "    articles = articles.copy()\n",
        "\n",
        "    for column in articles.columns:\n",
        "        # punctuation_remove_listに含まれる記号を除去する\n",
        "        articles[column] = articles[column].str.replace(fr\"[{''.join(cls.punctuation_remove_list)}]\", '')\n",
        "\n",
        "        # punctuation_replace_dictに含まれる記号を置換する\n",
        "        for replace_base, replace_target in cls.punctuation_replace_dict.items():\n",
        "            articles[column] = articles[column].str.replace(replace_base, replace_target)\n",
        "\n",
        "        # unicode正規化を行う\n",
        "        articles[column] = articles[column].apply(lambda x: unicodedata.normalize('NFKC', x))\n",
        "\n",
        "    return articles\n",
        "    \n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.handle_punctuations_in_articles = handle_punctuations_in_articles\n",
        "                                                        \n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])\n",
        "articles = SentimentGenerator.normalize_articles(articles)\n",
        "articles = SentimentGenerator.handle_punctuations_in_articles(articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaU26mQKL5_R"
      },
      "source": [
        "## 1.6. データセットの可視化\n",
        "データセットの解析及び可視化は、データ自体の特性を理解する上で役に立つだけでなく、期待していない情報を含んでいるかをわかる上でも役に立つ。本章では、扱っているコーパスがどのような特性を持つかを、品詞情報や単語の頻度により解析及び可視化を行い、観察結果に応じてデータセットに追加的な処理を行う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoXYGUiRL5_R"
      },
      "source": [
        "### 1.6.1. 品詞情報取得\n",
        "下記の続く解析で、多くのコードが品詞情報を要する。しかし、品詞情報の取得には少なからずの時間が所要されるため、一度コーパス全体において品詞情報を取得しておく戦略を取る。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ3QQo0qL5_R",
        "scrolled": true
      },
      "source": [
        "# コーパス全体をochasenによってparseする。\n",
        "# メモリー節約のため、解析で使用する['表層形', '形態素の品詞型']の情報のみを残す。\n",
        "parsed_headline_by_ochasen = articles['headline'].apply(lambda x: parse_by_ochasen(tagger=ochasen, text=x)[['表層形', '形態素の品詞型']])\n",
        "parsed_keywords_by_ochasen = articles['keywords'].apply(lambda x: parse_by_ochasen(tagger=ochasen, text=x)[['表層形', '形態素の品詞型']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qrp_XUuL5_R"
      },
      "source": [
        "display(parsed_headline_by_ochasen.head())\n",
        "display(parsed_headline_by_ochasen[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h5ENSX3L5_R"
      },
      "source": [
        "display(parsed_keywords_by_ochasen.head())\n",
        "display(parsed_keywords_by_ochasen[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKK-XxCvL5_S"
      },
      "source": [
        "### 1.6.2. 品詞情報解析\n",
        "本章ではコーパス全体における品詞の分布を解析する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6AncrHNL5_S"
      },
      "source": [
        "# コーパス全体の品詞分布を調べる前に先立ち、単一テキストから品詞分布を取得してみよう。\n",
        "text = 'テストテキスト。コーパスの品詞情報を取得します。'\n",
        "\n",
        "# 品詞情報を取得するため、ochasenを用いる\n",
        "parsed = parse_by_ochasen(tagger=ochasen, text=text)\n",
        "parsed['形態素の品詞型']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n28UIEVTL5_S"
      },
      "source": [
        "# 形態素の品詞型は多重のクラスとして成り立っている、ここでは、そのうち一番大きいくくりだけを取得する。\n",
        "word_classes = parsed['形態素の品詞型'].apply(lambda x: x.split('-')[0])\n",
        "word_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAUwLHOaL5_S"
      },
      "source": [
        "# 品詞ごとの数をカウントする。\n",
        "word_classes.groupby(word_classes).count().to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vALzIinbL5_S"
      },
      "source": [
        "# テキストごとの品詞カウントを累積し、コーパス全体の品詞カウントを得るため、カウントをdictionaryにアップデートする\n",
        "count_of_word_classes = {}\n",
        "# key: 品詞型, value: カウント数\n",
        "for key, value in word_classes.groupby(word_classes).count().to_dict().items():\n",
        "    # dictionaryに品詞が存在しない場合、新しく追加する。\n",
        "    if key not in count_of_word_classes:\n",
        "        count_of_word_classes[key] = value\n",
        "\n",
        "    # dictionaryに品詞が存在する場合、既存のカウントに観察されたカウントを足す。\n",
        "    else:\n",
        "        count_of_word_classes[key] += value\n",
        "\n",
        "# 確認する\n",
        "count_of_word_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yqu2xiyL5_S"
      },
      "source": [
        "# 上のコードをまとめてコーパス全体から品詞情報を取得する関数を作成。\n",
        "# コーパスの入力ではなく、時間短縮のため、すでにochasenよりparseされた情報を用いる。\n",
        "def get_count_of_word_classes(parsed_by_ochasen):\n",
        "    gc.collect()\n",
        "\n",
        "    count_of_word_classes = {}\n",
        "\n",
        "    for parsed in tqdm(parsed_by_ochasen):\n",
        "        # 一番大きいくくりの品詞型だけを取得する。\n",
        "        word_classes = parsed['形態素の品詞型'].apply(lambda x: x.split('-')[0])\n",
        "\n",
        "        # 単一テキストの品詞カウントをdictionaryにアップデート\n",
        "        # key: 品詞型, value: カウント数\n",
        "        for key, value in word_classes.groupby(word_classes).count().to_dict().items():\n",
        "            # dictionaryに品詞が存在しない場合、新しく追加する。\n",
        "            if key not in count_of_word_classes:\n",
        "                count_of_word_classes[key] = value\n",
        "\n",
        "            # dictionaryに品詞が存在する場合、既存のカウントに観察されたカウントを足す。\n",
        "            else:\n",
        "                count_of_word_classes[key] += value\n",
        "\n",
        "    return pd.Series(count_of_word_classes).sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv5Q3FolL5_S",
        "scrolled": true
      },
      "source": [
        "# headline, keywords各々のコーパスから品詞情報を取得する。\n",
        "headline_count_of_word_classes = get_count_of_word_classes(parsed_by_ochasen=parsed_headline_by_ochasen)\n",
        "keywords_count_of_word_classes = get_count_of_word_classes(parsed_by_ochasen=parsed_keywords_by_ochasen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKas_rV_L5_S"
      },
      "source": [
        "# headlineとkeywordsの品詞の分布がかなり違っていることからそれらの特性を理解することができる。\n",
        "count_of_word_classes_df = pd.concat([headline_count_of_word_classes.rename('headline'), keywords_count_of_word_classes.rename('keywords')], axis=1, sort=True)\n",
        "count_of_word_classes_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8qXHg5CL5_T",
        "scrolled": false
      },
      "source": [
        "# 比較のため、各々の単語数で各々を割る\n",
        "normalized_count_of_word_classes_df = count_of_word_classes_df / count_of_word_classes_df.sum()\n",
        "\n",
        "display(normalized_count_of_word_classes_df.applymap('{:,.6f}'.format))\n",
        "\n",
        "# keywordsは名詞の割合が98%以上であり、headlineでも高い割合を占める。\n",
        "# headlineは、名詞意外にも助詞、動詞の割合が高く、高い割合のテキストが文章構造を構築していると想定される。\n",
        "# keywordsは、名詞が割合が相当高い。コラム名どおり、関連するトピックなどのような名詞情報が含まれていると想定されるが、その他の品詞情報を含まれていることから、少ない数だけれど、文章構造に近いキーワードも入っていることが想定される。\n",
        "# headlineには記号がかなり含まれていることが観察されるが、記号に対する正規化や削除の前処理を追加的に行うことも考えられる。その際に有意味な情報を持つ記号を削除しないように注意。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80d-pVulL5_T",
        "scrolled": false
      },
      "source": [
        "# 上記のテーブルだけでは各々の違いを理解し難いので、可視化を行う。\n",
        "# データの不均等が激しいため、truncated barplotとして可視化する\n",
        "\n",
        "# 上下切断表示のため、縦二つのaxesを用意する。\n",
        "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(12, 5))\n",
        "\n",
        "# 上下両方のaxesに同様のプロットを行う。\n",
        "normalized_count_of_word_classes_df.sort_values('headline').plot(kind='bar', ax=ax[0])\n",
        "normalized_count_of_word_classes_df.sort_values('headline').plot(kind='bar', ax=ax[1])\n",
        "\n",
        "# 上下両方のy軸範囲を設定する。\n",
        "ax[0].set_ylim((normalized_count_of_word_classes_df).max().max() - 0.9, 1)\n",
        "ax[1].set_ylim(0, 0.03)\n",
        "\n",
        "# 上のaxesでは、bottom部分表示(ticker, labelなど)、下のaxesでは、top部分表示を除去する。\n",
        "ax[0].spines['bottom'].set_visible(False)\n",
        "ax[1].spines['top'].set_visible(False)\n",
        "\n",
        "ax[0].xaxis.tick_top()\n",
        "ax[0].tick_params(labeltop=False) \n",
        "ax[1].xaxis.tick_bottom()\n",
        "\n",
        "# プロット切断部に点線表示を行う。\n",
        "d = 0.01  \n",
        "kwargs = dict(transform=ax[0].transAxes, color='k',linestyle=':', lw=1, clip_on=False)\n",
        "ax[0].plot((-d, 1+d), (0, 0), **kwargs)    \n",
        "\n",
        "kwargs.update(transform=ax[1].transAxes)  \n",
        "ax[1].plot((-d, 1+d), (1, 1), **kwargs)\n",
        "\n",
        "# 上のaxesだけにlegendを表示するため、下のaxesではlegendを除去する。\n",
        "ax[1].legend().remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJN7ai41L5_T"
      },
      "source": [
        "### 1.6.3. 頻度解析\n",
        "一つのテキストや、コーパス全体を可視化する方法には、主にそのコーパスに含まれる単語や形態素の頻度による可視化がある。\n",
        "ここではwordcloudとscatter textを用いたコーパス全体における単語の頻度の情報を可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKZ1VLZ9L5_T"
      },
      "source": [
        "# 単語の可視化を行う前に当たって、どのコーパスにおいても多く登場するが、あまり意味を持たず、分析において役に立たない不用語(stopwords)を取得し、可視化からそれらを排除する。\n",
        "# 本解析では、名詞、動詞、形容詞、副詞以外を不用語として扱う。\n",
        "# 名詞である場合でも、数の情報を含む場合は不用語として扱う。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcAMm5HGL5_T"
      },
      "source": [
        "# build_punctuationsを少し変更し、stopwordsを取得するコードを作成する\n",
        "# コード内のひとつの違いは、non_skip_flagsのハンドルが追加されていることである。\n",
        "# 仮に品詞情報に数が含まれる場合、これらをstopwordsとして扱う。\n",
        "# コーパスの入力ではなく、時間短縮のため、すでにochasenよりparseされた情報を用いる。\n",
        "\n",
        "def build_stopwords(parsed_by_ochasen, non_skip_flags=[\"数\"], skip_flags = [\"名詞\", \"動詞\", \"形容詞\", \"副詞\"]):\n",
        "    gc.collect()\n",
        "    stopwords = set()\n",
        "    \n",
        "    for parsed in tqdm(parsed_by_ochasen):\n",
        "        # non_skip_flagsが入っているものは全てstopwordsとして扱う\n",
        "        # それ以外において、skip_flagsをひとつでも含んでいないものをstopwordsとして扱う\n",
        "        mask_include_non_skip_flags = parsed['形態素の品詞型'].apply(lambda x: any([non_skip_flag in x.split('-') for non_skip_flag in non_skip_flags]))\n",
        "        mask_exclude_skip_flags = parsed['形態素の品詞型'].apply(lambda x: not any([skip_flag in x.split('-') for skip_flag in skip_flags]))\n",
        "\n",
        "        #日、月、年度のようなユニット情報を含むものは全てstopwordsとして扱う\n",
        "        mask_include_unit_info = parsed['表層形'].apply(lambda x: False if re.fullmatch(r'\\d+(秒|分|時|日|月|カ月|年|人|ドル|円)', x) is None else True)\n",
        "\n",
        "        stopword_candidate = parsed['表層形'][mask_include_non_skip_flags | mask_exclude_skip_flags | mask_include_unit_info]\n",
        "        \n",
        "        # stopwordsセットにアップデートする。\n",
        "        stopwords = stopwords | set(stopword_candidate.tolist())\n",
        "        \n",
        "    # 追加的に単一アルファベットをstopwordsとして追加する\n",
        "    stopwords = stopwords | set(string.ascii_lowercase) | set(string.ascii_uppercase)\n",
        "\n",
        "    # 追加的に単一数字をstopwordsとして追加する\n",
        "    stopwords = stopwords | set([str(idx) for idx in range(10)])\n",
        "    \n",
        "    return stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbhXPxdL5_T",
        "scrolled": true
      },
      "source": [
        "headline_stopwords = build_stopwords(parsed_by_ochasen=parsed_headline_by_ochasen)\n",
        "keywords_stopwords = build_stopwords(parsed_by_ochasen=parsed_keywords_by_ochasen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYeCRUbVL5_T"
      },
      "source": [
        "stopwords = headline_stopwords | keywords_stopwords\n",
        "\n",
        "# 表示のため、リストに変換する。\n",
        "stopwords_list = sorted(stopwords)\n",
        "\n",
        "# 異なる特性を持つ色んなstopwordsを表示するためランダムにシャッフルする。\n",
        "random.Random(0).shuffle(stopwords_list)\n",
        "print(stopwords_list[:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFTkP043L5_T"
      },
      "source": [
        "#### wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7-BmIM_L5_T"
      },
      "source": [
        "# 単語の頻度順可視化方法にはwordcloudがある。頻度順に字の大きさが異なるため、全体の傾向を直感的にわかりやすい長所があるが、より詳しい解析や知見を得られない短所がある。\n",
        "# wordcouldの入力期待値は、単語がスペースとして区切りされている一つのテキストである。コーパス全体を入力とするためには、単語がスペースとして区切りされているコーパスをコーパスごとにスペースで繋げる必要がある。\n",
        "tagger = owakati\n",
        "words_with_space = articles['headline'].apply(lambda x: tagger.parse(x).strip(\"\\n\").rstrip())\n",
        "words_with_space = ' '.join(words_with_space)\n",
        "print(words_with_space[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOIHNxWLL5_U"
      },
      "source": [
        "# 可視化するための関数を定義する\n",
        "def display_wordcloud(tagger, texts, stopwords, collocations):\n",
        "    # textsがpd.Seriesでない時に、pd.Seriesに変換\n",
        "    if isinstance(texts, pd.Series) is False:\n",
        "        texts = pd.Series(texts)\n",
        "\n",
        "    # テキストは単語別にスペースで区切りされ、テキストごとはスペースでつながっていることが期待値\n",
        "    words_with_space = texts.apply(lambda x: tagger.parse(x).strip(\"\\n\").rstrip())\n",
        "    words_with_space = ' '.join(words_with_space)\n",
        "    \n",
        "    # wordcloudを表示するため、パラメータを渡しインスタンス化する。\n",
        "    # collocations=Falseの場合、連語による重複単語が表示されない。\n",
        "    wordcloud = WordCloud(\n",
        "        background_color=\"white\",\n",
        "        font_path=CONFIG['font_path'],\n",
        "        stopwords=stopwords,\n",
        "        width=2000,\n",
        "        height=1000,\n",
        "        collocations=collocations,\n",
        "        random_state=0,\n",
        "    ).generate(words_with_space)\n",
        "\n",
        "    # 表示サイズを設定し、表示する。\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2YjtepL5_U"
      },
      "source": [
        "# headlineのwordcloud\n",
        "display_wordcloud(tagger=owakati, texts=articles['headline'], stopwords=stopwords, collocations=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA-3JmZcL5_U"
      },
      "source": [
        "# keywordsのwordcloud\n",
        "display_wordcloud(tagger=owakati, texts=articles['keywords'], stopwords=stopwords, collocations=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mc2J7z_L5_U"
      },
      "source": [
        "#### scatter text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzg5MyrcL5_U"
      },
      "source": [
        "# 頻度基盤の可視化方法の一つには、頻度の情報だけでなく、その単語の一般性を表す使用分布を共に可視化できるscatter textある。\n",
        "# scatter textをプロットするにあたって、whitespace_nlp_with_sentences関数を使うためには、各々の単語が空白で区切られる必要がある。\n",
        "# wordcloudとは違い、こちらのコーパス全体を一つとして繋げる必要はない。\n",
        "# owakatiでトークナイズし、空白で区切る\n",
        "tokenized = articles['headline'].apply(lambda x: tagger.parse(x).strip(\"\\n\").rstrip())\n",
        "tokenized.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zruDh1UZL5_U",
        "scrolled": true
      },
      "source": [
        "parsed = tokenized.apply(st.whitespace_nlp_with_sentences)\n",
        "parsed.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD1j7OeQL5_U"
      },
      "source": [
        "# stopwordsを除去し、プロット膨大となるため、1000回以下観察された単語は除去する。\n",
        "corpus = st.CorpusWithoutCategoriesFromParsedDocuments(\n",
        "    parsed.rename('parse').to_frame(), parsed_col='parse'\n",
        ").build().get_unigram_corpus().remove_terms(stopwords, ignore_absences=True).remove_infrequent_words(minimum_term_count=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvqevE3rL5_U"
      },
      "source": [
        "# プロットのため、単語ごとの頻度情報、使用分布情報を取得する\n",
        "dispersion = st.Dispersion(corpus)\n",
        "dispersion_df = dispersion.get_df()\n",
        "\n",
        "# ビルドされた頻度情報及び使用分布情報から、どの基準を用いてプロットするかをX, Xpos, Y, Yposのcolumnsにセットする。\n",
        "dispersion_df = dispersion_df.assign(\n",
        "    X=lambda df: df.Frequency,\n",
        "    Xpos=lambda df: st.Scalers.log_scale(df.X),\n",
        "    Y=lambda df: df[\"Rosengren's S\"],\n",
        "    Ypos=lambda df: st.Scalers.scale(df.Y),\n",
        ")\n",
        "\n",
        "dispersion_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWPmSiPTL5_U"
      },
      "source": [
        "def display_scatter_text(tagger, texts, stopwords, filename='out'): \n",
        "    gc.collect()\n",
        "    # textsがpd.Seriesでない時に、pd.Seriesに変換\n",
        "    if isinstance(texts, pd.Series) is False:\n",
        "        texts = pd.Series(texts)\n",
        "    \n",
        "    # owakatiより、テキストをparseし、単語をスペースより区切り、whitespace_nlp_with_sentencesを適用。\n",
        "    tokenized = texts.apply(lambda x: tagger.parse(x).strip(\"\\n\").rstrip())\n",
        "    parsed = tokenized.apply(st.whitespace_nlp_with_sentences)\n",
        "\n",
        "    # bigramsをcorpusから取り除き、stopwordsを取り除き、また、1000回以下で観察された単語は除去する。\n",
        "    corpus = st.CorpusWithoutCategoriesFromParsedDocuments(\n",
        "        parsed.rename('parse').to_frame(), parsed_col='parse'\n",
        "    ).build().get_unigram_corpus().remove_terms(stopwords, ignore_absences=True).remove_infrequent_words(minimum_term_count=1000)\n",
        "    \n",
        "    # Dispersion関数をより、単語ごとの頻度情報及び使用分布情報を取得。\n",
        "    dispersion = st.Dispersion(corpus)\n",
        "    dispersion_df = dispersion.get_df()\n",
        "    dispersion_df = dispersion_df.assign(\n",
        "        X=lambda df: df.Frequency,\n",
        "        Xpos=lambda df: st.Scalers.log_scale(df.X),\n",
        "        Y=lambda df: df[\"Rosengren's S\"],\n",
        "        Ypos=lambda df: st.Scalers.scale(df.Y),\n",
        "    )\n",
        "    \n",
        "    # dataframe_scattertext関数より、可視化したhtmlをビルドできる。\n",
        "    html = st.dataframe_scattertext(\n",
        "        corpus,\n",
        "        plot_df=dispersion_df,\n",
        "        ignore_categories=True,\n",
        "        x_label='Log Frequency',\n",
        "        y_label=\"Rosengren's S\",\n",
        "        y_axis_labels=['More Dispersion', 'Medium', 'Less Dispersion'],\n",
        "    )\n",
        "\n",
        "    # htmlを書き出します。Google Driveの該当箇所に出力されるため、出力されたhtmlファイルをダウンロードし、ブラウザで開いて御覧ください。\n",
        "    with open(f'{CONFIG[\"base_path\"]}/visualizations/vis_{filename}_scatter.html', 'w') as f:\n",
        "      f.write(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf28yYlmL5_U"
      },
      "source": [
        "# headlineのscatter text\n",
        "# y軸は使用分布(Dispersion)を表す。Dispersionは特定の単語がコーパス内でどれだけ均等に分布するかを表す数値。1に近いほど均等に現れる単語で、0に近いほど特定の分野や部分でのみ現れる。\n",
        "# x軸はコーパス全体における単語の登場頻度のログスケールを表す。\n",
        "\n",
        "# 以下のコードを実行するとvisualizations配下にhtmlファイルが作成されます。作成されたファイルをGoogle Driveからダウンロードし、ブラウザで開いて確認ください。\n",
        "display_scatter_text(tagger=owakati, texts=articles['headline'], stopwords=stopwords, filename='headline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpbi4OeXL5_U"
      },
      "source": [
        "# keywordsのscatter text\n",
        "# 以下のコードを実行するとvisualizations配下にhtmlファイルが作成されます。作成されたファイルをGoogle Driveからダウンロードし、ブラウザで開いて確認ください。\n",
        "display_scatter_text(tagger=owakati, texts=articles['keywords'], stopwords=stopwords, filename='keywords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G5iFUqpL5_V"
      },
      "source": [
        "# 追加的に、品詞ごとにこれらの頻度や使用分布を可視化することができる。\n",
        "# その場合、コーパス内での、可視化するターゲット品詞以外の全ての単語を削除し、同様のコードを実行することより可視化できる。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWO1V1SxL5_V"
      },
      "source": [
        "### 1.6.4. トピック解析\n",
        "コーパス全体の特性を理解するために、そのコーパス全体がどのようなトピックで構成されているかを解析するトピックモデリング方法がある。  \n",
        "潜在ディリクレ配分法(Latent Dirichlet Allocation, LDA)はトピックモデリングの代表的アルゴリズムである。LDAはコーパスがトピックの混合で成り立っていて、そのトピックは確率分布に基盤し単語を生成すると仮定する。LDAはその過程を逆に辿ることより、コーパス全体を構成するトピック情報を取得できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZomUJ0MvL5_V"
      },
      "source": [
        "def build_invalid_tokens(parsed_by_ochasen, non_skip_flags=['数'], skip_flags = [\"名詞\"]):\n",
        "    gc.collect()\n",
        "    # ldaの学習時、単語の頻度情報を持つメトリックスが用いられる。できるだけ単語数を減らすため、名詞以外は全て取り除く。\n",
        "    return build_stopwords(parsed_by_ochasen=parsed_by_ochasen, non_skip_flags=non_skip_flags, skip_flags=skip_flags)\n",
        "\n",
        "def tokenize_for_lda(tagger, texts, invalid_tokens):\n",
        "    gc.collect()\n",
        "    # textsがpd.Seriesでない時に、pd.Seriesに変換\n",
        "    if isinstance(texts, pd.Series) is False:\n",
        "        texts = pd.Series(texts)\n",
        "\n",
        "    # owakatiを用いてトークナイズする\n",
        "    tokenized = texts.apply(lambda x: tagger.parse(x).split())\n",
        "\n",
        "    # 上記で定義したinvalid_tokensに含まれないトークンに精製する。\n",
        "    tokenized = tokenized.apply(lambda x: [token for token in x if token not in invalid_tokens])\n",
        "    \n",
        "    return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oj0hz54L5_V"
      },
      "source": [
        "def build_ldamodel(tagger, texts, invalid_tokens, num_topics=10):\n",
        "    gc.collect()\n",
        "    tokenized = tokenize_for_lda(tagger=tagger, texts=texts, invalid_tokens=invalid_tokens)\n",
        "\n",
        "    # 頻度情報をもつ単語辞書を作る\n",
        "    dictionary = gensim.corpora.Dictionary(tokenized)\n",
        "\n",
        "    # 生成されたcorpusは(word_id, word_frequency)の情報を持つ\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
        "    \n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, chunksize=5000, passes=10, random_state=0)\n",
        "    \n",
        "    return ldamodel, corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq7SPNiIL5_V",
        "scrolled": false
      },
      "source": [
        "# 以下のトピック解析コードは高いcomputation resourceが要求され、実行時間がかなり長い。\n",
        "# そのことに留意し実行するとしよう。\n",
        "for column in ['headline', 'keywords']:\n",
        "    parsed_by_ochasen = {\n",
        "        'headline': parsed_headline_by_ochasen,\n",
        "        'keywords': parsed_keywords_by_ochasen,\n",
        "    }[column]\n",
        "\n",
        "    invalid_tokens = build_invalid_tokens(parsed_by_ochasen=parsed_by_ochasen)\n",
        "    ldamodel, corpus = build_ldamodel(tagger=owakati, texts=articles[column], invalid_tokens=invalid_tokens)\n",
        "\n",
        "    display_markdown(f'#### column: {column}', raw=True)\n",
        "    \n",
        "    # 推定されたtopicと関連深い上位5つの単語をプリントする\n",
        "    for topic in ldamodel.print_topics(num_words=5):\n",
        "        print(topic)\n",
        "    \n",
        "    # トピック可視化\n",
        "    # 可視化したトピックのidが0ではなく1から始まることに注意。\n",
        "    # 左方の円は、各々の10個のトピックを表す。\n",
        "    # 各円との距離は、それぞれトピックがどれだけ離れているかを表す。\n",
        "    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, ldamodel.id2word, sort_topics=False)\n",
        "    # htmlを書き出します。出力されたhtmlファイルをダウンロードし、ブラウザで開いて御覧ください。\n",
        "    pyLDAvis.save_html(vis, f'{CONFIG[\"base_path\"]}/visualizations/vis_{column}_lda.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4AOT1mtL5_V"
      },
      "source": [
        "### 1.6.5. 解析後データの追加処理\n",
        "解析によって得られた知見から、場合に応じて、追加的なデータの精製を必要となる。テキスト内にノイズとなりうる単語の規則を観測した場合や、テキスト自体が無意味であるかノイズとなり得る場合などが該当する。上記の解析を通じて、コーパス全体において、人事の単語が非常に高い頻度で観測されているが、人事の内容はマーケットの予測に役に立つと思われない。今回は、人事の内容を含むニュースの割合が全体のどれぐらいになるかを調べ、必要に応じてそれらのニュースを除去する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBPX7FZEL5_V"
      },
      "source": [
        "# 上記で行った単語頻度数を基準とした可視化方法wordcloud, scatter textにおいて、'人事'という単語が多く観察されている。\n",
        "# ここでは、人事の内容はを含むニュース記事を取り除く作業を行う。\n",
        "\n",
        "# owakatiを用いて、人事の単語を含む記事を除去する方法もあるが、本番環境でのリソースを軽減させるため、単純に文字列から人事を含む全てのニュースの除去を行う。\n",
        "# headlineとkeywordsそれぞれに「人事」がどれぐらい含まれているかを確認\n",
        "headline_drop_mask = articles['headline'].str.contains('人事')\n",
        "keywords_drop_mask = articles['keywords'].str.contains('人事')\n",
        "\n",
        "print(f'number of 人事 in headline: {len(articles[headline_drop_mask])} / {len(articles)}')\n",
        "print(f'number of 人事 in keywords: {len(articles[keywords_drop_mask])} / {len(articles)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHDe6TcyL5_V"
      },
      "source": [
        "# headlineもしくは、keywordsどちらかで人事を含むニュース記事のindexマスクを作成。\n",
        "drop_mask = headline_drop_mask | keywords_drop_mask\n",
        "\n",
        "# '人事'を含む例を表示する\n",
        "articles[drop_mask].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5iPX9HNL5_V"
      },
      "source": [
        "# '人事'を含まないニュースだけに精製する。\n",
        "articles = articles[~drop_mask]\n",
        "\n",
        "# '人事'を含むニュースが存在するか確認する。\n",
        "(articles['headline'].str.contains('人事') | articles['keywords'].str.contains('人事')).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mBvE_X8L5_V"
      },
      "source": [
        "# 上記のコードを用いて、本番提出用のクラスにclassmethodを追加\n",
        "@classmethod\n",
        "def drop_remove_list_words(cls, articles, remove_list_words=[\"人事\"]):\n",
        "    articles = articles.copy()\n",
        "\n",
        "    for remove_list_word in remove_list_words:\n",
        "        # headlineもしくは、keywordsどちらかでremove_list_wordを含むニュース記事のindexマスクを作成。\n",
        "        drop_mask = articles[\"headline\"].str.contains(remove_list_word) | articles[\n",
        "            \"keywords\"\n",
        "        ].str.contains(remove_list_word)\n",
        "\n",
        "        # remove_list_wordを含まないニュースだけに精製する。\n",
        "        articles = articles[~drop_mask]\n",
        "\n",
        "    return articles\n",
        "    \n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.drop_remove_list_words = drop_remove_list_words\n",
        "                                                        \n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])\n",
        "articles = SentimentGenerator.normalize_articles(articles)\n",
        "articles = SentimentGenerator.handle_punctuations_in_articles(articles)\n",
        "articles = SentimentGenerator.drop_remove_list_words(articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FgoEMIRL5_V"
      },
      "source": [
        "## 1.7. 特徴量の生成\n",
        "テキストから特徴量を抽出するためには、単語の頻度基盤、単語の分散表現基盤、言語モデル基盤などの方法が存在する。  \n",
        "下記では、いくつかの方法を紹介する。単語の頻度基盤には、\n",
        "1. テキストから登場する各単語の頻度を行列化したDTM, TF-IDF\n",
        "2. 潜在意味を抽出するトピックモデリング方法LSA, LDA\n",
        "\n",
        "これらの方法は、単語の頻度行列の使用を基盤としているため、コーパス内に単語が増えれば増えるほど高いコンピュテーションリーソースが要求される。\n",
        "\n",
        "単語の分散表現基盤には、\n",
        "1. 中心単語から周辺単語を予測し、前後単語との関係性を基盤に分散表現を構築するWord2Vec(CBOW)\n",
        "2. 頻度基盤のLSAと予測基盤のWord2Vecを補うとして、頻度と予測両方を基盤に分散表現を構築するGlove\n",
        "3. 単語をSubword化し、予測基盤で分散表現を構築することよりOut-Of-Vocabularyに頑健なFastText\n",
        "4. RNN基盤の言語モデルbiLMを用いて、文脈を反映し分散表現を構築するElmo\n",
        "\n",
        "これらの方法は、単語を分散表現化する方法であるが、モデルに入力とするSparse vectorにテキスト内の単語を全て与えるか、単語分散表現を合成するなどの処理を行い、テキストの分散表現を取得することができる。\n",
        "\n",
        "言語モデル基盤には(本内容では、ニューラルネット基盤の言語モデルのみを説明する)、\n",
        "1. 循環神経網(Recurrent Neural Network, RNN)基盤(LSTM, GRU, LMを含む)の内部状態を特徴量として抽出\n",
        "2. Transformer(BERTなどattention基盤モデルを含む)基盤の内部状態を特徴量として抽出\n",
        "\n",
        "これらの方法は、膨大なデータの学習受容量を持ち、またそれらの膨大なデータから学習されたモデルは、テキスト内でのより高次元の潜在表現を学習できる。また、それらから抽出した特徴量は多様なタスクにおいて適用でき、且つ、高いパフォーマンスを表すことが知られている。\n",
        "\n",
        "本章では、BERT(Bidirectional Encoder Representations from Transformers)を用いて、テキストを入力し、そのテキストが持つ潜在意味を特徴量として抽出する方法を説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUNGiALEL5_W"
      },
      "source": [
        "### 1.7.1. 特徴量抽出機、前処理機の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arVIZhClL5_W"
      },
      "source": [
        "# 下記では、BERTモデルを扱う上で必要な、特徴量抽出時使うデバイスの定義、特徴量抽出機の定義、BERTの前処理機の定義を行う。\n",
        "\n",
        "@classmethod\n",
        "def _set_device(cls):\n",
        "    # 使用可能なgpuがある場合、そちらを利用し特徴量抽出を行う\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        cls.device = 'cuda'\n",
        "        print(\"[+] Set Device: GPU\")\n",
        "    else:\n",
        "        cls.device = 'cpu'\n",
        "        print(\"[+] Set Device: CPU\")\n",
        "\n",
        "@classmethod\n",
        "def _build_feature_extractor(cls):\n",
        "    # 特徴量抽出のため事前学習済みBERTモデルを用いる。\n",
        "    # ここでは、\"cl-tohoku/bert-base-japanese-whole-word-masking\"モデルを使用しているが、異なる日本語BERTモデルを用いても良い。\n",
        "    cls.feature_extractor = (\n",
        "        transformers.BertModel.from_pretrained(\n",
        "            \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
        "            return_dict=True,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 使用するdeviceを指定\n",
        "    cls.feature_extractor = cls.feature_extractor.to(cls.device)\n",
        "\n",
        "    # 今回、学習は行わない。特徴量抽出のためなので、評価モードにセットする。\n",
        "    cls.feature_extractor.eval()\n",
        "    \n",
        "    print(\"[+] Built feature extractor\")\n",
        "\n",
        "@classmethod\n",
        "def _build_tokenizer(cls):\n",
        "    # BERTモデルの入力とするコーパスはそのBERTモデルが学習された時と同様の前処理を行う必要がある。\n",
        "    # 今回使用する\"cl-tohoku/bert-base-japanese-whole-word-masking\"モデルは、mecab-ipadic-NEologdによりトークナイズされ、その後Wordpiece subword encoderよりsubword化している。\n",
        "    # Subwordとは形態素の類似な概念として、単語をより小さい意味のある単位に変換したものである。\n",
        "    # transformersのBertJapaneseTokenizerは、その事前学習モデルの学習時と同様の前処理を簡単に使用することができる。\n",
        "    # この章ではBertJapaneseTokenizerを利用し、トークナイズ及びsubword化を行う。\n",
        "    cls.bert_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "    print(\"[+] Built bert tokenizer\")\n",
        "    \n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator._set_device = _set_device\n",
        "SentimentGenerator._build_feature_extractor = _build_feature_extractor\n",
        "SentimentGenerator._build_tokenizer = _build_tokenizer\n",
        "                                                        \n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])\n",
        "articles = SentimentGenerator.normalize_articles(articles)\n",
        "articles = SentimentGenerator.handle_punctuations_in_articles(articles)\n",
        "articles = SentimentGenerator.drop_remove_list_words(articles)\n",
        "\n",
        "SentimentGenerator._set_device()\n",
        "SentimentGenerator._build_feature_extractor()\n",
        "SentimentGenerator._build_tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYOlttUCL5_W"
      },
      "source": [
        "### 1.7.2. BERTモデルを使用するための前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0qhsjpUL5_W"
      },
      "source": [
        "#　二つの前処理を比較してみる\n",
        "# SentimentGenerator.bert_tokenizer: mecab-ipadic-NEologd + Wordpiece\n",
        "# owakati: mecab-ipadic-NEologd\n",
        "\n",
        "# Subword化を行うbert_tokenizerの方がより小さい単位でトークン化されていることがわかる。\n",
        "# このようなsubwordを用いると、学習時観察されていない単語(Out of Vocabulary)に関する問題を緩和させることができる。\n",
        "text = '我らは走り出す。'\n",
        "display(SentimentGenerator.bert_tokenizer.tokenize(text))\n",
        "display(owakati.parse(text).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPuHWwiTL5_W"
      },
      "source": [
        "# 基本的にどの言語モデルもトークナイズ後のトークンをそのまま受け取ることはできない。トークンを数字に対応させるid化が必要とある。\n",
        "# 事前学習モデルを用いる場合、各々のトークンはすでにidが付与されているため、そのidに変更し、モデルの入力として扱う必要がある。\n",
        "# BertJapaneseTokenizerのencodeはこのようなid化を行ってくれる。\n",
        "display(SentimentGenerator.bert_tokenizer.encode(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nI-A9ZSL5_W"
      },
      "source": [
        "# id化したトークンを再度トークンに変更してみる\n",
        "for id in SentimentGenerator.bert_tokenizer.encode(text):\n",
        "    print(f'{id}: {SentimentGenerator.bert_tokenizer.decode(id)}')\n",
        "    \n",
        "# idをトークンに再度戻した時、元のデータには存在していない[CLS]と[SEP]を観察することができる。この二つはspecial tokensと呼ばれ、[CLS]は全ての文章の前に使用され出力層におけるtoken sequenceの結合意味からクラス分類を行うため用いられる。[SEP]は複数の文章を区切るために用いられる。\n",
        "# 入力は必ずこのフォーマットに従う必要がある。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pms7CJZFL5_W",
        "scrolled": true
      },
      "source": [
        "# BERTの入力値は上記で生成したtokenのids以外に, token_type_ids, attention_maskのベクトルを入力値として受け取る。\n",
        "# token_type_idsは複数の文章を区切るため用いられる。\n",
        "# attention maskは実際トークンが存在する部分とzero paddingされた部分を区切るため用いられる。\n",
        "\n",
        "encoded = SentimentGenerator.bert_tokenizer.encode_plus(\n",
        "    text,\n",
        "    None,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eioNaBgkL5_W",
        "scrolled": true
      },
      "source": [
        "# torchモデルに入力するためにはtensor形式に変え、deviceを指定する必要がある。\n",
        "input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "token_type_ids = torch.tensor([encoded['token_type_ids']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "\n",
        "display(input_ids)\n",
        "display(token_type_ids)\n",
        "display(attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6dT_bSL5_W"
      },
      "source": [
        "### 1.7.3. 特徴量抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBMvAVk6L5_W"
      },
      "source": [
        "# 2.前処理の最後で取得した[input_ids, token_type_ids, attention_mask]三つのベクトルを特徴量抽出機に入力し、BERTモデルのoutputを取る\n",
        "output = SentimentGenerator.feature_extractor(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtjJpaa4L5_X"
      },
      "source": [
        "# outputをdictionary形式に変換し、その中からどのようなkeyが存在するかをみてみる。\n",
        "output.__dict__.keys()\n",
        "\n",
        "# last_hidden_stateはモデル最終層出力の全てのsequenceのhidden stateを返す\n",
        "# pooler_outputはモデル最終層出力の最初のsequenceのhidden stateを返す\n",
        "# hidden_statesはモデル各層出力の全てのsequenceのhidden stateを返す\n",
        "# attentionsはattention softmax以降のattentions weightsを返す\n",
        "# cross_attentionsはdecoderのcross-attention層における、attention softmax以降のattentions weightsを返す"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6y509ckL5_X"
      },
      "source": [
        "# Deep neural networkではより後層に行くほど、高次表現を持つことが知られている。ここでは、最終層の一つ前のhidden stateを特徴量として抽出し使用することにする。\n",
        "# 最終層でなはなく、最終層手前の情報を利用する理由は、最終層は学習タスクにおけるオブジェクトに強く引き寄せられた情報を持つため、一般的には特徴量抽出のために、より豊富な情報を持つ最終層手前使用する。\n",
        "# この内容に関してより詳しく知りたい場合以下を参照: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
        "features = output['hidden_states'][-2]\n",
        "display(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKtnYvJrL5_X"
      },
      "source": [
        "# featuresの次元を見ると、[1, 8, 768]次元を持つが、各々順番に、[データ数、シーケンスサイズ、hidden stateのサイズ]を表している。\n",
        "display(features.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjBEi4MfL5_X"
      },
      "source": [
        "# 一つ注意すべきところが、長さの異なるシークエンスを入力すると以下のようにベクトルのサイズが変わってくる。\n",
        "text = 'こちらでは、より長い文章を用いて特徴量を抽出してみましょう。'\n",
        "encoded = SentimentGenerator.bert_tokenizer.encode_plus(\n",
        "    text,\n",
        "    None,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "token_type_ids = torch.tensor([encoded['token_type_ids']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(SentimentGenerator.device)\n",
        "\n",
        "output = SentimentGenerator.feature_extractor(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "features = output['hidden_states'][-2]\n",
        "features.size()\n",
        "\n",
        "# dimension1のサイズが変わっていることがわかる。最終的に、この章ではdimension1の次元を平均化し特徴量として扱うため、シークエンスの違いがそれほど問題とはならない。\n",
        "# しかし、モデルへの同時入力、並列化する上では問題となる。シーケンスの異なるベクトルを重ねることができないからである。\n",
        "# このような問題を扱うため、subwordsのシーケンスのmax_lengthを決め、それより短い場合は、max_lengthの長さとなるようにベクトルの末端を0で埋めるzero paddingがよく使われる。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSK_bZfmL5_Y"
      },
      "source": [
        "# 前処理機にmax_lengthのパラメータを渡し、padding='max_legnth'を指定すると、max_lengthの長さがなるようにzero paddingすることができる\n",
        "text = 'こちらでは、より長い文章を用いて特徴量を抽出してみましょう。'\n",
        "encoded = SentimentGenerator.bert_tokenizer.encode_plus(\n",
        "    text,\n",
        "    None,\n",
        "    max_length=512,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4JxH__zL5_Y"
      },
      "source": [
        "# 異なる長さを持つ複数のコーパスを用いて、paddingされたinputを用いてモデルに同時入力してみる。\n",
        "texts = ['短いテキスト', '少し長いテキストです', '長い長い長い長いテキストです']\n",
        "\n",
        "input_ids = []\n",
        "token_type_ids = []\n",
        "attention_mask = []\n",
        "for text in texts:\n",
        "    encoded = SentimentGenerator.bert_tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    token_type_ids.append(encoded['token_type_ids'])\n",
        "    attention_mask.append(encoded['attention_mask'])\n",
        "\n",
        "input_ids = torch.tensor(input_ids, dtype=torch.long).to(SentimentGenerator.device)\n",
        "token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).to(SentimentGenerator.device)\n",
        "attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(SentimentGenerator.device)\n",
        "\n",
        "output = SentimentGenerator.feature_extractor(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "features = output['hidden_states'][-2]\n",
        "features.size()\n",
        "\n",
        "# [3つのコーパス、シーケンスの長さ、hidden state]の次元の特徴量を取得できた。\n",
        "# dimention1は指定したmax_lengthと一致することがわかる。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdNTbBH3L5_Y"
      },
      "source": [
        "# 最終的には、dimension1を平均化し、各テキストごとに768次元のベクトルを特徴量として抽出する。\n",
        "features = features.mean(dim=1)\n",
        "features.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9fUg160L5_Y"
      },
      "source": [
        "# 上記のコードを纏め、テキストから前処理を行い、モデル入力に必要な各々のベクトルを返す関数を作成。\n",
        "@classmethod\n",
        "def build_inputs(cls, texts, max_length=512):\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    attention_mask = []\n",
        "    for text in texts:\n",
        "        encoded = cls.bert_tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "        )\n",
        "        \n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        token_type_ids.append(encoded['token_type_ids'])\n",
        "        attention_mask.append(encoded['attention_mask'])\n",
        "        \n",
        "    # torchモデルに入力するためにはtensor形式に変え、deviceを指定する必要がある。\n",
        "    input_ids = torch.tensor(input_ids, dtype=torch.long).to(cls.device)\n",
        "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).to(cls.device)\n",
        "    attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(cls.device)\n",
        "    \n",
        "    return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "# 上記のコードでビルドした、ベクトルをモデルに入力し、特徴量を抽出するコード作成\n",
        "@classmethod\n",
        "def generate_features(cls, input_ids, token_type_ids, attention_mask):\n",
        "    output = cls.feature_extractor(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "    features = output['hidden_states'][-2].mean(dim=1).cpu().detach().numpy()\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "# コーパス全体から特徴量を抽出するため、コーパス全体を同時にモデルへ入力することはメモリーの上限を遥かに超えてしまうので不可能に近い。\n",
        "# 入力するコーパスを数回に分割し、上記で作成したコードbuild_inputsとgenerate_featuresを用いて並列化処理を行うため、以下のコードを作成する。\n",
        "@classmethod\n",
        "def generate_features_by_texts(cls, texts, batch_size=2, max_length=512):\n",
        "    n_batch = math.ceil(len(texts) / batch_size)\n",
        "\n",
        "    features = []\n",
        "    for idx in tqdm(range(n_batch)):\n",
        "        input_ids, token_type_ids, attention_mask = cls.build_inputs(texts=texts[batch_size*idx:batch_size*(idx+1)], max_length=max_length)\n",
        "        \n",
        "        features.append(cls.generate_features(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask))\n",
        "        \n",
        "    features = np.concatenate(features, axis=0)\n",
        "    \n",
        "    # 抽出した特徴量はnp.ndarray形式となっており、これらは、日付の情報を失っているため、pd.DataFrame形式に変換する。\n",
        "    return pd.DataFrame(features, index=texts.index)\n",
        "\n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.build_inputs = build_inputs\n",
        "SentimentGenerator.generate_features = generate_features\n",
        "SentimentGenerator.generate_features_by_texts = generate_features_by_texts\n",
        "\n",
        "# SentimentGenerator使用する全体流れを記述\n",
        "articles = SentimentGenerator.load_articles(path=CONFIG[\"article_path\"])\n",
        "articles = SentimentGenerator.normalize_articles(articles)\n",
        "articles = SentimentGenerator.handle_punctuations_in_articles(articles)\n",
        "articles = SentimentGenerator.drop_remove_list_words(articles)\n",
        "\n",
        "SentimentGenerator._set_device()\n",
        "SentimentGenerator._build_feature_extractor()\n",
        "SentimentGenerator._build_tokenizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpySrQ-YcY6a"
      },
      "source": [
        "# 以下のコードでコーパス全体の特徴量を抽出できる。しかし、抽出には長い時間が要求されるため、注意。\n",
        "# headlineの特徴量を抽出\n",
        "headline_features = SentimentGenerator.generate_features_by_texts(texts=articles['headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ2PyuY9cd9B"
      },
      "source": [
        "# pklファイルとしてstoreしておく。\n",
        "headline_features.to_pickle(f'{CONFIG[\"base_path\"]}/headline_features/headline_features.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06WAvdVzchzD"
      },
      "source": [
        "# keywordsの特徴量を抽出\n",
        "keywords_features = SentimentGenerator.generate_features_by_texts(texts=articles['keywords'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-7iQDavL5_Y"
      },
      "source": [
        "# pklファイルとしてstoreしておく。\n",
        "keywords_features.to_pickle(f'{CONFIG[\"base_path\"]}/keywords_features/keywords_features.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZf7te6cf0HV"
      },
      "source": [
        "# PCAによるスコア取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZnuBIhofx9g"
      },
      "source": [
        "def _build_compressor(compress_method):\n",
        "    assert compress_method in ('pca', 'kpca')\n",
        "    if compress_method == 'pca':\n",
        "        return PCA(n_components=1)\n",
        "    \n",
        "    if compress_method == 'kpca':\n",
        "        return KernelPCA(kernel='rbf', n_components=1)\n",
        "\n",
        "def compress_feature_n_samples(features, compress_method, max_samples=500):    \n",
        "    feature_compressor = _build_compressor(compress_method=compress_method)\n",
        "    compressed_features = pd.Series(feature_compressor.fit_transform(features).reshape(-1), index=features.index)\n",
        "\n",
        "    sample_compressor = _build_compressor(compress_method=compress_method)\n",
        "\n",
        "    weekly_group = pd.Series(zip(compressed_features.index.year, compressed_features.index.week), index=compressed_features.index)\n",
        "    grouped_compressed_features = compressed_features.groupby(weekly_group).apply(lambda x: x[-max_samples:].reset_index(drop=True)).unstack()\n",
        "\n",
        "    return pd.Series(sample_compressor.fit_transform(grouped_compressed_features).reshape(-1), index=grouped_compressed_features.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2RaiqrlfUOw"
      },
      "source": [
        "for features, feature_type in [(headline_features, 'headline_features'), (keywords_features, 'keywords_features')]:\n",
        "    for compress_method in ['pca']:\n",
        "        compress_feature_n_samples(features=features, compress_method=compress_method).to_pickle(f'{feature_type}_method3_{compress_method}.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLivsKhweLpc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}