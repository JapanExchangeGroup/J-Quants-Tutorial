{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "20210226_chapter06_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Cq-mf9_hL5_G",
        "NBKLi7HJL5_J",
        "Iu-sSErpL5_K",
        "cdgLyMHgL5_L",
        "Av5UvhxgL5_O",
        "aoXYGUiRL5_R",
        "xKK-XxCvL5_S",
        "YJN7ai41L5_T",
        "kFTkP043L5_T",
        "7mc2J7z_L5_U",
        "kWO1V1SxL5_V",
        "I4AOT1mtL5_V",
        "WUNGiALEL5_W",
        "vYOlttUCL5_W",
        "hs6dT_bSL5_W",
        "nlnsMRhFL5_Y",
        "_1lhMz1mL5_Y",
        "xAc4RPuKL5_Z",
        "88c4kszML5_Z",
        "hrlhxKRFL5_a",
        "aDJBSghDL5_c",
        "MwHiud6RL5_c",
        "5Zr2wQiVL5_d",
        "1QdUsijlL5_e",
        "A8qo8PaAL5_f",
        "WldVU5IyL5_f",
        "UPeltoI-L5_f",
        "Mpj1YyrQL5_f"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08NytJ1xL5-8"
      },
      "source": [
        "## チュートリアル説明\n",
        "\n",
        "本チュートリアルでは、最終的にニュースコーパスから特徴量を抽出することを目的とする。  \n",
        "各章では、特徴量を抽出するにあたって、使用しているデータがどのような特性を持つかを解析し、必要に応じて正規化し、可視化する方法を説明する。  \n",
        "最後の章では、自然語処理モデルであるBERT(Bidirectional Encoder Representations from Transformers)を用いて、特徴量を抽出する方法を説明する。また、抽出された特徴量をLSTMを用いて週ごとに合成する方法を説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1y20QBL5_C"
      },
      "source": [
        "## 1.3. 基本準備\n",
        "本章では、必要なライブラリーと分析用辞書などをインストールし、チュートリアルのコードを正しく利用できる環境を構築する。  \n",
        "また、本チュートリアルで利用するライブラリーを宣言し、その他、チュートリアルを進めるにあたって必要な基本的な準備を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPOT-ueXrwd"
      },
      "source": [
        "# Google Colab環境ではGoogle Driveをマウントしてアクセスできるようにします。\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Google Drive をマウントします\n",
        "    from google.colab import drive\n",
        "    mount_dir = \"/content/drive\"\n",
        "    drive.mount(mount_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME8m28W5L5_C"
      },
      "source": [
        "### 1.3.3. 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1iMGfaJL5_C",
        "scrolled": true
      },
      "source": [
        "!apt update\n",
        "!apt install -y build-essential sudo mecab libmecab-dev mecab-ipadic-utf8 fonts-ipafont-gothic file\n",
        "!pip install pandas==1.1.5 numpy==1.19.5 scattertext==0.1.0.0 wordcloud==1.8.1 torch==1.7.1 torchvision==0.8.2 transformers==4.2.2 mecab-python3==0.996.6rc1 ipadic==1.0.0 neologdn==0.4 fugashi==1.0.5 japanize-matplotlib==1.1.3 gensim==3.8.3 pyLDAvis==2.1.2\n",
        "\n",
        "# mecab用の辞書をインストール\n",
        "!git clone https://github.com/neologd/mecab-ipadic-neologd.git --branch v0.0.7 --single-branch\n",
        "!yes yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdoDW0FSL5_D"
      },
      "source": [
        "### 1.3.4. ライブラリの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3IpBaeXL5_D"
      },
      "source": [
        "# 基本ライブラリー\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import string\n",
        "from copy import copy\n",
        "from glob import glob\n",
        "from itertools import chain\n",
        "import gc\n",
        "\n",
        "# テキスト解析関連\n",
        "import MeCab\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "# 可視化関連\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display, display_markdown, IFrame\n",
        "import scattertext as st\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "# ニューラルネット関連\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset as _Dataset\n",
        "\n",
        "# ノートブック上でpyLDAvisより可視化を行う場合の設定\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf1vuCv1L5_D"
      },
      "source": [
        "### 1.3.6. 実行環境の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrXsuolTL5_E"
      },
      "source": [
        "print(sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3v2Sc6HL5_E"
      },
      "source": [
        "### 1.3.7. ファイルパスの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QecMOQDGL5_E"
      },
      "source": [
        "# colab環境で実行する場合\n",
        "if 'google.colab' in sys.modules:\n",
        "    CONFIG = {\n",
        "        'base_path': f'{mount_dir}/MyDrive/JPX_competition/workspace',\n",
        "        'article_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/nikkei_article.csv.gz',\n",
        "        'stock_price_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/stock_price.csv.gz',\n",
        "        'stock_list_path': f'{mount_dir}/MyDrive/JPX_competition/data_dir_comp2/stock_list.csv.gz',\n",
        "        'dict_path': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
        "        'font_path': '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',\n",
        "    }\n",
        "else:\n",
        "    CONFIG = {\n",
        "        'base_path': '/notebook/workspace',\n",
        "        'article_path': '/notebook/data_dir_comp2/nikkei_article.csv.gz',\n",
        "        'stock_price_path': '/notebook/data_dir_comp2/stock_price.csv.gz',\n",
        "        'stock_list_path': '/notebook/data_dir_comp2/stock_list.csv.gz',\n",
        "        'dict_path': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
        "        'font_path': '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuxygYOiOdH"
      },
      "source": [
        "# 1.3.10. 本番提出用のクラス作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fip2yMj3iHFc"
      },
      "source": [
        "# 本番提出用のクラスを作成するため、関数の持たない基本クラスを定義する\n",
        "# 下記で段階的にクラスを作り上げる\n",
        "class SentimentGenerator(object):\n",
        "    # 以下は使用時にビルドされる。\n",
        "    # 各々に関しては、以下のチュートリアルで説明する。\n",
        "    article_columns = None\n",
        "    punctuation_replace_dict = None\n",
        "    punctuation_remove_list = None\n",
        "    device = None\n",
        "    feature_extractor = None\n",
        "    headline_feature_combiner_handler = None\n",
        "    keywords_feature_combiner_handler = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhfS_YJiL5_Y"
      },
      "source": [
        "## 1.7.4. LSTMによる可変特徴量統合\n",
        "本チュートリアルの最終モデルの評価関数は、Marketのweekly return予測することとして判定される。週ごとにデータ数が動的であるニュースデータは、それらから抽出した特徴量もその数が可変であり、それらの可変する情報を用いてモデリングする必要がある。本チュートリアルでは、週ごとのその数が可変する特徴量を一つの統合する戦略をとっている。そのため、可変長さの入力が可能なLSTMを用いて、Market returnの次の週の上げ下げの情報をラベルとして、特徴量を統合する学習を行った。\n",
        "\n",
        "本章では、そのLSTMのモデル作成及び、学習の方法を説明し、最終的に単一特徴量として抽出する方法を説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlnsMRhFL5_Y"
      },
      "source": [
        "### 1.7.5. BERT特徴量ロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJm7rvUZL5_Y"
      },
      "source": [
        "headline_features = pd.read_pickle(f'{CONFIG[\"base_path\"]}/headline_features/headline_features.pkl')\n",
        "keywords_features = pd.read_pickle(f'{CONFIG[\"base_path\"]}/keywords_features/keywords_features.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F43HNj8DL5_Y"
      },
      "source": [
        "headline_features.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_YhPWwqL5_Y"
      },
      "source": [
        "keywords_features.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1lhMz1mL5_Y"
      },
      "source": [
        "### Stockデータロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptp3sKcaL5_Y"
      },
      "source": [
        "# stock priceとstock listをロードする。\n",
        "stock_price = pd.read_csv(CONFIG[\"stock_price_path\"])\n",
        "stock_list = pd.read_csv(CONFIG[\"stock_list_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9I6WcbWL5_Y"
      },
      "source": [
        "stock_price.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHoHV6dRL5_Y"
      },
      "source": [
        "stock_list.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4RJNbB-L5_Z",
        "scrolled": true
      },
      "source": [
        "# stock_priceのデータ精製を行う。\n",
        "# ラベル作成のために使用するcolumnは['EndOfDayQuote Date', 'Local Code', \"EndOfDayQuote Open\", \"EndOfDayQuote ExchangeOfficialClose\"]のみである。\n",
        "# EndOfDayQuote Date: データの日付\n",
        "# Local Code: 銘柄コード\n",
        "# EndOfDayQuote Open: 始値\n",
        "# EndOfDayQuote ExchangeOfficialClose: 終値\n",
        "stock_price = stock_price[['EndOfDayQuote Date', 'Local Code', \"EndOfDayQuote Open\", \"EndOfDayQuote ExchangeOfficialClose\"]]\n",
        "\n",
        "# それぞれのcolumn名をわかりやすく変更する\n",
        "stock_price = stock_price.rename(columns={\n",
        "    'EndOfDayQuote Date': 'date',\n",
        "    'Local Code': 'asset',\n",
        "    'EndOfDayQuote Open': 'open',\n",
        "    'EndOfDayQuote ExchangeOfficialClose': 'close',\n",
        "})\n",
        "\n",
        "# データごとにindex形式が異なると大変扱いにくい。下記のコードより特徴量と同様のindexの形式を変更する。\n",
        "# pd.to_datetimeより、string形式の日付をpd.Timestamp形式に変換する\n",
        "# pd.Timestamp形式をpd.DatetimeIndex形式に変更し、time zoneをheadline_featuresと同様に設定する。\n",
        "# この際、headline_featuresとkeywords_featuresはarticlesのindexを使用しているため、timezoneが一致している。どちらを用いても良い。\n",
        "stock_price['date'] = pd.to_datetime(stock_price['date'])\n",
        "stock_price['date'] = pd.DatetimeIndex(stock_price['date']).tz_localize(headline_features.index.tz)\n",
        "\n",
        "# indexを['date', 'asset']順のpd.MultiIndex形式として設定する。\n",
        "stock_price = stock_price.set_index(['date', 'asset']).sort_index()\n",
        "\n",
        "# unstack()より銘柄情報をcolumnに移動させる。\n",
        "stock_price = stock_price.unstack()\n",
        "\n",
        "# 今回使用するデータは2020年以降のデータであるので、2020年以前のデータを切り捨てる。\n",
        "stock_price = stock_price['2020-01-01':]\n",
        "\n",
        "# 確認する\n",
        "display(stock_price.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QJPRRnXL5_Z"
      },
      "source": [
        "# stock_listデータから2020年12月末日時点での株式発行数の情報のみを取得する。\n",
        "# 使用するcolumnは、['Local Code', 'IssuedShareEquityQuote IssuedShare']である。\n",
        "# Local code: 銘柄コード\n",
        "# IssuedShareEquityQuote IssuedShare: 発行株式数\n",
        "stock_list = stock_list[['Local Code', 'IssuedShareEquityQuote IssuedShare']]\n",
        "\n",
        "# columns名をわかりやすく変更する\n",
        "stock_list = stock_list.rename(\n",
        "    columns={\n",
        "        'Local Code': 'asset',\n",
        "        'IssuedShareEquityQuote IssuedShare': 'shares'\n",
        "    }\n",
        ")\n",
        "\n",
        "# assetをindexとして設定する\n",
        "shares = stock_list.set_index('asset')['shares']\n",
        "\n",
        "# 確認する\n",
        "display(shares)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAc4RPuKL5_Z"
      },
      "source": [
        "### 使用する銘柄選定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLkakTfwL5_Z"
      },
      "source": [
        "本章では、以下の条件に従い、マーケットのリターンを計算する際に用いる銘柄を選定する。\n",
        "\n",
        "1. 2020年12月末日時点で、東京証券取引所に上場していること。\n",
        "2. 2020年12月末日時点で、時価総額が200億を上回っていること。\n",
        "\n",
        "上の条件を満たす銘柄を選ぶため、以下のロジックに従う。\n",
        "1. 12月末日時点で価格情報が存在するものを上場しているとみなす。\n",
        "2. 12月末日時点での株式発行数 * 終値から時価総額を計算し、200億円未満のものを切り捨てる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NchVtUbXL5_Z"
      },
      "source": [
        "# 条件１「2020年12月末日時点で、東京証券取引所に上場していること」に該当する銘柄を取得する。\n",
        "# まず、indexの最終日を取得する。\n",
        "last_date = stock_price.index[-1]\n",
        "\n",
        "# 2020年の末日が、最終日であることがわかる。\n",
        "display(last_date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybLjYFCoL5_Z"
      },
      "source": [
        "# 2020年の末日時点での終値が存在する銘柄を取得する。\n",
        "universe_condition_1 = stock_price.xs(last_date)[\"close\"].dropna().index\n",
        "\n",
        "# 条件一に該当する銘柄数を確認する。\n",
        "print(f'number_of_assets: {len(universe_condition_1)}/{len(stock_price[\"close\"].columns)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIFdIPXjL5_Z"
      },
      "source": [
        "# 条件２「2020年12月末日時点で、時価総額が200億を上回っていること」に該当する銘柄を取得する。\n",
        "# まず、株式発行数 * 終値から時価総額を計算する。(単位: 円)\n",
        "marketcap = (stock_price.xs(last_date)['close'] * shares)\n",
        "\n",
        "# 確認する\n",
        "display(marketcap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IISeMrepL5_Z"
      },
      "source": [
        "# 時価総額が200億円未満のものを切り捨てる\n",
        "universe_condition_2 = marketcap[marketcap >= 20000000000].index\n",
        "\n",
        "# 条件一に該当する銘柄数を確認する。\n",
        "print(f'number_of_assets: {len(universe_condition_2)}/{len(stock_price[\"close\"].columns)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLORwDncL5_Z"
      },
      "source": [
        "# 上の二つの条件を合成し、universeを設定する。\n",
        "universe = universe_condition_1 & universe_condition_2\n",
        "\n",
        "# universeをcsvでstoreしておきます。\n",
        "universe.to_series().rename('universe').to_frame().to_csv('universe.csv')\n",
        "\n",
        "# 条件に該当する銘柄数を確認する。\n",
        "print(f'number_of_assets: {len(universe)}/{len(stock_price[\"close\"].columns)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th6tMkDrL5_Z"
      },
      "source": [
        "# 選定された銘柄のみの価格情報に精製する。\n",
        "stock_price = stock_price[[column for column in stock_price.columns if column[-1] in universe]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c4kszML5_Z"
      },
      "source": [
        "### 1.7.6. 週ごとにグループされた特徴量とラベルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVcxaMfzL5_Z"
      },
      "source": [
        "# 各週ごとの特徴量を統合するためには、週ごとの全ての特徴量をグループすると扱いやすくなる。\n",
        "# ここでは、週ごとに特徴量とプライス情報をグループする方法を説明する。\n",
        "# また、週ごとのプライス情報をグループし、weekly returnを計算し、ラベルを作成する方法を説明する。\n",
        "@classmethod\n",
        "def _build_weekly_group(cls, df):\n",
        "    # index情報から、(year, week)の情報を得る。\n",
        "    return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
        "\n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator._build_weekly_group = _build_weekly_group\n",
        "\n",
        "# 特徴量に適用してみる\n",
        "display_markdown('#### features', raw=True)\n",
        "features = headline_features\n",
        "weekly_group = SentimentGenerator._build_weekly_group(df=features)\n",
        "display(weekly_group.head(3))\n",
        "display(weekly_group.tail(3))\n",
        "\n",
        "# プライスに適用してみる。\n",
        "# stock priceの2020年の1週目は、データが存在しないため、2020年の２週目から存在することがわかる。\n",
        "# これらのindexをマッチさせることは後ほど説明する。\n",
        "display_markdown('#### stock price', raw=True)\n",
        "weekly_group = SentimentGenerator._build_weekly_group(stock_price)\n",
        "display(weekly_group.head(3))\n",
        "display(weekly_group.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXOTo9otL5_Z"
      },
      "source": [
        "# 特徴量を週ごとにグループ化してみる\n",
        "weekly_group = SentimentGenerator._build_weekly_group(df=features)\n",
        "features = features.groupby(weekly_group).apply(lambda x: x[:])\n",
        "\n",
        "# 週の情報がindexのlevel 0に付与され、グループされていることがわかる。\n",
        "features.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2DZbaFXL5_Z"
      },
      "source": [
        "# trainとtestを区切る週をboundary_weekとして設定し、train用に用いられる特徴量と、test用に用いられる特徴量を区切る。\n",
        "boundary_week = (2020, 26)\n",
        "train_features = features[features.index.get_level_values(0) <= boundary_week]\n",
        "test_features = features[features.index.get_level_values(0) > boundary_week]\n",
        "\n",
        "display_markdown('#### train_features', raw=True)\n",
        "display(train_features.head(3))\n",
        "display(train_features.tail(3))\n",
        "\n",
        "display_markdown('#### test_features', raw=True)\n",
        "display(test_features.head(3))\n",
        "display(test_features.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IUaAgRAL5_a"
      },
      "source": [
        "# 今回は、stock priceを週ごとにグループし、翌週のopen to closeのreturn、つまりweekly forward returnをビルドする方法を説明する。\n",
        "# weekly forward returnをビルドする前に、まず、その週のopen to closeの weekly returnをビルドしてみよう。\n",
        "# weekly returnをビルドする関数を定義する。\n",
        "def _compute_weekly_return(x):\n",
        "    # その週の初営業日のopenから最終営業日のcloseまでのリターンを計算する。\n",
        "    weekly_return = ((x['close'].iloc[-1] - x['open'].iloc[0]) / x['open'].iloc[0])\n",
        "    \n",
        "    # その日のvolumneが0であるデータは、openが0となっている。\n",
        "    # openが0の場合、np.infの値となっているため、np.nanに変換し除去する。\n",
        "    # 銘柄ごとのリターンを単純平均し、marketのweekly_returnを計算する。\n",
        "    return weekly_return.replace([np.inf, -np.inf], np.nan).dropna().mean()\n",
        "\n",
        "weekly_group = SentimentGenerator._build_weekly_group(df=stock_price)\n",
        "weekly_return = stock_price.groupby(weekly_group).apply(_compute_weekly_return)\n",
        "\n",
        "display(weekly_return.head(3))\n",
        "display(weekly_return.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYDGlFYcL5_a"
      },
      "source": [
        "# weekly_returnをshift(-1)することより、翌週のreturn情報が現在のindexに入るようになる。\n",
        "weekly_fwd_return = weekly_return.shift(-1).dropna()\n",
        "\n",
        "display(weekly_fwd_return.head(3))\n",
        "display(weekly_fwd_return.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAfyyap5L5_a"
      },
      "source": [
        "# trainとtestを区切る週をboundary_weekとして設定し、train用に用いられる特徴量と、test用に用いられる特徴量を区切る。\n",
        "boundary_week = (2020, 26)\n",
        "train_labels = weekly_fwd_return[weekly_fwd_return.index <= boundary_week]\n",
        "test_labels = weekly_fwd_return[weekly_fwd_return.index > boundary_week]\n",
        "\n",
        "display_markdown('#### train_labels', raw=True)\n",
        "display(train_labels.head(3))\n",
        "display(train_labels.tail(3))\n",
        "\n",
        "display_markdown('#### test_labels', raw=True)\n",
        "display(test_labels.head(3))\n",
        "display(test_labels.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fva9HKd3L5_a"
      },
      "source": [
        "# fwd_returnの上げ下げの情報のみをラベルとして用いるため、上げを1.0, 下げを0.0に変換する\n",
        "train_labels = (train_labels >= 0) * 1.0\n",
        "test_labels = (test_labels >= 0) * 1.0\n",
        "\n",
        "display_markdown('#### train_labels', raw=True)\n",
        "display(train_labels.head(3))\n",
        "display(train_labels.tail(3))\n",
        "\n",
        "display_markdown('#### test_labels', raw=True)\n",
        "display(test_labels.head(3))\n",
        "display(test_labels.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmGbxO7tL5_a",
        "scrolled": true
      },
      "source": [
        "# 上記のコードを関数としておとす。\n",
        "@classmethod\n",
        "def build_weekly_features(cls, features, boundary_week):\n",
        "    assert isinstance(boundary_week, tuple)\n",
        "    \n",
        "    weekly_group = cls._build_weekly_group(df=features)\n",
        "    features = features.groupby(weekly_group).apply(lambda x: x[:])\n",
        "\n",
        "    train_features = features[features.index.get_level_values(0) <= boundary_week]\n",
        "    test_features = features[features.index.get_level_values(0) > boundary_week]\n",
        "    \n",
        "    return {'train': train_features, 'test': test_features}\n",
        "\n",
        "@classmethod\n",
        "def build_weekly_labels(cls, stock_price, boundary_week):\n",
        "    def _compute_weekly_return(x):\n",
        "        # その週の初営業日のopenから最終営業日のcloseまでのリターンを計算する。\n",
        "        weekly_return = ((x['close'].iloc[-1] - x['open'].iloc[0]) / x['open'].iloc[0])\n",
        "\n",
        "        # その日のvolumneが0であるデータは、openが0となっている。\n",
        "        # openが0の場合、np.infの値となっているため、np.nanに変換し除去する。\n",
        "        # 銘柄ごとのリターンを単純平均し、marketのweekly_returnを計算する。\n",
        "        return weekly_return.replace([np.inf, -np.inf], np.nan).dropna().mean()\n",
        "\n",
        "    assert isinstance(boundary_week, tuple)\n",
        "\n",
        "    weekly_group = cls._build_weekly_group(df=stock_price)\n",
        "    weekly_fwd_return = stock_price.groupby(weekly_group).apply(_compute_weekly_return).shift(-1).dropna()\n",
        "\n",
        "    train_labels = weekly_fwd_return[weekly_fwd_return.index <= boundary_week]\n",
        "    test_labels = weekly_fwd_return[weekly_fwd_return.index > boundary_week]\n",
        "    \n",
        "    train_labels = (train_labels >= 0) * 1.0\n",
        "    test_labels = (test_labels >= 0) * 1.0\n",
        "    \n",
        "    return {'train': train_labels, 'test': test_labels}\n",
        "\n",
        "# SentimentGeneratorに定義したclassmethodを追加する\n",
        "SentimentGenerator.build_weekly_features = build_weekly_features\n",
        "SentimentGenerator.build_weekly_labels = build_weekly_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrlhxKRFL5_a"
      },
      "source": [
        "### 1.7.7. Pytorch Dataset作成\n",
        "pytorchのモデルを効率よく学習させるためには、Custom datasetと、それを用いたdataloaderを定義する必要がある。ここでは、学習に適合するDatasetを定義する方法を説明する。  \n",
        "Datasetの`__getitem__`は、学習や、テスト時、Dataloaderを介してデータを取得するときに呼ばれ、`__init__`で事前に定義したデータをidからインデックシングする仕組みになっている。  \n",
        "そのため、idが付与されたら、そのidからfeatureとlabelをインデックシングできるような構造で`__init__`でデータを事前に定義しておく必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBe3Ct_ZL5_b"
      },
      "source": [
        "# ここでは、例として、trainデータのみを用いて、__init__で事前定義するデータをビルトしてみよう。\n",
        "# 任意に特徴量としては、headlineを使い、boundary_weekに2020年の26週目を設定。\n",
        "features = headline_features\n",
        "boundary_week = (2020, 26)\n",
        "\n",
        "# 上記で作成したコードを用いて週グループの特徴量とラベルをビルドする。\n",
        "weekly_features = SentimentGenerator.build_weekly_features(features=features, boundary_week=boundary_week)['train']\n",
        "weekly_labels = SentimentGenerator.build_weekly_labels(stock_price=stock_price, boundary_week=boundary_week)['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70MwTr4TL5_b"
      },
      "source": [
        "# 共通する週のみのデータを使うため、共通するindex情報を取得する。\n",
        "mask_index = weekly_features.index.get_level_values(0).unique() & weekly_labels.index\n",
        "display(mask_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeZC_2spL5_b"
      },
      "source": [
        "# 共通するindexのみのデータだけでreindexを行う。\n",
        "weekly_features = weekly_features[weekly_features.index.get_level_values(0).isin(mask_index)]\n",
        "weekly_labels = weekly_labels.reindex(mask_index)\n",
        "\n",
        "display_markdown('#### weekly_features', raw=True)\n",
        "display(weekly_features.head(3))\n",
        "display(weekly_features.tail(3))\n",
        "\n",
        "display_markdown('#### weekly_labels', raw=True)\n",
        "display(weekly_labels.head(3))\n",
        "display(weekly_labels.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Lj1PClL5_b"
      },
      "source": [
        "# idからweekの情報を取得できるよう、id_to_weekをビルドする\n",
        "id_to_week = {id: week for id, week in enumerate(sorted(weekly_labels.index))}\n",
        "id_to_week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN76HIF_L5_b"
      },
      "source": [
        "# 続き、__getitem__で付与されたidから、データを取得するロジックを説明する。\n",
        "# idからweekの情報を取得する\n",
        "# 例として、任意的にid = 10を用いる。\n",
        "id = 10\n",
        "\n",
        "week = id_to_week[id]\n",
        "week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWI6lWrZL5_b"
      },
      "source": [
        "# 学習時のリソース軽減のため、全ての特徴量を入力とするわけではなく、直近n個を入力とする。ここでは1000個として定義する。\n",
        "max_sequence_length = 1000\n",
        "\n",
        "x = weekly_features.xs(week, axis=0, level=0)[-max_sequence_length:]\n",
        "y = weekly_labels[week]\n",
        "\n",
        "# 上記のコードよりidが付与されたとき、idから週の情報を取得し、その週の情報から、特徴量とラベルを手にすることができた。\n",
        "display_markdown('#### 特徴量', raw=True)\n",
        "display(x.head(3))\n",
        "print('shape:', x.shape)\n",
        "\n",
        "display_markdown('#### ラベル', raw=True)\n",
        "display(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlQ9yHZ5L5_b"
      },
      "source": [
        "# pytorchでは、データをtorch.Tensorタイプとして扱うことが要求される。\n",
        "# 以下で、np.ndarrayをtensor形式に変換することができる。\n",
        "x = torch.tensor(x.values, dtype=torch.float)\n",
        "y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "display(x)\n",
        "display(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTJ1LhbcL5_b"
      },
      "source": [
        "# 今回、学習に用いるサンプル数が大変少ないため、過学習防止のため、少し工夫を重ねる。\n",
        "# 全体的な特徴量(ニュースの情報)の順序は維持しつつ、入力とする特徴量を数分割し、その分割の中でシャッフルを行う方法である。\n",
        "# この方法を用いることで、モデルが観察するデータが飛躍的に増加する効果が期待され、過学習を防止に繋がるはずである。\n",
        "def _shuffle_by_local_split(x, split_size=50):\n",
        "    return torch.cat([splitted[torch.randperm(splitted.size()[0])] for splitted in x.split(split_size, dim=0)], dim=0)\n",
        "\n",
        "x = _shuffle_by_local_split(x=x)\n",
        "display(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwc9TwHEL5_c"
      },
      "source": [
        "# 学習中においては、データを一つずつロードし、学習を行うより、複数個を同時に(batchを組み)並列演算を行う方が時間短縮に繋がる。\n",
        "# そのためには、行列を重ねる必要があり、データのシークエンスが同一である必要がある。本章では、このようなシークエンスの異なるデータを扱うため、\n",
        "# max_sequence_lengthを決め、最大のsequenceを合わせ、また、sequenceがmax_sequence_lengthに達しない場合は、前から0を埋め、sequenceを合わせる戦略を取る。\n",
        "\n",
        "# sequenceがmax_sequence_lengthに達しないrandomのtensorをxとして、定義し、以下のコードでzero paddingを行ってみる。\n",
        "x = torch.randn(100, 768)\n",
        "display_markdown('#### Padding前', raw=True)\n",
        "display(x)\n",
        "display(x.shape)\n",
        "\n",
        "if x.size()[0] < max_sequence_length:\n",
        "    x = F.pad(x, pad=(0, 0, max_sequence_length - x.size()[0], 0))\n",
        "    \n",
        "display_markdown('#### Padding後', raw=True)\n",
        "display(x)\n",
        "display(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSTEVI9HL5_c"
      },
      "source": [
        "# 上記のコードをまとめて、Datasetクラスを作成する。\n",
        "class Dataset(_Dataset):\n",
        "    def __init__(self, weekly_features, weekly_labels, max_sequence_length):\n",
        "        # 共通する週のみを使うため、共通するindex情報を取得する\n",
        "        mask_index = (\n",
        "            weekly_features.index.get_level_values(0).unique() & weekly_labels.index\n",
        "        )\n",
        "\n",
        "        # 共通するindexのみのデータだけでreindexを行う。\n",
        "        self.weekly_features = weekly_features[\n",
        "            weekly_features.index.get_level_values(0).isin(mask_index)\n",
        "        ]\n",
        "        self.weekly_labels = weekly_labels.reindex(mask_index)\n",
        "\n",
        "        # idからweekの情報を取得できるよう、id_to_weekをビルドする\n",
        "        self.id_to_week = {\n",
        "            id: week for id, week in enumerate(sorted(weekly_labels.index))\n",
        "        }\n",
        "\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def _shuffle_by_local_split(self, x, split_size=50):\n",
        "        return torch.cat(\n",
        "            [\n",
        "                splitted[torch.randperm(splitted.size()[0])]\n",
        "                for splitted in x.split(split_size, dim=0)\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.weekly_labels)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        # 付与されたidから週の情報を取得し、その週の情報から、特徴量とラベルを取得する。\n",
        "        week = self.id_to_week[id]\n",
        "        x = self.weekly_features.xs(week, axis=0, level=0)[-self.max_sequence_length :]\n",
        "        y = self.weekly_labels[week]\n",
        "\n",
        "        # pytorchでは、データをtorch.Tensorタイプとして扱うことが要求される。\n",
        "        # 全体的な特徴量(ニュースの情報)の順序は維持しつつ、入力とする特徴量を数分割し、その分割の中でシャッフルを行う。\n",
        "        x = self._shuffle_by_local_split(torch.tensor(x.values, dtype=torch.float))\n",
        "        y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "        # max_sequence_lengthに最大のsequenceを合わせ、sequenceがmax_sequence_lengthに達しない場合は、前から0を埋め、sequenceを合わせる\n",
        "        if x.size()[0] < self.max_sequence_length:\n",
        "            x = F.pad(x, pad=(0, 0, self.max_sequence_length - x.size()[0], 0))\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDJBSghDL5_c"
      },
      "source": [
        "### 1.7.8. 特徴量合成モデル作成\n",
        "本章では、特徴量統合モデルとして、LSTMを用いる。また、ラベルとして上げ下げのバイナリ情報を扱っているため、上げ下げの確率を出力とするモデルを定義し、binary_cross_entropyをlossとして学習を行う。出力層でのsentiment score及び、出力出力前層で、より高次元の情報を抽出し特徴量として用いる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-25fszXL5_c"
      },
      "source": [
        "class FeatureCombiner(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, compress_dim=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTMの定義、\n",
        "        # batch_firstより、出力次元の最初がbatchとなる。\n",
        "        # dropoutを用いて、内部状態のconnectionをdropすることより過学習を防ぐ。\n",
        "        # Sequenceがかなり長いため、初期入力された情報の消失が激しいと想定される。それらの防止のため、bidirectionalのモデルを使う。\n",
        "        self.cell = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.5,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        # より高次元の特徴量を抽出できるようにするため、classifierの手前で、compress_dim次元への線形圧縮を行う。\n",
        "        self.compressor = nn.Linear(hidden_size * 2, compress_dim)\n",
        "\n",
        "        # sentiment probabilityの出力層。\n",
        "        self.classifier = nn.Linear(compress_dim, 1)\n",
        "\n",
        "        # outputの範囲を[0, 1]とする。\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力値xから出力までの流れを定義する。\n",
        "        output, _ = self.cell(x)\n",
        "        output = self.sigmoid(self.classifier(self.compressor(output[:, -1, :])))\n",
        "        return output\n",
        "\n",
        "    def extract_feature(self, x):\n",
        "        # 入力値xから特徴量抽出までの流れを定義する。\n",
        "        output, _ = self.cell(x)\n",
        "        output = self.compressor(output[:, -1, :])\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHiud6RL5_c"
      },
      "source": [
        "### 1.7.9. 特徴量合成モデルのハンドラー作成\n",
        "「3. Pytorch Dataset 作成, 4. 特徴量合成モデル作成」 において、データのインデクシングロジックや、特徴量合成モデルは作成することができた。しかし、これらだけではモデル学習は行えない。学習のロジックや、学習されたモデルのセーブとロード、推定、特徴量抽出を実装する必要がある。本章では、FeatureCombinerHandlerのクラスを定義し、そのようなロジックを実装する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFcp_2_pL5_c"
      },
      "source": [
        "class FeatureCombinerHandler:\n",
        "    def __init__(self, feature_combiner_params, store_dir):\n",
        "        # モデル学習及び推論に用いるデバイスを定義する\n",
        "        if torch.cuda.device_count() >= 1:\n",
        "            self.device = 'cuda'\n",
        "            print(\"[+] Set Device: GPU\")\n",
        "        else:\n",
        "            self.device = 'cpu'\n",
        "            print(\"[+] Set Device: CPU\")\n",
        "\n",
        "        # モデルのcheckpointや抽出した特徴量及びsentimentをstoreする場所を定義する。\n",
        "        self.store_dir = store_dir\n",
        "        os.makedirs(store_dir, exist_ok=True)\n",
        "\n",
        "        # 上記で作成したfeaturecombinerを定義する。\n",
        "        self.feature_combiner = FeatureCombiner(**feature_combiner_params).to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        # 学習に用いるoptimizerを定義する。\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            params=self.feature_combiner.parameters(), lr=0.001,\n",
        "        )\n",
        "\n",
        "        # ロス関数の定義\n",
        "        self.criterion = nn.BCELoss().to(self.device)\n",
        "\n",
        "        # モデルのcheck pointが存在する場合、モデルをロードする\n",
        "        self._load_model()\n",
        "\n",
        "    # 学習に必要なデータ(並列のためbatch化されたもの)をサンプルする。\n",
        "    def _sample_xy(self, data_type):\n",
        "        assert data_type in (\"train\", \"val\")\n",
        "\n",
        "        # data_typeより、data_typeに合致したデータを取得するようにしている。\n",
        "        if data_type == \"train\":\n",
        "            # dataloaderをiteratorとして定義し、next関数として毎時のデータをサンプルすることができる。\n",
        "            # Iteratorは全てのデータがサンプルされると、StopIterationのエラーを発するが、そのようなエラーが出たとき、\n",
        "            # Iteratorを再定義し、データをサンプルするようにしている。\n",
        "            try:\n",
        "                x, y = next(self.iterable_train_dataloader)\n",
        "            except StopIteration:\n",
        "                self.iterable_train_dataloader = iter(self.train_dataloader)\n",
        "                x, y = next(self.iterable_train_dataloader)\n",
        "\n",
        "        elif data_type == \"val\":\n",
        "            try:\n",
        "                x, y = next(self.iterable_val_dataloader)\n",
        "            except StopIteration:\n",
        "                self.iterable_val_dataloader = iter(self.val_dataloader)\n",
        "                x, y = next(self.iterable_val_dataloader)\n",
        "\n",
        "        return x.to(self.device), y.to(self.device)\n",
        "\n",
        "    # モデルのパラメータをアップデートするロジック\n",
        "    def _update_params(self, loss):\n",
        "        # ロスから、gradientを逆伝播し、パラメータをアップデートする\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # 学習されたfeature_combinerのパラメータをcheck_pointとしてstoreするロジック\n",
        "    def _save_model(self, epoch):\n",
        "        torch.save(\n",
        "            self.feature_combiner.state_dict(),\n",
        "            os.path.join(self.store_dir, f\"{epoch}.ckpt\"),\n",
        "        )\n",
        "        print(f\"[+] Epoch: {epoch}, Model is saved.\")\n",
        "\n",
        "    # 学習されたcheckpointが存在す場合、feature_combinerにそのパラメータをロードするロジック\n",
        "    def _load_model(self):\n",
        "        # cudaで学習されたモデルなどを、cpu環境下でロードするときはこのパラメータが必要となる。\n",
        "        params_to_load = {}\n",
        "        if self.device == \"cpu\":\n",
        "            params_to_load[\"map_location\"] = torch.device(\"cpu\")\n",
        "\n",
        "        # .ckptファイルを探し、古い順から新しい順にソートする。\n",
        "        check_points = glob(os.path.join(self.store_dir, \"*.ckpt\"))\n",
        "        check_points = sorted(\n",
        "            check_points, key=lambda x: int(x.split(\"/\")[-1].replace(\".ckpt\", \"\")),\n",
        "        )\n",
        "\n",
        "        # check_pointが存在しない場合は、スキップする。\n",
        "        if len(check_points) == 0:\n",
        "            print(\"[!] No exists checkpoint\")\n",
        "            return\n",
        "\n",
        "        # 複数個のchieck_pointが存在する場合、一番最新のものを使い、モデルのパラメータをロードする\n",
        "        check_point = check_points[-1]\n",
        "        self.feature_combiner.load_state_dict(torch.load(check_point, **params_to_load))\n",
        "        print(\"[+] Model is loaded\")\n",
        "\n",
        "    # Datasetからdataloaderを定義するロジック\n",
        "    def _build_dataloader(\n",
        "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
        "    ):\n",
        "        # 上記3で作成したしたdatasetを定義する\n",
        "        dataset = Dataset(\n",
        "            weekly_features=weekly_features,\n",
        "            weekly_labels=weekly_labels,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "        )\n",
        "\n",
        "        # datasetのdataをiterableにロードできるよう、dataloaderを定義する、このとき、shuffle=Trueを渡すことで、データはランダムにサンプルされるようになる。\n",
        "        return DataLoader(dataset=dataset, shuffle=True, **dataloader_params)\n",
        "\n",
        "    # train用に、featuresとlabelsを渡し、datasetを定義し、dataloaderを定義するロジック\n",
        "    def set_train_dataloader(\n",
        "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
        "    ):\n",
        "        self.train_dataloader = self._build_dataloader(\n",
        "            dataloader_params=dataloader_params,\n",
        "            weekly_features=weekly_features,\n",
        "            weekly_labels=weekly_labels,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "        )\n",
        "\n",
        "        # dataloaderからiteratorを定義する\n",
        "        # iteratorはnext関数よりデータをサンプルすることが可能となる。\n",
        "        self.iterable_train_dataloader = iter(self.train_dataloader)\n",
        "\n",
        "    # validation用に、featuresとlabelsを渡し、datasetを定義し、dataloaderを定義するロジック\n",
        "    def set_val_dataloader(\n",
        "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
        "    ):\n",
        "        self.val_dataloader = self._build_dataloader(\n",
        "            dataloader_params=dataloader_params,\n",
        "            weekly_features=weekly_features,\n",
        "            weekly_labels=weekly_labels,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "        )\n",
        "\n",
        "        # dataloaderからiteratorを定義する\n",
        "        # iteratorはnext関数よりデータをサンプルすることが可能となる。\n",
        "        self.iterable_val_dataloader = iter(self.val_dataloader)\n",
        "\n",
        "    # 学習ロジック\n",
        "    def train(self, n_epoch):\n",
        "        # n_epochの回数分、全学習データを複数回用いて学習する。\n",
        "        for epoch in range(n_epoch):\n",
        "\n",
        "            # 各々のepochごとのaverage lossを表示するため、lossをstoreするリストを定義する。\n",
        "            train_losses = []\n",
        "            test_losses = []\n",
        "\n",
        "            # train_dataloaderの長さは、全ての学習データを一度用いるときの長さと同様である。\n",
        "            # batchを組むと、その分train_dataloaderの長さは可変し、ちょうど一度全てのデータで学習できる長さを返す。\n",
        "            for iter_ in tqdm(range(len(self.train_dataloader))):\n",
        "                # パラメータをtrainableにするため、feature_combinerをtrainモードにする。\n",
        "                self.feature_combiner.train()\n",
        "\n",
        "                # trainデータをサンプルする。\n",
        "                x, y = self._sample_xy(data_type=\"train\")\n",
        "\n",
        "                # feature_combinerに特徴量を入力し、sentiment scoreを取得する。\n",
        "                preds = self.feature_combiner(x=x)\n",
        "\n",
        "                # sentiment scoreとラベルとのロスを計算する。\n",
        "                train_loss = self.criterion(preds, y.view(-1, 1))\n",
        "\n",
        "                # 計算されたロスは、後ほどepochごとのdisplayに使用するため、storeしておく。\n",
        "                train_losses.append(train_loss.detach().cpu())\n",
        "\n",
        "                # lossから、gradientを逆伝播させ、パラメータをupdateする。\n",
        "                self._update_params(loss=train_loss)\n",
        "\n",
        "                # validation用のロースを計算する。\n",
        "                # 毎回計算を行うとコストがかかってくるので、iter_毎5回ごとに計算を行う。\n",
        "                if iter_ % 5 == 0:\n",
        "\n",
        "                    # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
        "                    # evalモードでは、dropoutの影響を受けない。\n",
        "                    self.feature_combiner.eval()\n",
        "\n",
        "                    # 各パラメータごとのgradientを計算するとリソースが高まる。\n",
        "                    # evaluationの時には、gradient情報を持たせないことで、メモリーの節約に繋がる。\n",
        "                    with torch.no_grad():\n",
        "                        # validationデータをサンプルする\n",
        "                        x, y = self._sample_xy(data_type=\"val\")\n",
        "\n",
        "                        # feature_combinerに特徴量を入力し、sentiment scoreを取得する。\n",
        "                        preds = self.feature_combiner(x=x)\n",
        "\n",
        "                        # sentiment scoreとラベルとのロスを計算する。\n",
        "                        test_loss = self.criterion(preds, y.view(-1, 1))\n",
        "\n",
        "                        # 計算されたロスは、後ほどepochごとのdisplayに使用するため、storeしておく。\n",
        "                        test_losses.append(test_loss.detach().cpu())\n",
        "\n",
        "            # 毎epoch終了後、平均のロスをプリントする。\n",
        "            print(\n",
        "                f\"epoch: {epoch}, train_loss: {np.mean(train_losses):.4f}, val_loss: {np.mean(test_losses):.4f}\"\n",
        "            )\n",
        "\n",
        "            # 毎epoch終了後、モデルのパラメータをstoreする。\n",
        "            self._save_model(epoch=epoch)\n",
        "\n",
        "    # 特徴量から、合成特徴量を抽出するロジック\n",
        "    def combine_features(self, features):\n",
        "        # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
        "        self.feature_combiner.eval()\n",
        "\n",
        "        # gradient情報を持たせないことで、メモリーの節約する。\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 特徴量をfeature_combinerのextract_feature関数に入力し、出力層手前の特徴量を抽出する。\n",
        "            # 抽出するとき、tensorをcpu上に落とし、np.ndarray形式に変換する。\n",
        "            return (\n",
        "                self.feature_combiner.extract_feature(\n",
        "                    x=torch.tensor(features, dtype=torch.float).to(self.device)\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "\n",
        "    # 特徴量から、翌週のsentimentを予測するロジック\n",
        "    def predict_sentiment(self, features):\n",
        "        # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
        "        self.feature_combiner.eval()\n",
        "\n",
        "        # gradient情報を持たせないことで、メモリーの節約する。\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 特徴量をfeature_combinerに入力し、sentiment scoreを抽出する。\n",
        "            # 抽出するとき、tensorをcpu上に落とし、np.ndarray形式に変換する。\n",
        "            return (\n",
        "                self.feature_combiner(x=torch.tensor(features, dtype=torch.float).to(self.device))\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "\n",
        "    # weeklyグループされた特徴量を入力に、合成特徴量もしくは、sentiment scoreを抽出するロジック\n",
        "    def generate_by_weekly_features(\n",
        "        self, weekly_features, generate_target, max_sequence_length\n",
        "    ):\n",
        "        assert generate_target in (\"features\", \"sentiment\")\n",
        "        generate_func = getattr(\n",
        "            self,\n",
        "            {\"features\": \"combine_features\", \"sentiment\": \"predict_sentiment\"}[\n",
        "                generate_target\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        # グループごとに特徴量もしくは、sentiment scoreを抽出し、最終的に重ねて返すため、リストを作成する。\n",
        "        outputs = []\n",
        "\n",
        "        # ユニークな週indexを取得する。\n",
        "        weeks = sorted(weekly_features.index.get_level_values(0).unique())\n",
        "\n",
        "        for week in tqdm(weeks):\n",
        "            # 各週ごとの特徴量を取得し、直近から、max_sequence_length分切る。\n",
        "            features = weekly_features.xs(week, axis=0, level=0)[-max_sequence_length:]\n",
        "\n",
        "            # 特徴量をモデルに入力し、合成特徴量もしくは、sentiment scoreを抽出し、outputsにappendする。\n",
        "            # np.expand_dims(features, axis=0)を用いる理由は、特徴量合成機の入力期待値は、dimention0がbatchであるが、\n",
        "            # featuresは、[1000, 768]の次元をもち、これらをunsqueezeし、[1, 1000, 768]に変換する必要がある。\n",
        "            outputs.append(generate_func(features=np.expand_dims(features, axis=0)))\n",
        "\n",
        "        # outputsを重ね、indexの情報とともにpd.DataFrame形式として返す。\n",
        "        return pd.DataFrame(np.concatenate(outputs, axis=0), index=weeks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zr2wQiVL5_d"
      },
      "source": [
        "### 1.7.10. 特徴量合成モデルの学習及び特徴量合成\n",
        "ここでは、実際作成したコードを元に、特徴量合成機の学習を行う。さらに、学習されたモデルを用いて、特徴量、sentiment scoreを抽出してみる。\n",
        "モデル学習において、過学習防止のため、二つのランダム性を与えた。ランダム性を固定し、再現可能にするため、seed若しくはrandom_stateを固定する方法を取ることができる。しかし、実装においては二つのランダム性を同時に左右しないといけないことから、コードの難易度が少々高まってしまうため、本チュートリアルでは省くとする。学習の実行毎に類似であるが少し異なる結果となり得ることに留意しよう。また、推論ではランダム性の影響はない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw06DvVZL5_d"
      },
      "source": [
        "# feature_combiner_handlerを定義する。そのとき、feature_combiner_paramsに{\"input_size\": 768, \"hidden_size\": 128}を渡しているが、\n",
        "# これは、BERTから抽出した特徴量のサイズが768、LSTMに入力する次元が、[sequence, input_size]を持つため、\"input_size\"に768を与え、\n",
        "# LSTMが持つ内部状態のパラメータの次元においては、適切な値をしておく。ここでは、128次元と設置しているが、過学習の恐れが高いとき、また、パラメータを減らしても十分学習受容量がある時には、より低い値をセットしよう。\n",
        "feature_combiner_handler = FeatureCombinerHandler(feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128}, store_dir=f'{CONFIG[\"base_path\"]}/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fivIt6zPL5_d"
      },
      "source": [
        "# 続き、学習用と、validation用のデータをビルドする必要がある。\n",
        "# 一般的に、データは、学習に用いられる学習データと、パラメータなどの調整のため用いられるvalidationデータ、学習後のモデルを評価するためのtestデータに分けるTrain-Test- Validation分割を取ることが多いが、\n",
        "# 本チュートリアルでは、データ数がかなり少ないため、Train-Test分割を行い、validation　lossの表示時にもtestデータを用いて行う。\n",
        "# 上記で作成したbuild_weekly_featuresとbuild_weekly_labelsを用いて、データセットを生成する。\n",
        "boundary_week = (2020, 26)\n",
        "weekly_features = SentimentGenerator.build_weekly_features(features, boundary_week)\n",
        "weekly_labels = SentimentGenerator.build_weekly_labels(stock_price, boundary_week)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kd6UTErL5_d"
      },
      "source": [
        "# 学習を行う前に、データのサンプル及び、batch処理を行ってくれるdataloaderをビルドする必要がある。\n",
        "\n",
        "# train dataloaderをsetする。\n",
        "# このとき、batch_sizeを4にすることで、4つのデータを並列に学習し、\n",
        "# num_workersを2にすることでdataloaderはcpu 2coreを用いて、並列的にロードされる。\n",
        "# max_sequence_lengthを1000にすることで、学習中には1000個のsequenceをmaxとして入力する。\n",
        "feature_combiner_handler.set_train_dataloader(\n",
        "    dataloader_params={\n",
        "        \"batch_size\": 4,\n",
        "        \"num_workers\": 1,\n",
        "    },\n",
        "    weekly_features=weekly_features['train'],\n",
        "    weekly_labels=weekly_labels['train'], \n",
        "    max_sequence_length=1000\n",
        ")\n",
        "\n",
        "# validation dataloaderをsetする。\n",
        "feature_combiner_handler.set_val_dataloader(\n",
        "    dataloader_params={\n",
        "        \"batch_size\": 4,\n",
        "        \"num_workers\": 1,\n",
        "    },\n",
        "    weekly_features=weekly_features['test'],\n",
        "    weekly_labels=weekly_labels['test'], \n",
        "    max_sequence_length=1000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y3raBpZL5_d"
      },
      "source": [
        "# 学習を行う。\n",
        "# n_epochは全てのデータを一度用いた学習回数を表し、ここでは、テストであるため1と設定する。\n",
        "feature_combiner_handler.train(n_epoch=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKmuUlnL5_d"
      },
      "source": [
        "# 学習後、特徴量抽出機から、特徴量やsentiment scoreを抽出することができる。\n",
        "# 今回は、一度学習されたモデルをロードし、特徴量とsentiment scoreを抽出する。\n",
        "\n",
        "# 上で定義した、feature_combiner_handlerを同様に定義すると、check_pointを探し、モデルがロードされる。\n",
        "feature_combiner_handler = FeatureCombinerHandler(feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128}, store_dir=f'{CONFIG[\"base_path\"]}/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_7iRODOL5_d"
      },
      "source": [
        "# sentiment scoreは以下のように取得できる\n",
        "# max_sequence_lengthは学習時と同様に、直近から利用する特徴量の最大の数を決めることができるが、\n",
        "# 評価時には、十分長い(全部かほぼ全部)の特徴量を合成するため、10000を与えている。\n",
        "sentiment_score = feature_combiner_handler.generate_by_weekly_features(weekly_features=weekly_features['test'], generate_target='sentiment', max_sequence_length=10000)\n",
        "display(sentiment_score.head(3))\n",
        "display(sentiment_score.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46VyG2IZL5_d"
      },
      "source": [
        "# 合成特徴量は以下のように取得できる\n",
        "# max_sequence_lengthは学習時と同様に、直近から利用する特徴量の最大の数を決めることができるが、\n",
        "# 評価時には、十分長い(全部かほぼ全部)の特徴量を合成するため、10000を与えている。\n",
        "combined_features = feature_combiner_handler.generate_by_weekly_features(weekly_features=weekly_features['test'], generate_target='features', max_sequence_length=10000)\n",
        "display(combined_features.head(3))\n",
        "display(combined_features.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wujMAkCXL5_d"
      },
      "source": [
        "# これらのpd.Dataframeをstoreし、ロードしてみる。\n",
        "# dataframeのstore時には、csv, pickle, parquet, h5など、さまざまな方法が存在する。\n",
        "# 本章では扱いやすいpickleでのstoreを説明する。\n",
        "\n",
        "# 以下のコマンドよりpickle形式で、storeすることができる。\n",
        "combined_features.to_pickle(f'{CONFIG[\"base_path\"]}/test/test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgf64eEUL5_e",
        "scrolled": true
      },
      "source": [
        "# 以下のコマンドよりファイルをロードすることができる。\n",
        "combined_features = pd.read_pickle(f'{CONFIG[\"base_path\"]}/test/test.pkl')\n",
        "\n",
        "display(combined_features.head(3))\n",
        "display(combined_features.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbuI_mquL5_e"
      },
      "source": [
        "# 上で作成したFeatureCombinerHandlerは、本番環境において特徴量の合成時にも用いられる。SentimentGeneratorに、インスタンスとしてビルドしておこう。\n",
        "SentimentGenerator.headline_feature_combiner_handler = FeatureCombinerHandler(feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128}, store_dir=f'{CONFIG[\"base_path\"]}/headline_features')\n",
        "SentimentGenerator.keywords_feature_combiner_handler = FeatureCombinerHandler(feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128}, store_dir=f'{CONFIG[\"base_path\"]}/keywords_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHm9fVALL5_e",
        "scrolled": false
      },
      "source": [
        "# 上記のコードをまとめ、headlineとkeywordsそれぞれにおいて、特徴量合成機を学習し、特徴量を抽出するコードを作成する。\n",
        "boundary_week = (2020, 26)\n",
        "for features, feature_type in [(headline_features, 'headline_features'), (keywords_features, 'keywords_features')]:\n",
        "    # feature_typeに合致するfeature_combiner_handlerをSentimentGeneratorから取得する。\n",
        "    feature_combiner_handler = {\n",
        "        'headline_features': SentimentGenerator.headline_feature_combiner_handler,\n",
        "        'keywords_features': SentimentGenerator.keywords_feature_combiner_handler,\n",
        "    }[feature_type]\n",
        "    \n",
        "    # 学習及び、validationに用いる、データをビルドする\n",
        "    weekly_features = SentimentGenerator.build_weekly_features(features, boundary_week)\n",
        "    weekly_labels = SentimentGenerator.build_weekly_labels(stock_price, boundary_week)\n",
        "\n",
        "    # train dataloaderをsetする。\n",
        "    # このとき、batch_sizeを4にすることで、4つのデータを並列に学習し、\n",
        "    # num_workersを2にすることでdataloaderはcpu 2coreを用いて、並列的にロードされる。\n",
        "    feature_combiner_handler.set_train_dataloader(\n",
        "        dataloader_params={\n",
        "            \"batch_size\": 4,\n",
        "            \"num_workers\": 1,\n",
        "        },\n",
        "        weekly_features=weekly_features['train'],\n",
        "        weekly_labels=weekly_labels['train'], \n",
        "        max_sequence_length=1000\n",
        "    )\n",
        "\n",
        "    # validation dataloaderをsetする。\n",
        "    feature_combiner_handler.set_val_dataloader(\n",
        "        dataloader_params={\n",
        "            \"batch_size\": 4,\n",
        "            \"num_workers\": 1,\n",
        "        },\n",
        "        weekly_features=weekly_features['test'],\n",
        "        weekly_labels=weekly_labels['test'], \n",
        "        max_sequence_length=1000\n",
        "    )\n",
        "\n",
        "    # 学習\n",
        "    feature_combiner_handler.train(n_epoch=20)\n",
        "\n",
        "    # 特徴量及びsentiment scoreを抽出し、pickleとしてstoreする。\n",
        "    feature_combiner_handler.generate_by_weekly_features(weekly_features=weekly_features['test'], generate_target='sentiment', max_sequence_length=10000).to_pickle(os.path.join(f'{CONFIG[\"base_path\"]}/{feature_type}', 'LSTM_sentiment.pkl'))\n",
        "    feature_combiner_handler.generate_by_weekly_features(weekly_features=weekly_features['test'], generate_target='features', max_sequence_length=10000).to_pickle(os.path.join(f'{CONFIG[\"base_path\"]}/{feature_type}', 'LSTM_features.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QdUsijlL5_e"
      },
      "source": [
        "### 1.7.11. 本番提出用のクラス作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZE0l-weL5_e"
      },
      "source": [
        "# ここまでの、ニュースのデータをロード、前処理、BERT特徴量、LSTMによる特徴量合成までの一連の処理を\n",
        "# `generate_lstm_features`関数として、SentimentGeneratorクラスに追加する。\n",
        "\n",
        "@classmethod\n",
        "def generate_lstm_features(\n",
        "    cls,\n",
        "    article_path,\n",
        "    start_dt=None,\n",
        "    boundary_week=(2020, 26),\n",
        "    target_feature_types=None,\n",
        "):\n",
        "    # target_feature_typesが指定されなかったらデフォルト値設定\n",
        "    dfault_target_feature_types = [\n",
        "        \"headline\",\n",
        "        \"keywords\",\n",
        "    ]\n",
        "    if target_feature_types is None:\n",
        "        target_feature_types = dfault_target_feature_types\n",
        "\n",
        "    # feature typeが想定通りであることを確認\n",
        "    assert set(target_feature_types).issubset(dfault_target_feature_types)\n",
        "\n",
        "    # ニュースデータをロードする。\n",
        "    articles = cls.load_articles(start_dt=start_dt, path=article_path)\n",
        "\n",
        "    # 前処理を行う。\n",
        "    articles = cls.normalize_articles(articles)\n",
        "    articles = cls.handle_punctuations_in_articles(articles)\n",
        "    articles = cls.drop_remove_list_words(articles)\n",
        "\n",
        "    # headlineとkeywordsの特徴量をdict型で返す。\n",
        "    lstm_features = {}\n",
        "\n",
        "    for feature_type in target_feature_types:\n",
        "        # コーパス全体のBERT特徴量を抽出する。\n",
        "        features = cls.generate_features_by_texts(texts=articles[feature_type])\n",
        "\n",
        "        # feature_typeに合致するfeature_combiner_handlerをclsから取得する。\n",
        "        feature_combiner_handler = {\n",
        "            \"headline\": cls.headline_feature_combiner_handler,\n",
        "            \"keywords\": cls.keywords_feature_combiner_handler,\n",
        "        }[feature_type]\n",
        "\n",
        "        # 特徴量を週毎のグループ化する。\n",
        "        weekly_features = cls.build_weekly_features(features, boundary_week)[\"test\"]\n",
        "\n",
        "        # Sentiment scoreを抽出する。\n",
        "        lstm_features[\n",
        "            f\"{feature_type}_features\"\n",
        "        ] = feature_combiner_handler.generate_by_weekly_features(\n",
        "            weekly_features=weekly_features,\n",
        "            generate_target=\"sentiment\",\n",
        "            max_sequence_length=10000,\n",
        "        )\n",
        "\n",
        "    return lstm_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkdCLCCML5_e"
      },
      "source": [
        "# ここまでSentimentGeneratorに追加したclassmethodをまとめ、SentimentGeneratorクラスを仕上げる。\n",
        "\n",
        "class SentimentGenerator(object):\n",
        "    article_columns = None\n",
        "    device = None\n",
        "    feature_extractor = None\n",
        "    headline_feature_combiner_handler = None\n",
        "    keywords_feature_combiner_handler = None\n",
        "    punctuation_replace_dict = None\n",
        "    punctuation_remove_list = None\n",
        "\n",
        "    @classmethod\n",
        "    def initialize(cls, base_dir=\"../model\"):\n",
        "        # 使用するcolumnをセットする。\n",
        "        cls.article_columns = [\"publish_datetime\", \"headline\", \"keywords\"]\n",
        "\n",
        "        # BERT特徴量抽出機をセットする。\n",
        "        cls._set_device()\n",
        "        cls._build_feature_extractor()\n",
        "        cls._build_tokenizer()\n",
        "\n",
        "        # LSTM特徴量合成機をセットする。\n",
        "        cls.headline_feature_combiner_handler = FeatureCombinerHandler(\n",
        "            feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128},\n",
        "            store_dir=f\"{base_dir}/headline_features\",\n",
        "        )\n",
        "        cls.keywords_feature_combiner_handler = FeatureCombinerHandler(\n",
        "            feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128},\n",
        "            store_dir=f\"{base_dir}/keywords_features\",\n",
        "        )\n",
        "\n",
        "        # 置換すべき記号のdictionaryを作成する。\n",
        "        JISx0208_replace_dict = {\n",
        "            \"髙\": \"高\",\n",
        "            \"﨑\": \"崎\",\n",
        "            \"濵\": \"浜\",\n",
        "            \"賴\": \"頼\",\n",
        "            \"瀨\": \"瀬\",\n",
        "            \"德\": \"徳\",\n",
        "            \"蓜\": \"配\",\n",
        "            \"昻\": \"昂\",\n",
        "            \"桒\": \"桑\",\n",
        "            \"栁\": \"柳\",\n",
        "            \"犾\": \"犹\",\n",
        "            \"琪\": \"棋\",\n",
        "            \"裵\": \"裴\",\n",
        "            \"魲\": \"鱸\",\n",
        "            \"羽\": \"羽\",\n",
        "            \"焏\": \"丞\",\n",
        "            \"祥\": \"祥\",\n",
        "            \"曻\": \"昇\",\n",
        "            \"敎\": \"教\",\n",
        "            \"澈\": \"徹\",\n",
        "            \"曺\": \"曹\",\n",
        "            \"黑\": \"黒\",\n",
        "            \"塚\": \"塚\",\n",
        "            \"閒\": \"間\",\n",
        "            \"彅\": \"薙\",\n",
        "            \"匤\": \"匡\",\n",
        "            \"冝\": \"宜\",\n",
        "            \"埇\": \"甬\",\n",
        "            \"鮏\": \"鮭\",\n",
        "            \"伹\": \"但\",\n",
        "            \"杦\": \"杉\",\n",
        "            \"罇\": \"樽\",\n",
        "            \"柀\": \"披\",\n",
        "            \"﨤\": \"返\",\n",
        "            \"寬\": \"寛\",\n",
        "            \"神\": \"神\",\n",
        "            \"福\": \"福\",\n",
        "            \"礼\": \"礼\",\n",
        "            \"贒\": \"賢\",\n",
        "            \"逸\": \"逸\",\n",
        "            \"隆\": \"隆\",\n",
        "            \"靑\": \"青\",\n",
        "            \"飯\": \"飯\",\n",
        "            \"飼\": \"飼\",\n",
        "            \"緖\": \"緒\",\n",
        "            \"埈\": \"峻\",\n",
        "        }\n",
        "\n",
        "        cls.punctuation_replace_dict = {\n",
        "            **JISx0208_replace_dict,\n",
        "            \"《\": \"〈\",\n",
        "            \"》\": \"〉\",\n",
        "            \"『\": \"「\",\n",
        "            \"』\": \"」\",\n",
        "            \"“\": '\"',\n",
        "            \"!!\": \"!\",\n",
        "            \"〔\": \"[\",\n",
        "            \"〕\": \"]\",\n",
        "            \"χ\": \"x\",\n",
        "        }\n",
        "\n",
        "        # 取り除く記号リスト。\n",
        "        cls.punctuation_remove_list = [\n",
        "            \"|\",\n",
        "            \"■\",\n",
        "            \"◆\",\n",
        "            \"●\",\n",
        "            \"★\",\n",
        "            \"☆\",\n",
        "            \"♪\",\n",
        "            \"〃\",\n",
        "            \"△\",\n",
        "            \"○\",\n",
        "            \"□\",\n",
        "        ]\n",
        "\n",
        "    @classmethod\n",
        "    def _set_device(cls):\n",
        "        # 使用可能なgpuがある場合、そちらを利用し特徴量抽出を行う\n",
        "        if torch.cuda.device_count() >= 1:\n",
        "            cls.device = \"cuda\"\n",
        "            print(\"[+] Set Device: GPU\")\n",
        "        else:\n",
        "            cls.device = \"cpu\"\n",
        "            print(\"[+] Set Device: CPU\")\n",
        "\n",
        "    @classmethod\n",
        "    def _build_feature_extractor(cls):\n",
        "        # 特徴量抽出のため事前学習済みBERTモデルを用いる。\n",
        "        # ここでは、\"cl-tohoku/bert-base-japanese-whole-word-masking\"モデルを使用しているが、異なる日本語BERTモデルを用いても良い。\n",
        "        cls.feature_extractor = transformers.BertModel.from_pretrained(\n",
        "            \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
        "            return_dict=True,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "\n",
        "        # 使用するdeviceを指定\n",
        "        cls.feature_extractor = cls.feature_extractor.to(cls.device)\n",
        "\n",
        "        # 今回、学習は行わない。特徴量抽出のためなので、評価モードにセットする。\n",
        "        cls.feature_extractor.eval()\n",
        "\n",
        "        print(\"[+] Built feature extractor\")\n",
        "\n",
        "    @classmethod\n",
        "    def _build_tokenizer(cls):\n",
        "        # BERTモデルの入力とするコーパスはそのBERTモデルが学習された時と同様の前処理を行う必要がある。\n",
        "        # 今回使用する\"cl-tohoku/bert-base-japanese-whole-word-masking\"モデルは、mecab-ipadic-NEologdによりトークナイズされ、その後Wordpiece subword encoderよりsubword化している。\n",
        "        # Subwordとは形態素の類似な概念として、単語をより小さい意味のある単位に変換したものである。\n",
        "        # transformersのBertJapaneseTokenizerは、その事前学習モデルの学習時と同様の前処理を簡単に使用することができる。\n",
        "        # この章ではBertJapaneseTokenizerを利用し、トークナイズ及びsubword化を行う。\n",
        "        cls.bert_tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "            \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "        )\n",
        "        print(\"[+] Built bert tokenizer\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_articles(cls, path, start_dt=None, end_dt=None):\n",
        "        # csvをロードする\n",
        "        # headline、keywordsをcolumnとして使用。publish_datetimeをindexとして使用。\n",
        "        articles = pd.read_csv(path)[cls.article_columns].set_index(\"publish_datetime\")\n",
        "\n",
        "        # str形式のdatetimeをpd.Timestamp形式に変換\n",
        "        articles.index = pd.to_datetime(articles.index)\n",
        "\n",
        "        # NaN値を取り除く\n",
        "        articles = articles.dropna()\n",
        "\n",
        "        # 必要な場合、使用するデータの範囲を指定する\n",
        "        return articles[start_dt:end_dt]\n",
        "\n",
        "    @classmethod\n",
        "    def normalize_articles(cls, articles):\n",
        "        articles = articles.copy()\n",
        "\n",
        "        # 欠損値を取り除く\n",
        "        articles = articles.dropna()\n",
        "\n",
        "        for column in articles.columns:\n",
        "            # スペース(全角スペースを含む)はneologdn正規化時に全て除去される。\n",
        "            # ここでは、スペースの情報が失われないように、スペースを全て改行に書き換え、正規化後スペースに再変換する。\n",
        "            articles[column] = articles[column].apply(lambda x: \"\\n\".join(x.split()))\n",
        "\n",
        "            # neologdnを使って正規化を行う。\n",
        "            articles[column] = articles[column].apply(lambda x: neologdn.normalize(x))\n",
        "\n",
        "            # 改行をスペースに置換する。\n",
        "            articles[column] = articles[column].str.replace(\"\\n\", \" \")\n",
        "\n",
        "        return articles\n",
        "\n",
        "    @classmethod\n",
        "    def handle_punctuations_in_articles(cls, articles):\n",
        "        articles = articles.copy()\n",
        "\n",
        "        for column in articles.columns:\n",
        "            # punctuation_remove_listに含まれる記号を除去する\n",
        "            articles[column] = articles[column].str.replace(\n",
        "                fr\"[{''.join(cls.punctuation_remove_list)}]\", \"\"\n",
        "            )\n",
        "\n",
        "            # punctuation_replace_dictに含まれる記号を置換する\n",
        "            for replace_base, replace_target in cls.punctuation_replace_dict.items():\n",
        "                articles[column] = articles[column].str.replace(\n",
        "                    replace_base, replace_target\n",
        "                )\n",
        "\n",
        "            # unicode正規化を行う\n",
        "            articles[column] = articles[column].apply(\n",
        "                lambda x: unicodedata.normalize(\"NFKC\", x)\n",
        "            )\n",
        "\n",
        "        return articles\n",
        "\n",
        "    @classmethod\n",
        "    def drop_remove_list_words(cls, articles, remove_list_words=[\"人事\"]):\n",
        "        articles = articles.copy()\n",
        "\n",
        "        for remove_list_word in remove_list_words:\n",
        "            # headlineもしくは、keywordsどちらかでremove_list_wordを含むニュース記事のindexマスクを作成。\n",
        "            drop_mask = articles[\"headline\"].str.contains(remove_list_word) | articles[\n",
        "                \"keywords\"\n",
        "            ].str.contains(remove_list_word)\n",
        "\n",
        "            # remove_list_wordを含まないニュースだけに精製する。\n",
        "            articles = articles[~drop_mask]\n",
        "\n",
        "        return articles\n",
        "\n",
        "    @classmethod\n",
        "    def build_inputs(cls, texts, max_length=512):\n",
        "        input_ids = []\n",
        "        token_type_ids = []\n",
        "        attention_mask = []\n",
        "        for text in texts:\n",
        "            encoded = cls.bert_tokenizer.encode_plus(\n",
        "                text,\n",
        "                None,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_token_type_ids=True,\n",
        "                truncation=True,\n",
        "            )\n",
        "\n",
        "            input_ids.append(encoded[\"input_ids\"])\n",
        "            token_type_ids.append(encoded[\"token_type_ids\"])\n",
        "            attention_mask.append(encoded[\"attention_mask\"])\n",
        "\n",
        "        # torchモデルに入力するためにはtensor形式に変え、deviceを指定する必要がある。\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(cls.device)\n",
        "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).to(cls.device)\n",
        "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(cls.device)\n",
        "\n",
        "        return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "    @classmethod\n",
        "    def generate_features(cls, input_ids, token_type_ids, attention_mask):\n",
        "        output = cls.feature_extractor(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        features = output[\"hidden_states\"][-2].mean(dim=1).cpu().detach().numpy()\n",
        "\n",
        "        return features\n",
        "\n",
        "    @classmethod\n",
        "    def generate_features_by_texts(cls, texts, batch_size=2, max_length=512):\n",
        "        n_batch = math.ceil(len(texts) / batch_size)\n",
        "\n",
        "        features = []\n",
        "        for idx in tqdm(range(n_batch)):\n",
        "            input_ids, token_type_ids, attention_mask = cls.build_inputs(\n",
        "                texts=texts[batch_size * idx : batch_size * (idx + 1)],\n",
        "                max_length=max_length,\n",
        "            )\n",
        "\n",
        "            features.append(\n",
        "                cls.generate_features(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids=token_type_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        features = np.concatenate(features, axis=0)\n",
        "\n",
        "        # 抽出した特徴量はnp.ndarray形式となっており、これらは、日付の情報を失っているため、pd.DataFrame形式に変換する。\n",
        "        return pd.DataFrame(features, index=texts.index)\n",
        "\n",
        "    @classmethod\n",
        "    def _build_weekly_group(cls, df):\n",
        "        # index情報から、(year, week)の情報を得る。\n",
        "        return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
        "\n",
        "    @classmethod\n",
        "    def build_weekly_features(cls, features, boundary_week):\n",
        "        assert isinstance(boundary_week, tuple)\n",
        "\n",
        "        weekly_group = cls._build_weekly_group(df=features)\n",
        "        features = features.groupby(weekly_group).apply(lambda x: x[:])\n",
        "\n",
        "        train_features = features[features.index.get_level_values(0) <= boundary_week]\n",
        "        test_features = features[features.index.get_level_values(0) > boundary_week]\n",
        "\n",
        "        return {\"train\": train_features, \"test\": test_features}\n",
        "\n",
        "    @classmethod\n",
        "    def generate_lstm_features(\n",
        "        cls,\n",
        "        article_path,\n",
        "        start_dt=None,\n",
        "        boundary_week=(2020, 26),\n",
        "        target_feature_types=None,\n",
        "    ):\n",
        "        # target_feature_typesが指定されなかったらデフォルト値設定\n",
        "        dfault_target_feature_types = [\n",
        "            \"headline\",\n",
        "            \"keywords\",\n",
        "        ]\n",
        "        if target_feature_types is None:\n",
        "            target_feature_types = dfault_target_feature_types\n",
        "        # feature typeが想定通りであることを確認\n",
        "        assert set(target_feature_types).issubset(dfault_target_feature_types)\n",
        "\n",
        "        # ニュースデータをロードする。\n",
        "        articles = cls.load_articles(start_dt=start_dt, path=article_path)\n",
        "\n",
        "        # 前処理を行う。\n",
        "        articles = cls.normalize_articles(articles)\n",
        "        articles = cls.handle_punctuations_in_articles(articles)\n",
        "        articles = cls.drop_remove_list_words(articles)\n",
        "\n",
        "        # headlineとkeywordsの特徴量をdict型で返す。\n",
        "        lstm_features = {}\n",
        "\n",
        "        for feature_type in target_feature_types:\n",
        "            # コーパス全体のBERT特徴量を抽出する。\n",
        "            features = cls.generate_features_by_texts(texts=articles[feature_type])\n",
        "\n",
        "            # feature_typeに合致するfeature_combiner_handlerをclsから取得する。\n",
        "            feature_combiner_handler = {\n",
        "                \"headline\": cls.headline_feature_combiner_handler,\n",
        "                \"keywords\": cls.keywords_feature_combiner_handler,\n",
        "            }[feature_type]\n",
        "\n",
        "            # 特徴量を週毎のグループ化する。\n",
        "            weekly_features = cls.build_weekly_features(features, boundary_week)[\"test\"]\n",
        "\n",
        "            # Sentiment scoreを抽出する。\n",
        "            lstm_features[\n",
        "                f\"{feature_type}_features\"\n",
        "            ] = feature_combiner_handler.generate_by_weekly_features(\n",
        "                weekly_features=weekly_features,\n",
        "                generate_target=\"sentiment\",\n",
        "                max_sequence_length=10000,\n",
        "            )\n",
        "\n",
        "        return lstm_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5siN3HANL5_f"
      },
      "source": [
        "## 1.7.12. 合成特徴量の解析\n",
        "今回前章において、作成したlstmモデルからfeaturesとsentimentを抽出した。sentimentはfeaturesから線形次元圧縮されたものであり、featuresのほうがより高次元として、高い情報を保持しているが、使用時の容易性などより続くチュートリアルにおいては、sentimentのみを用いてモデリングを行う。本章では、sentiment合成特徴量がどのような性質を獲得しているかを確認するため、学習のオブジェクトとなるマーケットのforward returnとの関係性を解析する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8qo8PaAL5_f"
      },
      "source": [
        "### 1.7.13.  合成特徴量のロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLVKKhdZL5_f"
      },
      "source": [
        "# 前章で抽出したlstm_sentimentをロードする。\n",
        "# コラムは、次元順に付与されたidを表し、sentimentの場合は１次元のみのデータとなっているため、0でインデクシングを行って以下、解析ではpd.series形式として扱う。\n",
        "headline_features = pd.read_pickle(f'{CONFIG[\"base_path\"]}/headline_features/LSTM_sentiment.pkl')[0].rename('features')\n",
        "keywords_features = pd.read_pickle(f'{CONFIG[\"base_path\"]}/keywords_features/LSTM_sentiment.pkl')[0].rename('features')\n",
        "\n",
        "display(headline_features.head())\n",
        "display(keywords_features.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WldVU5IyL5_f"
      },
      "source": [
        "### 1.7.14. ラベルをビルド"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0caAwiB0L5_f"
      },
      "source": [
        "# boundary_weekを学習時境界と同様に設定し、weekly_fwd_returnsをビルドする。\n",
        "weekly_group = SentimentGenerator._build_weekly_group(df=stock_price)\n",
        "weekly_returns = stock_price.groupby(weekly_group).apply(_compute_weekly_return)\n",
        "weekly_fwd_returns = weekly_returns.shift(-1).rename('weekly_fwd_returns')\n",
        "\n",
        "# 特徴量の期間と同様の期間のデータのみを使用する。\n",
        "weekly_fwd_returns = weekly_fwd_returns.reindex(headline_features.index).dropna()\n",
        "\n",
        "display(weekly_fwd_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPeltoI-L5_f"
      },
      "source": [
        "### 1.7.15. 相関係数確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfRmbGRkL5_f"
      },
      "source": [
        "# 二つのpd.Seriesをconcatenateする。\n",
        "# indexの違いがあるため、片方だけ存在するデータはドロップする。\n",
        "df = pd.concat([headline_features, weekly_fwd_returns], axis=1, sort=True).dropna()\n",
        "\n",
        "# 二つのコラムのシークエンス間の相関は、以下のように取得できる。\n",
        "display(df.corr()[df.columns[0]][df.columns[-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpN1QrZoL5_f"
      },
      "source": [
        "# corr関数にmethodを指定して、pearson及びspearman相関係数を表示する。\n",
        "def display_corr(df):\n",
        "    display(pd.Series(\n",
        "        {\n",
        "            \"pearson\": df.corr(method='pearson')[df.columns[0]][df.columns[-1]],\n",
        "            \"spearman\": df.corr(method='spearman')[df.columns[0]][df.columns[-1]]\n",
        "        }\n",
        "    ))\n",
        "    \n",
        "display_corr(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvI51uQiL5_f"
      },
      "source": [
        "# 上記の関数をheadline, keywords両方に適用に、ラベルとの相関を表示する。\n",
        "for features, feature_type in [(headline_features, 'headline_features'), (keywords_features, 'keywords_features')]:\n",
        "    display_markdown(f'#### feature_type: {feature_type}', raw=True)\n",
        "    df = pd.concat([features, weekly_fwd_returns], axis=1, sort=True).dropna()\n",
        "    display_corr(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpj1YyrQL5_f"
      },
      "source": [
        "### 1.7.16. 可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0e1oKX6L5_f"
      },
      "source": [
        "# 回帰を行うため、xとyとなるコラムを設定する。\n",
        "x_column = 'features'\n",
        "y_column = 'weekly_fwd_returns'\n",
        "\n",
        "# stats.linregressを用いて、単回帰直線の係数とバイアスを取得する。\n",
        "df = pd.concat([headline_features, weekly_fwd_returns], axis=1, sort=True).dropna()\n",
        "\n",
        "coef, bias, _, _, _ = stats.linregress(x=df[x_column], y=df[y_column])\n",
        "print(f'coef: {coef:.4f}, bias: {bias:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3dFLApjL5_f"
      },
      "source": [
        "def display_regplot(df, x_column='features', y_column='weekly_fwd_returns'):\n",
        "    # stats.linregressを用いて、単回帰直線の係数とバイアスを取得する。\n",
        "    coef, bias, _, _, _ = stats.linregress(x=df[x_column], y=df[y_column])\n",
        "\n",
        "    # seabornのregplotを用いて、単回帰直線及び、scatter sampleを表示する。\n",
        "    _, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
        "    sns.regplot(\n",
        "        x=x_column,\n",
        "        y=y_column,\n",
        "        data=df,\n",
        "        ax=ax,\n",
        "        line_kws={\n",
        "            \"label\": \"y={0:.4f}x+{1:.4f}\".format(coef, bias), # 取得した係数とバイアスを用いて単項式を表示する。\n",
        "        },\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for features, feature_type in [(headline_features, 'headline_features'), (keywords_features, 'keywords_features')]:\n",
        "    display_markdown(f'#### feature_type: {feature_type}', raw=True)\n",
        "    df = pd.concat([features, weekly_fwd_returns], axis=1, sort=True).dropna()\n",
        "    display_regplot(df=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwvd9eQIL5_g"
      },
      "source": [
        "# barplotより、特徴量とラベル間での相互関係を確認する。\n",
        "# 二つのデータを一つの軸上で単純にプロットすると値のノルムや平均、分散の違いから、相互的な動きを確認しにくい。\n",
        "df.plot(kind='bar', figsize=(16, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6QvzI-2L5_g"
      },
      "source": [
        "# ここでは、手元にあるデータを平均と分散を用いてノーマライズしたzscoreを使って相互関係可視化してみよう。\n",
        "def normalize(df):\n",
        "    # zscore normalizeする。\n",
        "    return pd.DataFrame(stats.zscore(df), index=df.index, columns=[f'Z({column})' for column in df.columns])\n",
        "\n",
        "\n",
        "def display_bar_plot(df):\n",
        "    # zscore normalizeしたデータを用いてbarplotする。\n",
        "    normalize(df).plot(kind='bar', figsize=(16, 3))\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "for features, feature_type in [(headline_features, 'headline_features'), (keywords_features, 'keywords_features')]:\n",
        "    display_markdown(f'#### feature_type: {feature_type}', raw=True)\n",
        "    df = pd.concat([features, weekly_fwd_returns], axis=1, sort=True).dropna()\n",
        "    display_bar_plot(df=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVuK88BL5_g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}