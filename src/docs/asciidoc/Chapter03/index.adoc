include::../attribute.adoc[]

== ポートフォリオを構築しよう

=== 「ニュース分析チャレンジ」のチュートリアルの構成

　3章から6章では、J-Quantsで開催するコンペティションのうち、link:https://signate.jp/competitions/443[「ニュース分析チャレンジ」]に係るチュートリアルを提供します。
本コンペティションでは、2章に記載している「ファンダメンタルズ分析チャレンジ」と同様に、データ分析や株式取引には興味はあるが、きっかけがないという方を主な対象として、投資にまつわるデータ・環 境を提供し、株式市場におけるデータ利活用の可能性を試していただくことを期待しています。

大まかに3章から6章にかけてどのようなことが記載されているのかを以下で説明します。

==== 3章の概要

　3章では本コンペティションに参加するための基本知識を学ぶことができます。主に以下のような題材を扱います。

- コンペティション課題の説明
- データセットの説明
- 提出モデルの予測出力の定義
- バックテスト手法
- シンプルなポートフォリオの組成方法
- ポートフォリオの評価方法
- 構築したモデルの提出方法

==== 4章の概要

　4章では本チュートリアルの2章で構築した最高値・最安値モデルを使用したポートフォリオの構築方法を紹介しています。3章では機械学習的な手法を用いていませんが、4章では機械学習的手法で作成したモデルを利用しています。また、本コンペティションで提供されている適時開示情報についても簡単に触れています。3章より内容は複雑になりますが、実際に機械学習をポートフォリオ組成に活かすための知識を学ぶことができます。主に以下のような題材を扱います。

- 2章で作成したモデルの修正
- モデルを用いたポートフォリオの組成方法
- 適時開示情報の有効活用

==== 5章の概要

　5章では本コンペティションの特色であるニュースデータを活用した取引戦略を構築するための手法を紹介しています。近年のディープニューラルネットワークの研究の発展により、自然言語処理の分野においても大きな技術の発展がありました。5章ではその中でも大きな成果の一つであるBERT(Bidirectional Encoder Representations from Transformers) (参考: link:https://qiita.com/omiita/items/72998858efc19a368e50[自然言語処理の王様「BERT」の論文を徹底解説]) を用いた特徴量抽出手法を紹介しています。
　また、ニュースデータの前処理や可視化も丁寧に説明しており、本コンペの特色であるニュースデータの解析に必要な以下のような基礎知識を学ぶことができます。

- テキストデータの基礎知識
- テキストデータの前処理方法
- テキストデータの可視化方法
- BERTモデルによるニュースデータからの特徴量抽出手法

==== 6章の概要

　6章では5章で取得したBERT特徴量を用いた取引戦略を構築するための手法の一つとしてLSTM(Long Short Term Memory)(参考: link:https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca[LSTMネットワークの概要])を用いたスコア生成手法を紹介しています。ただし、本章はディープニューラルネットワークの基礎知識等を前提とした、かなり高度な内容になっており、少し発展的な内容となります。

=== 「ニュース分析チャレンジ」について
　本節では、本コンペティションについての概要やスケジュールを記載しています。

*コンペティションの課題内容*

　本コンペティションでは、現金100万円を原資として、ある週の週初営業日に始値で購入、その週の週末営業日に売却するとした場合に、できる限り利益を得るポートフォリオの予測に取り組んでいただきます。 +
　そして、前述の予測を予め決められた4週間の間に4ラウンド行っていただき、その4ラウンド全ての合計の利益を競っていただきます。

*コンペティションの概要*

[options="header, autowidth"]
|===
|*項目* | *内容*
|コンペティション名| ニュース分析チャレンジ
|主な対象者| 株式市場を対象としたデータ分析の初学者・データサイエンスに知見のある有識者・テキストデータ分析の有識者等
|入力内容（利用データ） | 銘柄情報・株価情報・ファンダメンタル情報等・日経電子版見出しテキストデータ・適時開示データ
|出力内容（予測対象）| 現金100万円を原資とし、ある週の週初営業日に複数銘柄（最低5銘柄以上）を始値で購入、その週の週末営業日に終値で売却するとした場合のポートフォリオ
|参加を通じて得られる知見|
- 株価や企業業績の推移などの時系列データの解析手法 +
- 市場動向の把握手法 +
- リスク分析 +
- テキストデータの解析手法
|===

*スケジュール*

[options="header, autowidth"]
|===
|*日時* | *内容*
|2021年3月19日（金）| コンペティション開始（データダウンロードのみ可能、モデル提出は不可）
|2021年3月26日（金）| モデル提出開始
|2021年5月9日（日）| モデル提出締切
|2021年5月10日（月）〜 6月4日（金） | モデル評価期間（4週間4ラウンド）
|2021年7月頃 | 入賞者の決定
|===

=== 予測対象
　本節では、本コンペティションの予測対象であるポートフォリオやその構成銘柄の条件等について詳しく説明しています。

*予測対象*

　本コンペティションの予測対象は、現金100万円を原資として、ある週の週初営業日に始値で購入、その週の週末営業日に売却するとした場合に高い利益を得るポートフォリオとなります。下図に最終的な課題内容及び評価についてイメージを示しています。 +
　ポートフォリオとは、「資産運用の世界で様々な資産または銘柄の組み合わせのこと」（引用：link:https://www.jpx.co.jp/glossary/ha/417.html[JPXHP用語集]）を指します。

image::news_predict_target.png[news_predict_target]

==== ポートフォリオに組入れできる銘柄の条件

　本コンペティションの予測対象となるポートフォリオを構成する銘柄は、次に挙げる条件を全て満たすものになりますのでご注意ください。

A)	2020年12月末時点で、東京証券取引所に上場していること

B)	普通株式であること（種類株ではないこと）

C)	ETF、ETN、REIT、優先出資証券、インフラファンド、外国株のいずれにも該当しないこと

D)  2020年12月末時点で、時価総額が200億を上回っていること

==== ポートフォリオの条件
　本コンペティションの予測対象となるポートフォリオは、次に挙げる条件を全て満たすものになります。条件を満たさないポートフォリオは失格となります。

A)	原資100万円のうち、少なくとも50万円は株の購入に充てられていること（すなわち、原資100万円全てを投資しなくても良く、50万円は銘柄購入に、残り50万円を現金として保有することは可能）

B)	少なくとも5銘柄以上で構成されるポートフォリオであること

[[anchor-3.3.4]]
==== 提出するモデルの予測出力の定義

　本コンペティションで提出していただくモデルの出力は以下のフォーマット及び条件を満たす必要があります。

===== 出力フォーマット

　以下のcsv形式で出力
[options="header, autowidth"]
|===
|*列*| *名前* | *型* | *例* | *説明*
| 1列目 | date | string | 2021-02-01 | 株を購入する日付（各週の月曜日の日付）
| 2列目 | Local Code | int64 | 8697 | 銘柄コード
| 3列目 | budget | int64 | 50000 | 購入金額
|===

CSVの出力例を以下に示します。

image::csv_sample.png[csv_format_sample]

===== フォーマットの条件

- ヘッダが付与されていること
- 5銘柄以上選択されていること
- dateは予測対象の週の月曜日の日付であること
- 購入金額は1以上であること
- 銘柄を購入優先度順に上から出力すること
- Nanが含まれないようにすること

===== 特記事項及び注意点

　以下では、各条件を踏まえた特記事項及び注意点につきまして、説明します。

* 本コンペティションでは、*各銘柄は1株単位で購入可能* とします。
    - 東京証券取引所をはじめとする全国の証券取引所で実際に売買する際は、売買単位は100株単位に統一されていますが、本コンペティションでは、ポートフォリオの組成を可能な限り幅広く行っていたくことを考慮して、売買単位を1株単位としています。
* ポートフォリオに組入れできる銘柄の条件に合致しない銘柄がモデルの予測に含まれている場合は、評価の際に該当のレコードは評価対象外として無視されます。（エラーとはなりません。）
* モデルの予測においてdate列が週初営業日以外の日付のレコードは、評価対象外として無視されます。
* Budget列が1未満のレコードは評価対象外として無視されます。
* 週初営業日に値段が付かなかった銘柄*のレコードは評価対象外として無視されます。 +
※ なお、週末営業日に値段が付かなかった場合には、その日より前の日で終値が存在する日の終値を用います。
* 購入金額が50万円に到達していなかった場合は、選択された銘柄を入力順（テーブルデータの並び順）に1株ずつ50万円以上になるまで買い足されます。（評価時の具体的な処理は<<anchor-3.3.5,3.3.5. 評価方法>>を参照してください。）
* 購入金額が100万円を超える場合は、100万円以内の範囲で購入可能な株数に調整されます。（評価時の具体的な処理は<<anchor-3.3.5,3.3.5. 評価方法>>を参照してください。）
* *最終的に購入された銘柄数が5銘柄未満の場合若しくは購入した金額が50万円未満の場合は、エラーとなり失格となります。*

[[anchor-3.3.5]]
==== 評価方法

　本コンペティションでは、モデルで予測したポートフォリオで得られる利益の総合計による定量評価方法を採用します。 +
　まず、各週にポートフォリオから得られる利益を以下の算出式を用いて計算します。

====
[plantuml, math-sample1, svg]
----
@startmath
その週の運用実績 = 保有している株式の合計評価額 + 保持現金 - 原資(100万円)
@endmath
----
保有している株式の合計評価額 = 保有している株式のその週の週末営業日の終値 × 保持する株数 +
保持現金 = その週の週初営業日に株式の購入に利用せずに手元に残った現金

====

　その上で、各週の運用実績を合計し、最終スコアとして評価し、**利益の高い順に**順位付けします。(ただし、利益が全く同じであった場合には、提出が早かった方を上位とします。)

===== バックテスト関数（backtest.py）の挙動
　本コンペティションにおける評価では、チュートリアルレポジトリに配置してある独自のバックテストライブラリ `backtest.py` を用いています。 +
　モデルにより予測されたポートフォリオについて、具体的な処理の概要は以下のとおりです。なお、詳細な挙動につきましては、実際のコードをご参照ください。

【前提条件チェック等】 +
1. 予測されたポートフォリオのフォーマットについて以下のチェックを行う(※)。 +
　　- date列（ヘッダー）が存在するか +
　　- Local Code列（ヘッダー）が存在するか +
　　- budget列（ヘッダー）が存在するか +
　　- データにNaNが含まれていないか +
2. 予測されたポートフォリオのLocal Code列について、「ポートフォリオに組入れできる条件」に合致するレコードに絞り込む。 +
3. 予測されたポートフォリオのdate列について、月曜日のみのレコードに絞り込む。 +
4. 予測されたポートフォリオのbudget列について、1円以上のレコードに絞り込む。

【購入処理】 +
5. 2〜4で絞り込んだポートフォリオについて、５銘柄以上選択されているかチェックを行う(※)。 +
6. ポートフォリオのLocal Code列の上から順に、原資金100万円を上限として、budget列で指定された金額で購入処理を行う。（なお、各ラウンドの週初営業日の始値で購入するため、指定された金額と実際の購入金額は異なることが想定され、予算と実際の購入金額の差額は残額となります。） +
7. 予測されたポートフォリオの全てのレコードについて6の処理を行い、実際の購入金額が50万円未満の場合は、ポートフォリオのLocal Code列の上から順に、更に1株ずつ50万円以上となるまで購入処理を繰り返す。 +
8. 購入処理後に残った原資金は現金保有とする。 +
9. 6〜8 までの購入処理を終えたポートフォリオについて、実際に購入した銘柄数が５銘柄以上かつ実際の購入金額が50万円以上かチェックを行う(※)。

【売却処理・評価】 +
10. 購入した各銘柄について、各ラウンドの週末営業日の終値で売却したこととし、週初営業日からの始値からの運用実績（保有している株式の合計評価額 ＋ 保持現金 - 購入原資金）を算出する。 +
　*なお、(※)と記されているチェック項目については、チェックでエラーとなった場合は、評価算出されず失格となりますので、ご注意ください。以下に、Public期間について具体例を用いて処理について例示しております。*

以下に、本コンペティションのPublic期間における評価について具体例を用いて処理について例示しております。

image::public_csv_sample.png[public_csv_sample]

　上表の左側がモデルが予測したポートフォリオの出力例です。上表の右側は説明のために付け加えたものになります。

・前提条件チェック等の1〜4に従い、条件の確認と銘柄等の絞り込みを行います。 +
・購入処理の5で５銘柄以上含まれていることを確認し、6の処理により購入を行います。 +
・ポートフォリオの全てのレコード列を処理した結果、実際の購入金額の総額は474,860円/5銘柄だった（9983の始値が指定してbudgetを超過し購入できていない）ため、7の処理により再度ポートフォリオの上から１株ずつ追加で購入する処理が行われます。具体的には7203を１株購入→50万円check→未達→9984を１株購入→50万円check→未達→9983を1株購入→50万円基準達成（実際の購入金額588,960円/６銘柄、保持現金411,040円） +
・6〜８までの処理を終え、９で実際に購入した銘柄数が５銘柄以上及び実際の購入金額が50万円以上かをチェックし、購入処理は完了します。 +
・最後に購入した銘柄について、そのラウンドの週末営業日の終値で評価し、保持現金と合わせてそのラウンドの運用実績とします。

==== リーダーボード
　一般的に、データ分析コンペティションにおけるリーダーボード（Leaderboard）とは、コンペティション参加者の投稿内容に対する評価（スコア、実行時間等）をランキング形式で並べる表を意味します。本コンペティションで提供するリーダーボードは、パブリックリーダーボード（以下、Public LB）とプライベートリーダーボード（以下、Private LB）の2つで構成されます。以下では、それぞれのリーダーボードの仕様等について説明します。

　まず、本コンペティションのPublic LBは、コンペティション開催日より過去の期間を対象として評価を実施します。具体的には、本コンペティションのPublic LBでは、2021年2月1日（月）に始値で購入し、2021年2月5日（金）の終値で売却するとした際に、できる限り利益を得ることができるポートフォリオを予測します。よってPublic LBでは、予測されたポートフォリオで利益の高い順に順位付けされます。

　なお、Public LBの評価期間における各銘柄の株価は、各Webサイト等で取得可能であることから、本コンペティションのPublic LBではチーティングが容易であるため、本コンペティションのPublic LBは、他の一般的なコンペティションにおけるPublic LBとは異なり、スコアを競うというよりは、モデルが正常に投稿できることを確認するための環境として位置付けとしておりますので、ご留意ください。以下に、Public LBの評価例として図を示します。

image::public_evaluation.png[public_evaluation]

　次に、本コンペティションのPrivate LBについて説明します。本コンペティションのPrivate LBでは、リークを可能な限り防止するため、モデル提出締切日よりも将来のデータを用いて、Private LBを出力します。 +
　Private LBの各ラウンドの具体的な評価期間は以下のとおりです。

- 2021年5月10日（月）の始値で購入し、2021年5月14日（金）の終値で売却とした際の運用実績（ラウンド1）
- 2021年5月17日（月）の始値で購入し、2021年5月21日（金）の終値で売却とした際の運用実績（ラウンド2）
- 2021年5月24日（月）の始値で購入し、2021年5月28日（金）の終値で売却とした際の運用実績（ラウンド3）
- 2021年5月31日（月）の始値で購入し、2021年6月 4日（金）の終値で売却とした際の運用実績（ラウンド4）

**上記期間における4ラウンドの運用実績を合計し、総利益の高い順に順位付けされます。**

以下に、Private LBの評価例として図を示します。

image::private_evaluation.png[private_evaluation]

以上を踏まえ、本コンペティションにおけるPublic LBとPrivate LBの概要を、表にまとめます。

===== 本コンペティションにおけるリーダーボードの仕様

[options="header, autowidth"]
|===
|*項目* | *Public LB* | *Private LB*
|用途| モデルが正常に投稿できることを確認するための環境 | 本コンペティションの最終的なランキングを表示
|予測対象の期間| 2021年2月1日（月）〜2021年2月5日（金） | 2021年5月10日（月）〜2021年6月4日（金）までの4ラウンド
|予測対象の条件| 3.3.1及び3.3.2項に示すとおり　| 同左
|予測内容 | 予測対象期間の週初営業日に購入し、当該期間の週末営業日に売却した際にできる限り利益を得るポートフォリオ | 各週の週初営業日に購入し、その週の週末営業日に売却した際にできる限り利益を得るポートフォリオ
|評価方法 | 3.3.3項に示す評価方法 | 3.3.3項に示す評価方法を、4ラウンド行いその合計
|===

[[anchor-3.4]]
=== データセットの説明

　ここでは、本コンペティションで提供している各データについて説明します。提供されるデータは以下の13種類です。 +
link:https://signate.jp/competitions/443/data[SIGNATEのコンペティションサイト]よりダウンロードしてください。

*データ概要*

[options="header, autowidth"]
|===
|*ファイル名* | *説明*
|stock_list | 各銘柄の情報が記録されたデータ ※1 ※2
|stock_price | 各銘柄の株価情報（始値・高値・安値・終値等）が記録されたデータ ※1
|stock_fin | 各銘柄のファンダメンタル情報（決算数値データや配当データ等）が記録されたデータ ※1
|stock_labels | 各銘柄で決算発表が行われた日の取引所公式終値から、その日の翌営業日以降N営業日間における取引所公式終値の最高値および最安値への変化率を記録したデータ ※1
|stock_fin_price | データが扱いやすいようにstock_price及びstock_finをマージしたデータ ※1
|nikkei_article | 日経電子版見出し・メタデータ *3
|article | 日経電子版見出し・メタデータの分類語 記事種別
|industry | 日経電子版見出し・メタデータの分類語 業界コード
|industry2 | 日経電子版見出し・メタデータの分類語 業界コード(PDコード)
|region | 日経電子版見出し・メタデータの分類語 地域
|theme | 日経電子版見出し・メタデータの分類語 記事内容のテーマコード
|tdnet | 適時開示資料のメタデータ (API経由でPDFを取得可能 ※4)
|disclosureItems | 適時開示資料の公開項目コード
|headline_features.pkl | 本チュートリアルの5章で作成したヘッドラインの特徴量をpkl化したファイル
|keywords_features.pkl | 本チュートリアルの5章で作成したキーワードの特徴量をpkl化したファイル
|headline_features.zip | 本チュートリアルの6章で作成したヘッドラインの特徴量及びsentiment scoreをpkl化したファイル
|keywords_features.zip | 本チュートリアルの6章で作成したキーワードの特徴量及びsentiment scoreをpkl化したファイル
|purchase_date.csv | 評価対象週における 初日の日付を指定するファイル
|===

　提供データについては、一部データを除き2016年1月初から2020年12月末をcsvファイル形式、2021年1月初からのデータについては、本コンペティション専用のAPIにて提供いたします。APIによるデータ取得につきましては、8章をご参照ください。

※1 ファンダメンタルズ分析チャレンジと共通の5種類のデータについては「2.2. データセットの説明」をご参照ください。 +
※2 stock_listについては、ニュース分析チャレンジでは「universe_comp2」という列が追加されているため、以下に説明を記載しています。 +
*3 当該データについては、2020年以降のデータとなります。 +
※4 PDFのファイル取得は、2020年1月以降のものが対象です。ランタイム環境ではPDF/XBRLの提供はございません。

==== 銘柄情報: stock_list

　stock_listは、基本的にはファンダメンタルズ分析チャレンジと共通となりますが、ニュース分析チャレンジの予測対象銘柄を判別するための「universe_comp2」というカラムが追加されています。本コンペティションではポートフォリオに「universe_comp2」が True と設定されている銘柄のみを組み入れる必要があります。 +
　*ポートフォリオに予測対象銘柄以外を組み入れた場合は、その銘柄についての購入指示は無視されて評価対象外となります。*

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|prediction_target  | 予測対象銘柄 | bool | True
|Effective Date  | 銘柄情報の基準日 | int64 | 20201030
|Local Code   | 株式銘柄コード | int64 | 1301
|Name (English)  | 銘柄名 | object | KYOKUYO CO.,LTD.
|Section/Products  | 市場・商品区分 | object | First Section (Domestic)
|33 Sector(Code)  | 銘柄の33業種区分(コード) | int64 | 50
|33 Sector(name)  | 銘柄の33業種区分(名前) | object | Fishery, Agriculture and Forestry
|17 Sector(Code)   | 銘柄の17業種区分(コード) | int64 | 1
|17 Sector(name)  | 銘柄の17業種区分(名前) | object | FOODS
|Size Code (New Index Series)  | TOPIXニューインデックスシリーズ規模区分(コード) | object | 7
|Size (New Index Series) | TOPIXニューインデックスシリーズ規模区分 | object | TOPIX Small 2
|IssuedShareEquityQuote AccountingStandard  | 会計基準 単独:NonConsolidated、連結国内:ConsolidatedJP、連結SEC:ConsolidatedUS、連結IFRS:ConsolidatedIFRS | object | ConsolidatedJP
|IssuedShareEquityQuote ModifyDate |更新日 | object | 2020/11/06
|IssuedShareEquityQuote IssuedShare  |発行済株式数 | float64 | 10928283.0
|universe_comp2 | *ニュース分析チャレンジの予測対象銘柄* | bool | True
|===

(JPX東証上場銘柄一覧より引用 https://www.jpx.co.jp/markets/statistics-equities/misc/01.html) +
(Quick xignite API Market Data API Catalogより引用 https://www.marketdata-cloud.quick-co.jp/Products/)

==== 日経電子版見出し・メタデータ: nikkei_article

　nikkei_articleでは、日経電子版の見出しおよびメタデータを提供しています。記事見出しやキーワードなどの言語データに加え、一部のレコードには該当の記事に関連する株式コードや、記事の分類情報が含まれています。分類情報については別途CSVファイルでも提供しておりますのでご参照ください。


[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|article_id | 記事ID(DBユニークキー) | object | TDSKDBDGXMZO5518670003022020QM8000
|publish_datetime | "掲載日(datetime型)""2016-09-25T23:33:26Z"" など ※新聞各紙の時分は00:00:00固定"| object | 2020-02-03T17:58:25+09:00
|media_code| "媒体略号 媒体のユニークコード ・日本経済新聞朝刊:NKM、NK2、NK3、NK4、NK5、NK6 ・日本経済新聞夕刊:NKE ・日本経済新聞地方経済面:NKL ・日経産業新聞:NSS、SS2・日経MJ:NRS、RS2 ・日経速報ニュースアーカイブ:NKR" | object | NKE
|media_name | "媒体名称 ""日本経済新聞 朝刊""、""日経速報ニュースアーカイブ""など" | object | 日本経済新聞電子版
|men_name | "面名 地方経済面の場合に収録。""名古屋朝刊社会面""、""埼玉”など" | object | 名古屋朝刊社会面
|page_from | "検索掲載開始ページ掲載ページ。但し、地方経済面のページ情報は、掲載されたページではなく、どの地方の記事であるかを意味する。※対応表参照" | object |
|picture_flag | "絵・写真・表の有無”” or ”有”" | object | 有
|paragraph_cnt | "記事本文段落数" | int64 | 2
|char_length | "記事本文文字数" | int64 | 221
|headline | "記事見出し" | object | 東商取の売買高、１月は10％増、４カ月ぶり前年越え
|keywords | "キーワード 記事の文中から主題語として切り出したワード(またはその正式名称)" | object | 東京商品取引所\n売買高\n前年\n日本取引所グループ\n同月
|classifications | "分類情報 記事内容のテーマコード(#W〜)・業界コード(#B〜)、証券コード等の会社コード(T〜、N〜、PD〜)、紙 面名等の記事分類キーワード($〜)、コラム名(「〜」)" | object | ＄絵写表記事\nＴ８６９７\nＰＤ５２１\nＮ００４０４３１\nＮ００７５１０７
|company_g.stock_code | 株式コード | object | 8697
|===

===== 日経電子版見出し・メタデータの確認

ここでは「日経電子版見出し・メタデータ」を把握するために、データを確認していきます。データの確認に使用したnotebookは「handson/Chapter03/20210415_chapter03_news_data_visualization.ipynb」に配置しています。必要に応じてご参照ください。

データを読み込みます。

[source,python]
----
# 入力パラメーターを設定します。ランタイム環境での実行時と同一フォーマットにします
inputs = {
    "stock_list": f"{dataset_dir}/stock_list.csv.gz",
    "nikkei_article": f"{dataset_dir}/nikkei_article.csv.gz",
}
# 銘柄リストを取得
df_stock_list = pd.read_csv(inputs["stock_list"])
# 投資対象銘柄を取得
stock_codes = df_stock_list.loc[df_stock_list.loc[:, "universe_comp2"] == True, "Local Code"]
# 日経電子版見出し・メタデータ読み込み
df_nikkei_article = pd.read_csv(inputs["nikkei_article"])
# インデックスを記事の掲載日に設定
df_nikkei_article.set_index("publish_datetime", inplace=True)
# インデックスを日付型に変換
df_nikkei_article.index = pd.to_datetime(df_nikkei_article.index)
# インデックスで安定ソート
df_nikkei_article.sort_index(kind="mergesort", inplace=True)
----

データの件数を確認します。

[source,python]
----
# 件数を確認
df_nikkei_article.info()
----

出力
[source]
----
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 178393 entries, 2020-01-01 00:00:00+09:00 to 2020-12-31 23:26:35+09:00
Data columns (total 12 columns):
 #   Column                Non-Null Count   Dtype
---  ------                --------------   -----
 0   article_id            178393 non-null  object
 1   media_code            178393 non-null  object
 2   media_name            178393 non-null  object
 3   men_name              0 non-null       float64
 4   page_from             0 non-null       float64
 5   picture_flag          75294 non-null   object
 6   paragraph_cnt         178393 non-null  int64
 7   char_length           178393 non-null  int64
 8   headline              178393 non-null  object
 9   keywords              177894 non-null  object
 10  classifications       173346 non-null  object
 11  company_g.stock_code  68369 non-null   object
dtypes: float64(2), int64(2), object(8)
memory usage: 17.7+ MB
----

上記からデータの件数とデータの範囲がわかったので、より具体的な値を把握するために先頭の1レコードを出力します。

[source,python]
----
# データの開始日と内容を確認
df_nikkei_article.head(1).T
----

image::nikkei_article/nikkei_article01.png[処理結果, width="70%"]

データの最終行を表示します。

[source,python]
----
# データの最終日を確認
df_nikkei_article.tail(1).T
----

image::nikkei_article/nikkei_article02.png[処理結果, width="70%"]

記事の見出しである headline 列の文字数の分布をプロットします。

[source,python]
----
# 見出しの文字数の分布をプロット
df_nikkei_article.headline.str.len().hist(figsize=(10,10), bins=50)
----

image::nikkei_article/nikkei_article03.png[処理結果, width="70%"]

記事の見出しは25文字がピークであることを確認できます。


company_g.stock_code 列にはニュースに関連する銘柄コードが含まれています。銘柄コードが含まれているデータを確認します。

[source,python]
----
# 銘柄コードが含まれているデータを確認
df_nikkei_article.loc[~df_nikkei_article.loc[:, "company_g.stock_code"].isnull()].head().T
----

image::nikkei_article/nikkei_article04.png[処理結果, width="70%"]

一つのニュース記事について「\n」を区切り文字として複数の銘柄コードが格納されていることがわかります。

1週間毎のニュースの記事件数と銘柄コードが含まれている記事の件数をプロットするためのデータを作成します。

[source,python]
----
# 月曜日を開始日として、週次の記事件数と銘柄コードを含む記事件数を取得
df_weekly_count = df_nikkei_article.resample("W-MON", label="left", closed="left")[["article_id", "company_g.stock_code"]].count()
# 集計内容を確認
df_weekly_count.head(3)
----

image::nikkei_article/nikkei_article05.png[処理結果, width="70%"]

1週間毎の記事件数の統計量を確認します。

[source,python]
----
# 週次件数の統計量を確認
df_weekly_count.describe()
----

image::nikkei_article/nikkei_article06.png[処理結果, width="70%"]

1週間毎の記事件数の平均は3,300件超、銘柄コードが含まれている記事件数の平均は1,200件超であることがわかります。

次に、1週間毎の記事件数の分布を確認します。

[source,python]
----
#  週次件数の分布を確認
df_weekly_count.hist(figsize=(20, 10), alpha=0.5, bins=25)
----

image::nikkei_article/nikkei_article07.png[処理結果, width="70%"]

1週間毎の記事件数の推移をプロットします。

[source,python]
----
# プロット
ax = df_weekly_count.plot(figsize=(20, 8))
# グリッド設定
ax.grid(True)
----

image::nikkei_article/nikkei_article08.png[処理結果, width="70%"]

データの始点・終点では月曜日を起点としているため曜日の数が十分でないこと、5,7,8,9月は祝日が影響して記事件数が少なくなっているようです。


ここからは記事に紐付けられている銘柄コードについて確認します。
記事データから抽出した銘柄コード毎に記事件数をプロットします。ここでは対象とする銘柄コードを投資対象銘柄に限定して確認します。

[source,python]
----
# 記事に記載されている銘柄コードを取得
s_stocks = df_nikkei_article.loc[~df_nikkei_article.loc[:, "company_g.stock_code"].isnull(), "company_g.stock_code"].str.split("\n")
# 銘柄コード別の件数を取得し、記事件数の多い順にソート
s_stock_counts = pd.Series(Counter(chain.from_iterable(s_stocks))).sort_values(ascending=False)
# 投資対象銘柄に絞り込み
s_stock_counts = s_stock_counts.loc[s_stock_counts.index.astype(int).isin(stock_codes)]
# データフレームに変換して、インデックスをリセット
df_stock_counts = s_stock_counts.to_frame().reset_index(drop=True)
# カラム名を設定
df_stock_counts.rename(columns={0: "article_count"}, inplace=True)
# 投資対象銘柄全体におけるdisclosureItems別の件数の割合を算出
df_stock_counts.loc[:, "percentage"] = (df_stock_counts["article_count"] / df_stock_counts["article_count"].sum()) * 100
# 件数割合の累積を計算
df_stock_counts.loc[:, "cumulative_percentage"] = df_stock_counts["percentage"].cumsum()
# 投資対象銘柄コード別の件数をプロット (全銘柄)
ax = df_stock_counts[["article_count"]].plot(figsize=(20, 8))
#  グリッド設定
ax.grid(True)
# 凡例を左上に表示
ax.legend(loc="upper left")
# 2つ目のy軸を作成
ax2 = ax.twinx()
# 開示件数割合の累積をプロット
df_stock_counts[["cumulative_percentage"]].reset_index(drop=True).plot(ax=ax2, color="orange")
# 凡例を右上に表示
ax2.legend(loc="upper right")
----

image::nikkei_article/nikkei_article09.png[処理結果, width="70%"]

x軸は銘柄の数を示しています。青色線は左側のy軸に対応しており記事件数を表しています、黄色線は右側のy軸に対応しており記事件数全体における累積割合を示しています。一部の銘柄に記事件数が集中しており、約1750銘柄中250銘柄で記事件数全体の70%程度を占めていることがわかります。

記事件数の多い順に上位50銘柄をプロットします。

[source,python]
----
# 投資対象銘柄コード別の記事件数 (件数上位50銘柄)
ax = s_stock_counts.head(50).plot.bar(figsize=(20, 8))
#  グリッド設定
ax.grid(True)
----

image::nikkei_article/nikkei_article10.png[処理結果, width="70%"]

`7203` のトヨタ自動車、および `9984` のソフトバンクグループに関する記事件数が突出しており、以後記事件数が減少していることがわかります。ニュースの件数は銘柄ごとにかなり偏っていることがわかります。


==== 日経電子版見出し・メタデータの分類語 記事種別: article

　articleでは、日経電子版見出し・メタデータの分類情報(classifications)に記載の記事種類の項目一覧を提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|code | 分類情報コード | object | #K1
|article | 記事種類項目 | object | 人事記事
|===

==== 日経電子版見出し・メタデータの分類語 業界コード: industry

　industryでは、日経電子版見出し・メタデータの分類情報に記載の業界一覧を提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|code | 分類情報コード | object | ＃Ｂ００１０
|industry1 | 業界項目一覧の大項目 | object | 資源・エネルギー
|industry2 | 業界項目一覧の小項目 | object | 石油・鉱業・エネルギー
|===

==== 日経電子版見出し・メタデータの分類語 業界コード(PDコード): industry2

　industry2では、日経電子版見出し・メタデータの分類情報の業界(PDコード)一覧を提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|pdcode | 分類情報 | object | ＰＤ０１１
|Industry | 業界項目一覧 | object | 飼料
|===

==== 日経電子版見出し・メタデータの分類語 地域: region

　regionでは、日経電子版見出し・メタデータの分類情報の地域一覧を提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|code | 分類情報 | object | ＃Ａ７００
|category1 | 地域一覧の大項目 | object | 外国
|category2 | 地域一覧の小項目 | object | インド
|===

==== 日経電子版見出し・メタデータの分類語 記事内容のテーマコード: theme

　themeでは、日経電子版見出し・メタデータの分類情報のテーマ一覧を提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|code | 分類情報 | object | ＃Ｗ１０１０１
|category1 | テーマ一覧の大項目 | object | 企業
|category2 | テーマ一覧の中項目 | object | 事業組み替え
|category3 | テーマ一覧の小項目 | object | 事業組み替え
|===

==== 適時開示資料のメタデータ: tdnet

　tdnetでは、適時開示資料のメタデータを提供しています。適時開示資料については「2.1.5. 決算短信・財務諸表」をご参照ください。「disclosureItems (公開項目コード)」の一覧は別途csvファイルで提供しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|datetime | 開示日付および開示番号 | object | 2016-02-19:16:00:00#20160210412154
|disclosedDate | 開示日付 | object | 2016-02-19
|disclosedTime | 開示時刻 | object | 16:00:00
|disclosureNumber | 開示番号 | int64 | 20160210412154
|code | 銘柄コード |  int64 | 79860
|name | 銘柄略称 | object | Ｊ－日本アイエスケイ
|disclosureItems | 公開項目コード | object | ["11301"]
|title | 表題 | object | 平成27年12月期　決算短信〔日本基準〕（連結）
|handlingType | 取扱属性 | object | null
|modifiedHistory | 開示履歴番号 | int64 | 1
|pdfGeneralFlag | 全文情報PDFファイル存在フラグ | int64 | 1
|pdfSumaryFlag | サマリ情報PDFファイル存在フラグ | int64 | 1
|xbrlFlag | XBRL関連ファイル存在フラグ | int64 | 1
|===

===== 適時開示資料のメタデータの確認

ここでは「適時開示資料のメタデータ」を把握するために、データを確認していきます。データの確認に使用したnotebookは「handson/Chapter03/20210415_chapter03_news_data_visualization.ipynb」に配置しています。必要に応じてご参照ください。

データを読み込みます。

[source,python]
----
# 入力パラメーターを設定します。ランタイム環境での実行時と同一フォーマットにします
inputs = {
    "stock_list": f"{dataset_dir}/stock_list.csv.gz",
    "nikkei_article": f"{dataset_dir}/nikkei_article.csv.gz",
    "disclosureItems": f"{dataset_dir}/disclosureItems.csv.gz",
}
# 銘柄リストを取得
df_stock_list = pd.read_csv(inputs["stock_list"])
# 投資対象銘柄を取得
stock_codes = df_stock_list.loc[df_stock_list.loc[:, "universe_comp2"] == True, "Local Code"]
# 適時開示資料のメタデータ読み込み
df_tdnet = pd.read_csv(inputs["tdnet"])
# インデックスを開示日時に設定
df_tdnet.index = pd.to_datetime(df_tdnet["disclosedDate"].astype(str) + " " + df_tdnet["disclosedTime"])
# インデックスで安定ソート
df_tdnet.sort_index(kind="mergesort", inplace=True)
# 公開項目コード一覧を読み込み
df_disclosureitems = pd.read_csv(inputs["disclosureItems"])
----

データの件数を確認します。

[source,python]
----
# 件数を確認
df_tdnet.info()
----

出力
[source]
----
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 510962 entries, 2016-01-12 08:00:00 to 2020-12-30 21:00:00
Data columns (total 13 columns):
 #   Column            Non-Null Count   Dtype
---  ------            --------------   -----
 0   datetime          510962 non-null  object
 1   disclosedDate     510962 non-null  object
 2   disclosedTime     510962 non-null  object
 3   disclosureNumber  510962 non-null  int64
 4   code              510962 non-null  int64
 5   name              510962 non-null  object
 6   disclosureItems   510962 non-null  object
 7   title             510962 non-null  object
 8   handlingType      1694 non-null    object
 9   pdfGeneralFlag    510962 non-null  int64
 10  modifiedHistory   510962 non-null  int64
 11  pdfSumaryFlag     510962 non-null  int64
 12  xbrlFlag          510962 non-null  int64
dtypes: int64(6), object(7)
memory usage: 54.6+ MB
----

上記からデータの件数とデータの範囲がわかったので、より具体的な値を把握するために先頭の1レコードを出力します。

[source,python]
----
# データの開始日と内容を確認
df_tdnet.head(1).T
----

image::tdnet/tdnet01.png[処理結果, width="70%"]

データの最終行を表示します。

[source,python]
----
# データの最終日を確認
df_tdnet.tail(1).T
----

image::tdnet/tdnet02.png[処理結果, width="70%"]

データを加工して解析しやすくします。

[source,python]
----
# 銘柄コードを4桁に変更
df_tdnet.loc[:, "stock_code"] = df_tdnet.loc[:, "code"].astype(str).str[:4].astype(int)
# 投資対象銘柄に絞り込み
filter_universe = df_tdnet.loc[:, "stock_code"].isin(stock_codes)
# 月曜日を開始日として、投資対象銘柄の週次の件数を取得
df_tdnet_weekly_count = df_tdnet.loc[filter_universe].resample("W-MON", label="left", closed="left")[["disclosureNumber"]].count()
# 集計内容を確認
df_tdnet_weekly_count.head(3)
----

image::tdnet/tdnet03.png[処理結果, width="30%"]


週毎の開示件数をプロットします。

[source,python]
----
# プロット
ax = df_tdnet_weekly_count.plot(figsize=(20, 8))
# グリッド設定
ax.grid(True)
----

image::tdnet/tdnet04.png[処理結果, width="70%"]

開示件数には波があることがわかります。

週毎の開示件数の分布をプロットします。

[source,python]
----
#  資対象銘柄の週次件数の分布を確認
df_tdnet_weekly_count.hist(figsize=(10, 10))
----

image::tdnet/tdnet05.png[処理結果, width="50%"]

開示情報の公開項目コードについて確認していきます。

まずは、公開項目コードをリストとして取得します。

[source,python]
----
# 投資対象銘柄のdisclosureItemsをリスト形式のシリーズとして取得
s_disclosureItems = df_tdnet.loc[filter_universe, "disclosureItems"].apply(literal_eval)
# データを確認
s_disclosureItems[:10]
----

image::tdnet/tdnet06.png[処理結果, width="70%"]

次に、公開項目毎の件数を集計したあとに、公開項目コードと日本語名を結合します。

[source,python]
----
# 投資対象銘柄のdisclosureItems別の件数を取得
s_disclosureitems_count = pd.Series(Counter(chain.from_iterable(s_disclosureItems)))
# カラム名を設定
s_disclosureitems_count.name = "disclosureItems_count"
# 公開項目コードをint型に変更
s_disclosureitems_count.index = s_disclosureitems_count.index.astype(int)
# 項目の日本語名を表示するために適時開示資料の公開項目コードと結合
df_disclosureitems_with_label = pd.merge(s_disclosureitems_count, df_disclosureitems, left_index=True, right_on=["公開項目コード"])
# indexを設定
df_disclosureitems_with_label.set_index("コード値定義", inplace=True)
# 項目の日本語名を表示するために適時開示資料の公開項目コードと結合
df_disclosureitems_with_label = pd.merge(s_disclosureitems_count, df_disclosureitems, left_index=True, right_on=["公開項目コード"])
# indexを設定
df_disclosureitems_with_label.set_index("コード値定義", inplace=True)
----

公開項目コード毎の開示情報件数をプロットします。

[source,python]
----
# 投資対象銘柄のdisclosureItems別の件数を多い順に並び替え
df_count_by_disclosureitems_with_label = df_disclosureitems_with_label.loc[:, ["disclosureItems_count"]].sort_values("disclosureItems_count", ascending=False).reset_index(drop=True)
# 投資対象銘柄全体におけるdisclosureItems別の件数の割合を算出
df_count_by_disclosureitems_with_label.loc[:, "percentage"] = (df_count_by_disclosureitems_with_label["disclosureItems_count"] / df_count_by_disclosureitems_with_label["disclosureItems_count"].sum()) * 100
# 件数割合の累積を計算
df_count_by_disclosureitems_with_label.loc[:, "cumulative_percentage"] = df_count_by_disclosureitems_with_label["percentage"].cumsum()
# プロット (全て)
ax = df_count_by_disclosureitems_with_label[["disclosureItems_count"]].plot(figsize=(20, 8))
#  グリッド設定
ax.grid(True)
# 凡例を左上に表示
ax.legend(loc="upper left")
# 2つ目のy軸を作成
ax2 = ax.twinx()
# 開示件数割合の累積をプロット
df_count_by_disclosureitems_with_label[["cumulative_percentage"]].reset_index(drop=True).plot(ax=ax2, color="orange")
# 凡例を右上に表示
ax2.legend(loc="upper right")
----

image::tdnet/tdnet07.png[処理結果, width="70%"]

件数の多い上位50項目を公開項目名をX軸にしてプロットします。

[source,python]
----
# 投資対象銘柄のdisclosureItems別に件数を多い順にプロット (上位50項目)
ax = df_disclosureitems_with_label.loc[:, ["disclosureItems_count"]].sort_values("disclosureItems_count", ascending=False).head(50).plot(kind="bar", figsize=(20, 8))
#  グリッド設定
ax.grid(True)
----

image::tdnet/tdnet08.png[処理結果, width="100%"]

銘柄毎に開示情報の件数をプロットします。

[source,python]
----
# 投資対象銘柄別に開示件数を集計して、開示件数の多い順に並び替え
df_tdnet_count_by_stock_code = df_tdnet.loc[filter_universe].groupby("stock_code")[["disclosureNumber"]].count().sort_values("disclosureNumber", ascending=False)
# 投資対象銘柄全体における開示件数の割合を集計
df_tdnet_count_by_stock_code.loc[:, "percentage"] = (df_tdnet_count_by_stock_code["disclosureNumber"] / df_tdnet_count_by_stock_code["disclosureNumber"].sum()) * 100
# 開示件数割合の累積を計算
df_tdnet_count_by_stock_code.loc[:, "cumulative_percentage"] = df_tdnet_count_by_stock_code["percentage"].cumsum()
# 投資対象銘柄別に開示件数を多い順にプロット
ax = df_tdnet_count_by_stock_code[["disclosureNumber"]].reset_index(drop=True).plot(figsize=(20, 8))
# グリッド設定
ax.grid(True)
# 凡例を左上に表示
ax.legend(loc="upper left")
# 2つ目のy軸を作成
ax2 = ax.twinx()
# 開示件数割合の累積をプロット
df_tdnet_count_by_stock_code[["cumulative_percentage"]].reset_index(drop=True).plot(ax=ax2, color="orange")
# 凡例を右上に表示
ax2.legend(loc="upper right")
----

image::tdnet/tdnet09.png[処理結果, width="70%"]

開示件数の多い上位50銘柄をプロットします。

[source,python]
----
# 投資対象銘柄別に開示件数を多い順にプロット (上位50銘柄)
ax = df_tdnet.loc[filter_universe].groupby("stock_code")[["disclosureNumber"]].count().sort_values("disclosureNumber", ascending=False).head(50).plot(kind="bar", figsize=(20, 8))
# 　グリッド設定
ax.grid(True)
----

image::tdnet/tdnet10.png[処理結果, width="70%"]


==== 適時開示資料の公開項目コード: disclosureItems

　disclosureItemsは、適時開示資料の公開項目が含まれています。例えば、一般に「自己株式の取得」はポジティブなイベント、「災害に起因する損害又は業務遂行の過程で生じた損害」はネガティブなイベントとして認識されます。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|分類 | 分類番号 | int64 | 11
|公開項目番号 | 公開項目番号 | int64 | 102
|公開項目コード | 公開項目コード | int64 | 11102
|コード値定義 | 内容説明 | object | 発行登録及び需要状況調査の開始
|===

==== ニュース記事ヘッドライン特徴量: headline_features.pkl

　headline_features.pklは、本チュートリアルの5章で作成したヘッドラインの特徴量をpkl化したファイルです。詳細は本チュートリアルの5.8.4をご参照ください。

==== ニュース記事キーワード特徴量: keywords_features.pkl

　keywords_features.pklは、本チュートリアルの5章で作成したキーワードの特徴量をpkl化したファイルです。詳細は本チュートリアルの5.8.4をご参照ください。

==== ニュース記事ヘッドライン特徴量（LSTM）: headline_features.zip

　headline_features.zipは、本チュートリアルの6章で作成したヘッドラインの特徴量及びsentiment scoreをpkl化したファイルです。詳細は本チュートリアルの6.2.9. 特徴量合成モデルの学習及び特徴量合成をご参照ください。

==== ニュース記事キーワード特徴量（LSTM）: keywords_features.zip

　keywords_features.zipは、本チュートリアルの6章で作成したキーワードの特徴量及びsentiment scoreをpkl化したファイルです。詳細は本チュートリアルの6.2.9. 特徴量合成モデルの学習及び特徴量合成をご参照ください。

==== 日付指定ファイル: purchase_date.csv
　評価対象週について、初日の日付を指定するファイルです。ランタイム環境ではこのファイルから日付を取得してポートフォリオを作成します。その際に株価データなどは指定された日付のデータを含んでいない点に注意が必要です。例えば、purchase_dateに `2021/02/01` が指定されている場合、stock_priceデータの最も日付の新しいデータは `2021/01/29` になります。そのため、本チュートリアルでは主に金曜日の日付を使用して予測を出力した後、予測からポートフォリオを組成する際に対象日付が翌週の月曜日となるように調整しています。

[cols="2,3,1,1", options="header"]
|===
|*変数名* | *説明* | *型* | *例*
|purchase date | 評価対象週の初日の日付 | object | 2021/02/01
|===

=== 環境構築

==== 実行環境の選択

　環境構築方法については https://signate.jp/features/runtime/detail[SIGNATE: Runtime 投稿方法: ローカル開発環境の構築方法は？] にも説明がありますが、本コンペの特色を考慮し、実行環境の選択方法を説明します。

　3章・4章のチュートリアルではDockerとGoogle Colaboratoryの両方を利用可能です。特にWindows環境をご利用でDocker環境の構築が難しい場合は、ぜひGoogle Colaboratoryをご利用ください。本チュートリアルで提供されるnotebookはGoogle Colaboratoryでも動作可能となっております。*なお、Google Colaboratoryをご利用になる場合には、以下のように各ライブラリのバージョンを指定する必要がございます。*

----
joblib==1.0.1
numpy==1.19.5
pandas==1.1.5
scikit-learn==0.20.3
scipy==1.2.1
seaborn==0.9.0
----

　5章・6章で提供しているnotebookを最後まで実行する場合、ディープニューラルネットワークの学習・推論を行うため、*CPU環境ではかなり時間がかかります。そのため、GPU環境での実行をおすすめしております。* なお、Google ColaboratoryではGPU環境がご利用いただけますので、GPU環境をお持ちでない場合はGoogle Colaboratoryをご利用ください。

[[anchor-3.5.2]]
==== Google Colaboratoryをご利用の場合

　本コンペティションの3章、4章のチュートリアルをGoogle Colaboratory上で動かすためには、まず以下の手順でGoogle Drive上にファイルを設置します。

1. Google DriveのMy Driveに``JPX_competition``というフォルダーを作成します。

2. 1で作成した``JPX_competition``フォルダーにデータを保存するための``data_dir_comp2``フォルダーを作成します。

3. 1で作成した``JPX_competition``フォルダーにバックテスト用コードを保存するための``backtest``フォルダーを作成します。

4. link:https://signate.jp/competitions/443/data[SIGNATEのコンペティションサイト] よりダウンロードした各種データを2で作成した``data_dir_comp2``フォルダーにアップロードします。

5. `backtest.py` を link:https://github.com/JapanExchangeGroup/J-Quants-Tutorial/tree/main/handson/Chapter03/backtest/backtest.py[こちら] からダウンロードし、3で作成した``backtest``フォルダにアップロードします。

　次にGoogle Colaboratory上でチュートリアルのnotebookを展開します。本チュートリアルのnotebookはGoogle Colaboratory上でも実行可能となっております。各章のnotebookは以下のそれぞれのリンク先を開き、開いたページでRawを右クリックし、「リンク先を名前をつけて保存」を選択することでダウンロード可能です。

link:https://github.com/JapanExchangeGroup/J-Quants-Tutorial/blob/main/handson/Chapter03/20210224_chapter03_tutorial.ipynb[3章のnotebook] +
link:https://github.com/JapanExchangeGroup/J-Quants-Tutorial/blob/main/handson/Chapter04/20210224_chapter04_tutorial.ipynb[4章のnotebook]

以下、3章を例にGoogle Colaboratoryでチュートリアルのnoteboookを使用する方法を説明します。

1. Google Drive の My Drive 内に作成した``JPX_competition``フォルダーに3章用のnotebookを保存するための``Chapter03``フォルダーを作成します。

2. 上記のリンク先から3章のnotebookをダウンロードして、先程作成した``Chapter03``フォルダーに``20210224_chapter03_tutorial.ipynb``というファイル名で保存してください。

3. Google Driveにアップロードした `20210224_chapter03_tutorial.ipynb` ファイルをダブルクリックして Google Colaboratory で開きます。

4. Google Colaboratoryの環境で本チュートリアルを実行する場合、最初に以下のコードを実行して Google Colaboratory 上の notebook から Google Drive にアクセスできるようにしてください。

[source,python]
----
# Google Colab環境ではGoogle Driveをマウントしてアクセスできるようにします。
import sys

if 'google.colab' in sys.modules:
    # Google Drive をマウントします
    from google.colab import drive
    mount_dir = "/content/drive"
    drive.mount(mount_dir)
----

　ここまでの作業を実施した結果、Google Driveの``JPX_competition``フォルダーは以下になります。

[soruce]
----
/content/drive/MyDrive/JPX_competition/
├── Chapter03
│   ├── 20210224_chapter03_tutorial.ipynb
│   └── backtest
│       └── backtest.py
└── data_dir_comp2
    ├── article.csv.gz
    ├── disclosureItems.csv.gz
    ├── industry2.csv.gz
    ├── industry.csv.gz
    ├── nikkei_article.csv.gz
    ├── region.csv.gz
    ├── stock_fin.csv.gz
    ├── stock_fin_price.csv.gz
    ├── stock_labels.csv.gz
    ├── stock_list.csv.gz
    ├── stock_price.csv.gz
    ├── tdnet.csv.gz
    └── theme.csv.gz
----

==== Dockerをご利用の場合

　本チュートリアルのリポジトリを `git clone` した後、以下の手順を実行していただくことで、Dockerコンテナ内でjupyter notebookを動かすことができます。リポジトリ内の `Chapter03` ディレクトリには、本チュートリアルのコードを記載した ipynb(`20210224_chapter03_tutorial.ipynb`) ファイルを配置しておりますので必要に応じてご活用ください。

　Windows環境の場合、コマンド実行には「PowerShell」などをご使用ください。なお、PowerShellの利用に当たっては、最新のセキュリティ事情を踏まえご自身でご判断ください。Dockerのインストールについてはlink:http://docs.docker.jp/get-docker.html[こちら]をご参照ください。Dockerの制約としてマウントするパスにはアルファベット、数字、「_」、「.」、「-」以外の文字を使用するとエラーとなることがあるため、パスが前述の文字のみで構成されているディレクトリをご使用ください。 +
　なお、Windows 10 Homeをご利用の場合、Dockerの利用に制限がある場合がありますので、Dockerに習熟した方以外はGoogle Colaboratoryの利用を推奨します。

[source,bash]
----
cd handson/

# 実行するコンテナはsignateユーザーで実行されるため、マウントした領域に書き込めるようにパーミッションを変更します。
chmod -R 777 .

# データ配置先のディレクトリを作成
mkdir data_dir_comp2
# その後作成したhandson/data_dir_comp2に、コンペティションサイトよりデータをダウンロードし配置します。

# Dockerでjupyter notebookを起動します。(初回実行時は約10GBのコンテナイメージをダウンロードします。)
# jupyter notebook作業用に handson ディレクトリを /notebook としてマウントしています。
# jupyter notebook は port 8888でtokenとpasswordを空にして、vscode のjupyter pluginからアクセスできるように xsrf 対策を無効化しています。
docker run --name tutorial --shm-size=2G -v ${PWD}:/notebook -p8888:8888 --rm -it signate/runtime-jpx:2021.03 jupyter notebook --ip 0.0.0.0 --allow-root --no-browser --no-mathjax --NotebookApp.disable_check_xsrf=True  --NotebookApp.token='' --NotebookApp.password='' /notebook

# ブラウザで以下のURLにアクセスしてjupyter notebookの画面が表示されていて、本チュートリアル用のnotebookが表示されていることを確認します。
http://localhost:8888/
----

[[anchor-3.6]]
=== バックテスト環境の構築

　本コンペティション用のバックテスト環境を構築します。金融の世界でバックテストとは一般に価格時系列データを使用して取引をシミュレーションし取引ルールやアルゴリズムなどを評価することを言います。本コンペティションの課題は、モデルにより予測されたポートフォリオのパフォーマンスを競う課題であるため、実際にポートフォリオとして運用した際の期待収益性等について確認・評価することが重要です。また、本コンペティションの評価においても使用しております。実際の評価の具体的な流れは、<<anchor-3.3.5, 3.3.5. 評価方法>>をご参照ください。

　バックテストを実施するために最初に必要なことは要件を適切に把握することです。例えば、投資対象となる銘柄群 (ユニバースとも言います)、取引の時間間隔 (数分、数時間、数日)、投資予算の上限、同時に保有可能な銘柄数、1銘柄に投資可能な上限等です。これらを把握しバックテストを実装していくことになります。バックテストの要件を把握することは、モデルを作る際にもモデルの出力要件や評価方法を把握することにもつながります。

　評価を公正にするために本コンペティションの評価に用いるものと全く同じロジックが実装されたバックテスト用のコード `backtest.py` を公開します。このファイルは、link:https://github.com/JapanExchangeGroup/J-Quants-Tutorial/tree/main/handson/Chapter03/backtest/backtest.py[こちら]からダウンロードしてください。

　なお、本チュートリアルではバックテストをスクラッチで実装していますが、アルゴリズムトレーディング用のバックテストライブラリも存在します。有名なライブラリをtips集の「バックテスト用ライブラリ」でご紹介していますのでご参照ください。

==== 必要なデータ

　バックテストの実行には以下のデータが必要になります。

- ポートフォリオ: バックテスト対象のポートフォリオデータです。（モデルにより予測されたポートフォリオ）
- stock_price: 株価情報を使用してポートフォリオのリターンを算出します。（stock_price.csv.gz）
- stock_list: 銘柄リストを使用して予測対象銘柄の情報を取得します。（stock_list.csv.gz）

　ポートフォリオは、<<anchor-3.3.4, 3.3.4. 提出するモデルの予測出力の定義>>に記載されているフォーマットのcsvとなります。

==== 使い方

　Backtestモジュールをインポートします。パスが通っていない場合は必要に応じて、sys.pathにBacktestモジュールを配置しているディレクトリを追加してからimportします。
[source,python]
----
# 以下はパスを通すためのコードになりますので、必要に応じてアンコメントして実行してください。
# import sys
# module_dir = "Backtestモジュールを配置したディレクトリへのフルパス"
# sys.path.append(module_dir)

from backtest import Backtest
----

　バックテストを実行するための事前準備として、バックテストに必要なデータを読み込みます。

[source,python]
----
data_dir = "/notebook/data_dir_comp2"  # "左記パスは例です。各自データを配置してるディレクトリへのパスへ変更してください"

# バックテストに必要なデータを取得します。
backtest_codes, backtest_price = Backtest.prepare_data(data_dir)
----

　バックテストを実行したいポートフォリオデータを読み込みます。

[source,python]
----
portofolio_file_path = "SUBMIT.csv"  # 左記パスは例です。各自モデルにより予測されたポートフォリオデータが格納されているパスへ変更してください。
df_submit = Backtest.load_submit(portofolio_file_path)
----

　バックテストを実行します。

[source,python]
----
df_results, df_stocks = Backtest.run(df_submit, backtest_codes, backtest_price)
----

===== 「Backtest.run」メソッドの説明

　バックテストの実行用メソッドである Backtest.run() への入力データは以下になります。

[source]
----
第一引数: ポートフォリオ (DataFrame)
第二引数: ユニバース (銘柄コード) (List)
第三引数: 株価情報 (DataFrame)
----

　バックテストの返り値は以下になります。

[source]
----
df_results: バックテスト結果のサマリー (DataFrame)
df_stocks: 個別銘柄の購入数や日々の価格情報 (DataFrame)
----

==== 結果の見方

　バックテストを実行するとバックテスト結果のサマリーが格納された `df_results` と、購入数および日々の価格情報が格納された `df_stocks` の2つのDataFrameが返されます。それぞれのDataFrameの項目は以下の通りです。

　`df_results` の項目は以下の通りです。

[source]
----
bought: 購入金額
cash: 現金
date: 対象週の開始日
day_1: ポートフォリオの月曜日終値での基準価格 (現金含む)
day_2: ポートフォリオの火曜日終値での基準価格 (現金含む)
day_3: ポートフォリオの水曜日終値での基準価格 (現金含む)
day_4: ポートフォリオの木曜日終値での基準価格 (現金含む)
day_5: ポートフォリオの金曜日終値での基準価格 (現金含む)
day_1_return: 月曜日のリターン (%)
day_2_return: 火曜日のリターン (%)
day_3_return: 水曜日のリターン (%)
day_4_return: 木曜日のリターン (%)
day_5_return: 金曜日のリターン (%)
day_1_pl: 月曜日の損益
day_2_pl: 火曜日の損益
day_3_pl: 水曜日の損益
day_4_pl: 木曜日の損益
day_5_pl: 金曜日の損益
exp: 日次リターンの平均
holiday: 祝日の曜日 (0: 月曜日, 4: 金曜日)
sharp: 日次リターンのシャープレシオ
std: 日次リターンの標準偏差
week_pl: 週の損益
week_return: 週のリターン
----

　`df_stocks` の項目は以下の通りです。

[source]
----
actual: 実際に購入した数量
date: 基準日付 (月曜日日付)
Local Code: 銘柄コード
budget: 指定した購入金額
n: 購入順
entry: 週の始値
day_1: 月曜日の終値
day_2: 火曜日の終値
day_3: 水曜日の終値
day_4: 木曜日の終値
day_5: 金曜日の終値
bought: 購入金額
actual: 購入株数
----

[[anchor-3.6.4]]
==== バックテストの評価軸と取引戦略

　バックテストを実行するとトレードの履歴を得ることができます。それらのトレード履歴を分析して評価してみましょう。トレードの代表的な評価軸としては、以下のようなものがあります。

[options="header, autowidth"]
|===
|評価軸 | 説明
|勝率| トレードがどの程度勝つかを示す指標です。勝ちトレード数 (利益が0を超えるトレード数) を総トレード数で割ることで算出します。
|平均リターン | トレード1回あたりの平均の利益率を示します。合計損益を総トレード数で割ることで算出します。
|最大ドローダウン | 累積リターンの最大地点からの下落率のことです。取引戦略のリスクを知る上で重要な指標です。
|ベータ | ベンチマークとなる指数や取引対象のユニバースに対する合計収益の連動率です。たとえば、ベンチマークとなる指数が10%上昇したときに11%上昇したらベータは1.1となります。
|ペイオフレシオ | トレード1回あたりの損益率を示します。トレードの平均利益を平均損失で割ることで算出します。
|シャープレシオ | リスクに応じた利益を得られているかを示します。リターンから安全資産利回りを引いてそのリターンの標準偏差で割ることで算出します。週・月・年などの計算の単位で大きく結果が変わることに注意が必要です。日本株のアルゴリズムトレードの評価では、安定資産利回りは0%で計算することが多いです。
|インフォメーション・レシオ | ベンチマークと比べて安定した利益を得られているかを示します。リターンからベンチマーク利回りを引いたものを平均して標準偏差で割ることで算出します。
|===

　バックテスト実行時は、一つの評価軸のみを確認するのではなく、複数の評価軸を組み合わせて解釈することでトレード戦略の特性を把握することが重要です。考慮すべき観点は多数存在しますが、その一部を紹介します。

- 勝率のみに注目すると、小さい勝ちを積み重ねていても、実際には大きな負けにより収益を失っていることに気づかないことがあります。そのため、平均リターンやペイオフレシオとセットで確認し、高い勝率を実現するために何をトレードオフにしているかに注目しましょう。
- 取引戦略はマーケット全体の動きの影響を強く受けます。その取引戦略のユニバースに対する特性を知るために、まずはベータを確認しましょう。一般的にベータが高い取引戦略を取っている場合、マーケット全体の暴落時に大きな暴落が発生することになるため、ベータが高い取引戦略では最大ドローダウンが大きくなる傾向があることに注意が必要です。逆に低いベータで同等の収益性が実現できている取引戦略は、高いベータの取引戦略よりも相対的にリスクが低く、パフォーマンスが安定すると考えることができます。
- 最大ドローダウンは、その取引戦略のリスクを評価する上で重要な評価軸です。自分の取引戦略が過去に最大でどのくらいの下落をしたかは常に把握しましょう。
- 取引戦略の評価をする際に対比軸となるベンチマークをどのように設定するかを常に考えましょう。本チュートリアルでは、取引戦略の効果を評価するため取引可能な全銘柄の平均リターンをベンチマークに採用しています。本コンペティションではデータは提供されていませんが、TOPIXや日経平均株価等もベンチマークとして採用されることが多く、それぞれ特性が異なります。
- 取引戦略を評価する際は、バックテストを行う期間におけるマーケットの特性を把握することが重要です。例えば、2020年は新型コロナウイルス感染症(COVID‑19)の影響で、前半マーケットは大きく値下がりした後、世界的に経済対策のための金融緩和が加速し結果的に歴史的な株高となりました。このような期間でも安定的に勝てる取引戦略は、前半の暴落に対する備えと、後半のトレンドが発生した時に収益化できる特性を両方備えている必要があります。2016年から2019年においてもそれぞれマーケットの特性がありますので、取引戦略の評価をする際はできるだけ多角的に多用な期間を評価し、その取引戦略の強み・弱みをしっかりと把握することが重要です。
- 取引戦略を評価する上で重要となるのが実現可能性です。今回は取引コストを0と仮定しているため、細かくポートフォリオを入れ替えることにパフォーマンスのペナルティは発生しませんが、実際に取引を行う場合は取引手数料は重要な要素となります。また、本コンペの評価手法に関連する箇所としては、取引対象として指定した数量を市場の始値で本当に購入できるか、というポイントもあります。本コンペでは取引対象のユニバースを時価総額200億円以上としているため、個人で取引を行う上でも流動性等についてそこまで大きな問題にはなりませんが、より時価総額が小型の株式銘柄をユニバースとして利用する場合はこの観点も注意してください。
- 平均リターンや勝率などを株価の動きそのものではなく、取引対象銘柄の平均をベンチマークとして、ベンチマークとの相対的な差を計算することで相対的なリターン(残差リターンなどと呼ばれます)や相対的な勝率を計算することができます。例えば、ベンチマークが10％下落したときに、ポートフォリオが1%の下落に収まっている場合、相対的なリターンは+9%勝っていることになります。このようなベンチマーク対比の計算も合わせて行うことで、より多角的な評価をすることが可能になります。


=== シンプルなポートフォリオ組成モデルの作成

　バックテストの実行方法を理解したので、いよいよシンプルなポートフォリオ組成モデルを作成しましょう。ポートフォリオ組成モデルを作成することを通して、本コンペティションに投稿するモデルの出力や評価方法を把握し、最終的には作成したモデルを投稿して結果がリーダーボードに掲載されることを確認します。

　本節では以下を説明しています。

- ポートフォリオ組成モデルの作成方法
- バックテストの使用方法
- 投稿用パッケージの作成方法

　具体的には、以下のステップで進めていきます。

[source]
----
1. 必要なライブラリの読み込み
2. データセットの読み込み
3. ポートフォリオ組成戦略の策定
4. ポートフォリオの組成
5. 出力の調整
6. バックテストの実行
7. バックテストの評価
8. 投稿用パッケージのディレクトリ作成
9. 作成したコードをランタイム実行用にクラスにまとめる
10. 提出用パッケージの作成と提出
----

==== 必要な入力データ等

■バックテスト用クラス +
本コンペティションの評価方法と同等のロジックを実装したバックテスト用のクラスを提供しています。本notebookからimportできる必要があります。import時にエラーとなる場合は、`backtest.py` ファイルをダウンロードしている確認の上、sys.path に追加して再実行してください。なお、バックテスト用のクラスの取得方法は、「<<anchor-3.6, 3.6. バックテスト環境の構築>>」をご参照ください。

■データセット +
本章で構築するモデルにおいては、以下のデータファイルを使用します。そのため、コンペティションサイトからダウンロードしたデータファイルを配置し、ディレクトリパスを `dataset_dir` 変数に設定してください。

- stock_list.csv.gz
- stock_price.csv.gz


==== 必要なライブラリの読み込み

　ランタイム環境とGoogle Colaboratory環境の両者で共通のライブラリを使用するためにバージョンを調整します。

[source,python]
----
!pip install --no-cache-dir joblib==1.0.1 numpy==1.19.5 pandas==1.1.5 scikit-learn==0.20.3 scipy==1.2.1 seaborn==0.9.0
----

　以下のライブラリを読み込みます。

[source,python]
----
import io
import os
import sys
import zipfile

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm.auto import tqdm
from scipy import stats
from IPython.core.magic import register_cell_magic
----

　次に本コンペティションの評価検証用のバックテストモジュールを読み込みます。インポート時にエラーが出た場合は、sys.path に backtest.py ファイルを配置したディレクトリを追加してから再度インポートしてください。

[source,python]
----
# インポート時にエラーが出た場合は、以下のmodule_dirをbacktest.pyを配置したディレクトリに変更してください。
import sys
if 'google.colab' in sys.modules:
  # Backtestを配置したディレクトリへのフルパスを指定します
  module_dir = f"{mount_dir}/MyDrive/JPX_competition/Chapter03/backtest"
else:
  # Backtestを配置したディレクトリへのフルパスを指定します
  module_dir = "/notebook/Chapter03/backtest"
sys.path.append(module_dir)

from backtest import Backtest
----

　Pandasのデータを表示する際に表略されないように設定を変更します。

[source,python]
----
# 表示用の設定を変更します
pd.options.display.max_rows = 100
pd.options.display.max_columns = 100
pd.options.display.width = 120
----

==== データセットの読み込み

　データセットを配置したディレクトリのパスを設定します。Google Colabをご利用の場合は Google Drive にデータセットをアップロードして、そのディレクトリを指定してください。また、データセットの取得方法および内容については「<<anchor-3.4, 3.4. データセットの説明>>」をご参照ください。

[source,python]
----
# データセットを配置したディレクトリのパスを設定
if 'google.colab' in sys.modules:
    dataset_dir = f"{mount_dir}/MyDrive/JPX_competition/data_dir_comp2"
else:
    dataset_dir = "/notebook/data_dir_comp2"
----

　本コンペティションのランタイム環境におけるデータセットへのアクセスは、 `ScoringService.predict()` メソッドに渡されるinputsパラメーターを通して行う必要があります。そのため、以下のように本notebook環境でもランタイム環境と共通の方法でデータセットにアクセスすることで、コードが複雑になったり投稿用にコードを編集したりしなくても済むようにしています。

[source,python]
----
# 入力パラメーターを設定します。ランタイム環境での実行時と同一フォーマットにします
inputs = {
    "stock_list": f"{dataset_dir}/stock_list.csv.gz",
    "stock_price": f"{dataset_dir}/stock_price.csv.gz",
}
----

　本コンペティションでは2016年以降のデータを提供していますが、本notebookではモデル作成・評価時の処理時間を短くするために 2020-01-01 以降のデータを使用してバックテストを実施・評価します。なお、実際に評価をする場合は可能な限り長い期間を評価に利用することを推奨します。

[source,python]
----
# 投資対象日付を指定します
start_dt = pd.Timestamp("2020-01-01")
----

　ランタイム環境においては `ScoringService.predict()` メソッドに渡されるinputsパラメーターに `purchase_date` というキー名で予測対象日が記載されたCSV形式のファイルへのパスが渡され、そのファイル内に記載されている日付を予測対象日として使用します。ここではランタイム環境に対応するために `purchase_date` が存在する場合は、指定された日付を使用するロジックを組み込んでおきます。purchase_date のフォーマットについては link:https://signate.jp/competitions/443/data[SIGNATEのコンペティションサイト] にサンプルファイルが配置されているためそちらをご参照ください。

[source, python]
----
if "purchase_date" in inputs.keys():
    # ランタイム環境では指定された投資対象日付を使用します
    # purchase_dateを読み込み
    df_purchase_date = pd.read_csv(inputs["purchase_date"])
    # purchase_dateの最も古い日付を投資対象日付として使用します
    start_dt = pd.Timestamp(df_purchase_date.sort_values("Purchase Date").iloc[0, 0])
----

　本コンペティションでは投資対象となる銘柄群 (ユニバース) が設定されています。そのため、ユニバース内の銘柄に絞って処理を実施するために銘柄情報を読み込みます。

[source,python]
----
# 銘柄情報読み込み
df_stock_list = pd.read_csv(inputs["stock_list"])
# 問題2のユニバース (投資対象の条件を満たす銘柄群) 取得
codes = df_stock_list.loc[
    df_stock_list.loc[:, "universe_comp2"] == True, "Local Code"
].unique()
----

　以下では、シンプルに株価情報のみを利用してポートフォリオを組成するために株価情報を読み込んでいます。本コンペティションでは、データセットはcsv.gz形式で提供していますので、データの型情報が保存されていません。そのため、特に日付型のカラムについては明示的に変換する必要があります。read_csvのparse_dateパラメーター等、日付型を指定する方法は複数ありますが、本notebookでは一度読み込んでから変換しています。

[source,python]
----
# 価格情報読み込み、インデックス作成
df_price = pd.read_csv(inputs["stock_price"]).set_index("EndOfDayQuote Date")
# 日付型に変換
df_price.index = pd.to_datetime(df_price.index, format="%Y-%m-%d")
----

　処理時間を最適化するために処理対象データを日付でフィルタをして絞り込みます。本notebookでは過去20営業日のデータを使用して特徴量を作成するため、投資対象日付から過去20営業日時点のデータを含める必要がありますが、バッファとして過去30日のデータを含めることにします。同時に株価情報をユニバースと一致するように絞り込んでいます。

[source,python]
----
# 投資対象日の前週金曜日時点で予測を出力するため、予測出力用の日付を設定します。
pred_start_dt = pd.Timestamp(start_dt) - pd.Timedelta("3D")
# 特徴量の生成に必要な日数をバッファとして設定
n = 30
# データ絞り込み日付設定
data_start_dt = pred_start_dt - pd.offsets.BDay(n)
# 日付で絞り込み
filter_date = df_price.index >= data_start_dt
# 銘柄をユニバースで絞り込み
filter_universe = df_price.loc[:, "Local Code"].isin(codes)
# 絞り込み実施
df_price = df_price.loc[filter_date & filter_universe]
----

　`head()` や `tail()` メソッドを使用して処理結果が期待通りとなっていることを確認しながら進めていきます。ここではデータが2019年11月20日以降に絞り込まれていることが確認できます。`.T`  プロパティ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html#pandas.DataFrame.transpose を使用して行と列を入れ替えることで可読性が上がる場合があります。

　ここで計算したデータセットのフォーマットを確認するために、データセットの先頭を見てみます。

[source,python]
----
df_price.head(3).T
----

image::simple/simple.png[処理結果, width="70%"]

[[anchor-3.7.4]]
==== ポートフォリオ構築戦略の策定

　ポートフォリオを組成するための特徴量を作成します。今回は以下の2種類の戦略を採用します。

1. リターン・リバーサル (逆張り) 戦略を採用して、過去1ヶ月(約20営業日)の株価下落率の上位25銘柄を選択します。
(「リターン・リバーサル」 野村證券証券用語解説集より引用 https://www.nomura.co.jp/terms/japan/ri/A01944.html)
2. トレンドフォロー (順張り) 戦略を採用して、過去1ヶ月(約20営業日)の株価上昇率の上位25銘柄を選択します。
(「トレンドフォロー」 野村證券証券用語解説集より引用 https://www.nomura.co.jp/terms/japan/ta/A02002.html)

　過去1ヶ月(20営業日)の株価下落率/上昇率を計算するために、銘柄毎にグループ化してから株価変化率を計算します。 +
　本コンペティションに提出するポートフォリオは各週の週初の営業日に買付実施されるため、買付日前週の金曜日終値時点を銘柄選択の基準とします。基準日のデータを取得するために単純に金曜日にのみ絞り込んだ場合、金曜日が祝日の場合にその翌週の銘柄を選択できなくなるおそれがあるため、平日でリサンプル(pandasにおいて平日を意味する ``B`` を指定してresample関数を呼んでいます)し、欠損値がある場合には前日の値を使うように前方補完(pandasではffill関数を利用)を実施します。これにより、金曜日に必ずデータが存在するようにしています。

[source,python]
----
# 欠損値がある場合にも正しく動作しているかを確認するため、処理前に木曜日、金曜日が祝日である2020-07-23、2020-07-24のレコードが存在しないことを確認しておきます。
df_price.loc["2020-07-22":"2020-07-27"].head(4)
----

image::simple/simple1.png[処理結果, width="90%"]

[source,python]
----
# groupby を使用して処理するために並び替え
df_price.sort_values(["Local Code", "EndOfDayQuote Date"], inplace=True)
# 銘柄毎にグループにします。
grouped_price = df_price.groupby("Local Code")[
    "EndOfDayQuote ExchangeOfficialClose"
]
# 銘柄毎に20営業日の変化率を作成してから、金曜日に必ずデータが存在するようにリサンプルして前方補完します。
df_feature = grouped_price.apply(
    lambda x: x.pct_change(20).resample("B").ffill().dropna()
).to_frame()
----

　大量のデータを処理する場合、処理によっては数分から数時間かかる場合があります。処理済みのデータを保存しておくことで、処理時間のかかる処理を省略して作業できるようにすることは重要なテクニックの一つです。今回はメモリ上に別の変数として保存しておきますが、セッションが閉じられる際に処理済みデータもクリアされてしまうことに加えて、大量のデータである場合はメモリ上に保存しておくとメモリを圧迫するため、ファイルに書き出しておいて必要な時にファイルから読み込むのが良い方法です。

　Pandasには様々な形式でのデータの入出力用メソッド (link:https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html[リンク]) が用意されています。例えば、データが圧縮されて型が保存される `to_hdf` メソッド (link:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html[リンク])を使用してファイルに書き出し、対応する `read_hdf` メソッド (link:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html[リンク])で読み出すことで簡単にデータフレームを読み書きできます。

　平日を指定してデータのリサンプル行い、欠損値に前日の値を使うように前方補完した結果が期待通りになっているかを確認しましょう。2020-07-23及び2020-07-24はそれぞれ木曜日及び金曜日の祝日でしたので、2020-07-23及び2020-07-24に2020-07-22の値が入っていれば良いことになります。

[source,python]
----
# 上記は比較的時間のかかる処理なので、処理済みデータを別に残しておきます。
df_work = df_feature.copy()

# 処理後に木曜日、金曜日が祝日である2020-07-23、2020-07-24のレコードが前方補完されていることを確認します。
df_work.loc[(slice(None), slice("2020-07-22", "2020-07-27")),].head(4)
----

　下図のように木曜及び金曜の祝日である2020-07-23及び2020-07-24に2020-07-22の値が入っていることが確認できます。

image::simple/simple2.png[処理結果, width="50%"]

　以下のコードでデータを整えます。インデックスとカラム名を調整しています。

[source,python]
----
# インデックスが銘柄コードと日付になっているため、日付のみに変更します。
df_work = df_work.reset_index(level=[0])
# カラム名を変更します。
df_work.rename(
    columns={"EndOfDayQuote ExchangeOfficialClose": "pct_change"},
    inplace=True,
)
# データをpred_start_dt以降の日付に絞り込みます。
df_work = df_work.loc[df_work.index >= pred_start_dt]
----

　特徴量が生成されていることを確認します。

[source,python]
----
# df_workの最初の3行を出力する。
df_work.head(3)
----

image::simple/simple3.png[処理結果, width="50%"]


==== ポートフォリオ組成を行う上での注意事項

　本コンペのようにデータサイエンス的な手法で取引戦略を構築する上で重要なのは、取引戦略に合致した銘柄選択を実施してポートフォリオを組成することです。取引戦略をデータサイエンスな的手法で構築している場合、そこから実現したい収益は、統計的に他の銘柄よりも収益性が高い銘柄をモデルが選択することで達成しているはずです。しかし、実際に良いモデルを作ることができても、そこから収益を実現するためには工夫が必要となります。ここでは、その際の注意点を考察します。

===== 取引対象銘柄数の選択

　予測モデルのスコアから収益を実現するためには、ポートフォリオを組成する取引対象銘柄の数の選択が重要となります。予測モデルから算出されるスコアが高い収益を仮に予測できていたとしても、取引対象銘柄の数が少なすぎると決算などの銘柄の個々の要因により予測した収益性の効果が消されることがあります。

　同一の合計金額で株式を購入する際、スコアの上位5銘柄で組成したポートフォリオと上位20銘柄で組成したポートフォリオのどちらのポートフォリオが、銘柄の個々の要因の影響を受けにくいかを考えると、20銘柄の方が影響を受けにくいことになります。ただし、取引銘柄数を増やしすぎると取引戦略の実現に必要な現金が増えたり、スコアの有効性があまり無い銘柄もポートフォリオに含まれる可能性があるため、最終的な購入銘柄数はバックテストを通して調整する必要があります。

　なお、データサイエンス的手法の利用用途において、まれに発生する現象を探索するような方法(アノマリーディテクションなどと呼ばれます)があります。このようなモデルでは20銘柄も有効なスコアが出ていない可能性があります。この場合、バックテストを通して、有効性が高いスコアのしきい値を探索し、ある一定のスコアを超えていたら売買を行うようなロジックを構築することもあります。

===== 購入金額の決定

　購入金額は特別な理由が無い限りは、銘柄ごとにある程度均等になるように購入するのが良いでしょう。直感的には予測スコアの上位の銘柄を、より多く購入したいと考えるかもしれませんが、この試みは結果的に予測スコアの上位の銘柄の個々の要因により負ける可能性を増やすことに繋がります。予測モデルで収益性を予測してみると、そのスコアの統計的な効果はさほど強くない(未来の値動きに対する相関係数は0.1から高くても0.2程度に収まる)ことが多く、上位のスコアの銘柄に対して取引金額の割合をより多くするほどの統計的な強度は観測されていないことが多いです。

　また、最上位の予測スコアの銘柄には注意が必要です。データサイエンス的な手法でモデルを作ると、外れ値等に反応するモデルが出来上がることがあります。このような動きは、例えば特別損失の適時開示等のコーポレーションアクションで引き起こされている事が多く、リスク回避の観点からはそのようなモデルは取引対象に含めるべきではありません。本コンペティションのデータには、TDnetのデータが含まれているため、そのような銘柄を予め取引対象のユニバースから除外するアプローチも後ほど紹介しています。

==== ポートフォリオの組成手順

　今回のポートフォリオでは買付日前週の金曜日終値時点を銘柄選択の基準とするため、金曜日のデータのみに絞り込みます。すでに「<<anchor-3.7.4,3.7.4. ポートフォリオ構築戦略の策定>>」において、金曜日が祝日の場合の処理はしていますので、そのまま金曜日に絞り込んで問題ありません。

[source,python]
----
# 金曜日のデータのみに絞り込みます
df_work = df_work.loc[df_work.index.dayofweek == 4]
----

　金曜日のデータのみとなっていることを確認します

[source,python]
----
# df_workの最初の2行を出力する。
df_work.head(2)
----

image::simple/simple4.png[処理結果, width="50%"]

　次に、日付毎にグループ化してから、下落率上位25銘柄を選択しています。本コンペティションの評価では、出力されたポートフォリオに記載されている順番で銘柄が購入されるため、なるべくリターンが高くなる銘柄から先に出力して購入されるようにすることが最適と考えられます。ここでは、下落率が高い銘柄ほどリターンの大きくなるとの仮説を立てて、`pct_change` について昇順で並べ替えてから銘柄を選択しています。こうすることで下落率が高い銘柄順に出力されるようにしています。

　なお、ここでは説明をシンプルにするために特に理由なく25銘柄を選択していますが、例えば、5銘柄から50銘柄まで5銘柄ずつ増加させた合計10個のポートフォリオを組成してバックテストでパフォーマンスを比較することで、この戦略における最適な選択銘柄数を見つけられるかもしれません。また、その場合、50銘柄のポートフォリオを組成しておいて、バックテスト投入時に銘柄数を絞り込むロジックを組むことで処理時間を最適化できるかもしれません。

[source,python]
----
# 日付毎に処理するためグループ化します
grouped_work = df_work.groupby("EndOfDayQuote Date", as_index=False)
----

[source,python]
----
# 選択する銘柄数を指定します
number_of_portfolio_stocks = 25

# ポートフォリオの組成方法を戦略に応じて調整します
strategies = {
    # リターン・リバーサル戦略
    "reversal": {"asc": True},
    # トレンドフォロー戦略
    "trend": {"asc": False},
}

# 戦略に応じたポートフォリオを保存します
df_portfolios = {}

# strategy_id が設定されていない場合は全ての戦略のポートフォリオを作成します
if "strategy_id" not in locals():
    strategy_id = None

for i in [strategy_id] if strategy_id is not None else strategies.keys():
    #  日付毎に戦略に応じた上位25銘柄を選択します。
    df_portfolios[i] = grouped_work.apply(
        lambda x: x.sort_values(
            "pct_change", ascending=strategies[i]["asc"]
        ).head(number_of_portfolio_stocks)
    )
----

　以下を確認します。

1. 1つの週に対して25銘柄選択されていること
2. 戦略に応じてpct_changeの値が反転していること

[source,python]
----
# 結果結合用
buff = []
# 戦略毎に処理
for i in df_portfolios.keys():
    # ポートフォリオを表示用に保存
    buff.append(
        df_portfolios[i]
        # マルチインデックスは操作しにくいので日付のみに変更します
        .reset_index(level=[0])
        # 先頭の26レコードを取得します
        .head(26)
        # 結合した後の列名をわかりやすくするために変更します
        .rename(columns={v: f"{i}_" + v for v in df_portfolios[i].columns})
    )
# 結合して保存
pd.concat(buff, axis=1)
----

image::simple/simple5.png[処理結果, width="70%"]

　ポートフォリオに組み入れる銘柄を決めたので、各銘柄について購入金額を指定します。今回はシンプルにするために50,000円を一律で指定して購入金額の総額を25銘柄分で合計125万とすることで、1株の価格が5万円を超えている銘柄が含まれていても予算上限に近い金額を購入できるようにしています。株価を参照して銘柄数や購入金額を調整することも検討してみてください。なお、本コンペティションのポートフォリオは、対象週の月曜日日付を指定する必要がありますので、金曜日から月曜日日付に変更しています。

[source,python]
----
# 銘柄ごとの購入金額を指定
budget = 50000
# 戦略毎に処理
for i in df_portfolios.keys():
    # 購入株式数を設定
    df_portfolios[i].loc[:, "budget"] = budget
    # インデックスを日付のみにします
    df_portfolios[i].reset_index(level=[0], inplace=True)
    # 金曜日から月曜日日付に変更
    df_portfolios[i].index = df_portfolios[i].index + pd.Timedelta("3D")
----

これでポートフォリオが完成しました。完成したデータを確認します。

[source,python]
----
# 戦略毎に処理
for i in df_portfolios.keys():
    # 戦略名を表示
    display(i)
    # 表示
    display(df_portfolios[i].head(3))
----

image::simple/simple6.png[処理結果, width="50%"]

　ポートフォリオ組成に用いた特徴量やその他のカラムが残っているため、本コンペティションで決められている出力フォーマットと一致するように出力を調整します。

==== 出力の調整

　本コンペティションのポートフォリオの出力フォーマットは「<<anchor-3.3.4, 3.3.4. 提出するモデルの予測出力の定義>>」をご参照ください。ここでは出力フォーマットに合わせるためにインデックス名やカラム数を調整します。

[source,python]
----
# 戦略毎に処理
for i in df_portfolios.keys():
    # インデックス名を設定
    df_portfolios[i].index.name = "date"
    # 出力するカラムを絞り込みます
    df_portfolios[i] = df_portfolios[i].loc[:, ["Local Code", "budget"]]
----

　ポートフォリオが出力フォーマットと一致していることを確認します。

[source,python]
----
# 戦略毎に処理
for i in df_portfolios.keys():
    # ポートフォリオを確認
    display(df_portfolios[i].head(3))
----

image::simple/simple7.png[処理結果, width="30%"]

　本コンペティションの `ScoringService.predict` の出力仕様はcsv形式の文字列であるため、仕様に合わせて出力します。

[source,python]
----
# 出力保存用
outputs = {}
# 戦略毎に処理
for i in df_portfolios.keys():
    # 出力します
    out = io.StringIO()
    # CSV形式で出力
    df_portfolios[i].to_csv(out, header=True)
    # 出力を保存
    outputs[i] = out.getvalue()
----

　出力を確認します。

[source,python]
----
# 戦略毎に処理
for i in outputs.keys():
    # 戦略名を表示
    print(f'// "{i}"')
    # 出力を確認
    print("\n".join(outputs[i].split("\n")[:4]))
----

出力
[source]
----
// "reversal"
date,Local Code,budget
2020-01-06,4592,50000
2020-01-06,3254,50000
2020-01-06,6875,50000
// "trend"
date,Local Code,budget
2020-01-06,6387,50000
2020-01-06,4772,50000
2020-01-06,6195,50000
----

　バックテスト用に出力を保存しておきます。

[source,python]
----
# 戦略毎に処理
for i in outputs.keys():
    # 出力を保存します。
    with open(f"chapter03-tutorial-01-{i}.csv", mode="w") as f:
        # ポートフォリオをファイルに書き出します。
        f.write(outputs[i])
----


==== バックテストの実行

　本コンペティションの Public LB および Private LB と同等の評価ロジックを実装したバックテスト用の `backtest.py` ファイルを使用して、作成したポートフォリオを評価します。
バックテストの使用法などの詳細は、「<<anchor-3.6, 3.6. バックテスト環境の構築>>」をご参照ください。

　初めにバックテストを使用する際に必要なデータを準備します。バックテストの実行には以下の3つのデータが必要になります。

1. ユニバース (stock_list.csv.gz)
2. 株価 (stock_price.csv.gz)
3. テスト対象のポートフォリオ

　`Backtest.prepare_data` に1および2のデータを保存しているディレクトリへのパスを指定して、必要なデータをロードします。

[source,python]
----
# データを保存しているディレクトリを指定します。
backtest_dataset_dir = dataset_dir
# バックテストに必要なデータを読み込みます。
backtest_codes, backtest_price = Backtest.prepare_data(backtest_dataset_dir)
----

　バックテスト対象の戦略であるリターン・リバーサル戦略とトレンドフォロー戦略を定義します。

[source,python]
----
# ポートフォリオの組成方法
backtest_strategies = {
    # リターン・リバーサル戦略
    "reversal": {},
    # トレンドフォロー戦略
    "trend": {},
}
----

　`Backtest.load_submit` にテスト対象のポートフォリオを保存したファイルへのパスを指定して読み込みます。`load_submit` ではデータを読み込み時にフォーマットのチェックをしたり、レコード順を付与するなどのバックテスト実行のための前処理を実施しています。

[source,python]
----
# ポートフォリオデータ保存用
df_submits = {}
# 先ほど出力したポートフィリオデータを読み込みます
for i in backtest_strategies.keys():
    # ポートフォリオを読み込み
    df_submits[i] = Backtest.load_submit(f"chapter03-tutorial-01-{i}.csv")
----

　3つのデータを指定してバックテストを実行します。

[source,python]
----
# バックテスト結果リターン情報保存用
results = {}
# バックテスト結果銘柄情報保存用
stocks = {}
# 戦略毎に処理
for i in tqdm(backtest_strategies.keys()):
    # バックテストを実行します
    results[i], stocks[i] = Backtest.run(df_submits[i], backtest_codes, backtest_price)
----

image::simple/simple8.png[処理結果, width="50%"]

　返り値を確認します。評価は下記で実施します。

[source,python]
----
# バックテスト結果のサマリー
results["reversal"].head(3)
----

image::simple/simple9.png[処理結果, width="100%"]

[source,python]
----
# 銘柄毎の情報
stocks["reversal"].head(3)
----

image::simple/simple10.png[処理結果, width="70%"]

==== バックテストの評価

　バックテストを実行して取得した、週毎のリターン情報と各銘柄毎の購入結果数の情報を評価していきます。各評価項目の定義については、「<<anchor-3.6.4, 3.6.4. バックテストの評価軸と取引戦略>>」をご参照ください。

　結果の評価として以下を実施します。

*週毎のリターンデータ*

1. 週毎の運用実績（PL）の分布をプロット (週毎の運用実績の合計値が本コンペティションの評価項目)
2. 週毎の運用実績の統計量の算出
3. 週毎の勝率・ペイオフレシオ・シャープレシオの算出
4. 週毎のリターン推移のプロット
5. 曜日別分析のためのバイオリンプロット
6. 週毎のリターンの累積プロット
7. ユニバースとの散布図
8. ユニバースに対するベータを計算

===== 1.週毎の運用実績の分布をプロット

　まず、週毎の運用実績の分布をプロットしてみます。

[source,python]
----
# 描画領域を定義
fig, axes = plt.subplots(1, len(results), figsize=(8 * len(results), 8), sharex=True, sharey=True)

# 戦略毎に処理
for i, k in enumerate(results.keys()):
    # 描画位置を指定
    ax = axes[i]
    # 分布をプロット
    results[k].week_pl.hist(bins=25, ax=ax)
    # タイトルを設定
    ax.set_title(f"{k}: week pl distribution")
#　描画
plt.show()
----

image::simple/simple11.png[処理結果, width="70%"]

　trendとreversalは共にほぼ0平均に見えますが、reversalは分布が若干広いように見え、週によっては大きなマイナスも観測されていることがわかります。おそらくCOVID-19の暴落が発生した週が該当すると想像できます。ただし、大きなプラスの週もあるため、その負けを取り返しているかもしれません。ただ、このプロットだけではあまり戦略の良し悪しはわかりません。


===== 2.週毎の運用実績の統計量を算出

　週毎の運用実績の統計量を算出します。

[source,python]
----
# week_plの分布の統計量

# 結合用データ保存
buff = []
# ストラテジー毎に処理
for k in results.keys():
    # week_plの統計量を取得します。
    df = results[k].loc[:, ["week_pl"]].describe().T
    # インデックスを編集してストラテジーのIDにする
    df.index = [k]
    # インデックス名変更
    df.index.name = "strategy_id"
    # 結合用に保存
    buff.append(df)
# 結合して表示
pd.concat(buff)
----

image::simple/simple12.png[処理結果, width="70%"]

　週毎の運用実績の統計量を確認すると、reversalがtrendより平均(mean)が高いことがわかります。ただし、中央値(50%)を確認するとtrendの方が高いのでreversalは小さな勝ちではなく、大きな勝ちを利用して平均を押し上げていることがわかります。また、25%分位点では大きな差異はないのに最小値はreversalがずっと小さいことから、大きな負けがあると想定されます。

===== 3.週毎の勝率、ペイオフレシオ、シャープレシオを算出

　週毎の勝率、ペイオフレシオ、シャープレシオを算出します。

[source,python]
----
# 結合用データ保存
buff = []
# 戦略毎に処理
for k in results.keys():
    df_return = results[k]
    # 計算結果保存用
    d = {}
    # 件数
    d["count"] = df_return.shape[0]
    # 勝率
    d["win_ratio"] = (
        df_return.loc[df_return.loc[:, "week_return"] > 0].shape[0] / d["count"]
    )
    # ペイオフレシオ
    d["payoff_ratio"] = df_return.loc[
        df_return.loc[:, "week_return"] > 0, "week_return"
    ].mean() / (
        -1 * df_return.loc[df_return.loc[:, "week_return"] <= 0, "week_return"].mean()
    )
    # シャープレシオ
    d["sharp"] = (
        df_return.loc[:, "week_return"].mean() / df_return.loc[:, "week_return"].std()
    )
    # 平均PL
    d["avgPL"] = df_return.loc[:, "week_pl"].mean()
    # week_plの合計
    d["PL"] = df_return.loc[:, "week_pl"].sum()
    # strategy_idを設定
    df = pd.DataFrame([d], index=[k])
    # インデックス名を指定
    df.index.name = "strategy_id"
    # 結合用に保存
    buff.append(df)
# 結合して表示
pd.concat(buff)
----

image::simple/simple13.png[処理結果, width="50%"]

　reversalの週毎の勝率は50%ですが、payoff_ratioも1がより大きく一回の勝ちが負けよりおおきいことがわかります。trendはを勝率が60%を超えており、reversalよりも安定的に勝てるポートフォリオの可能性があります。

===== 4.週毎に曜日別のリターンをプロット

　週毎の1日目から5日目までのリターンの推移をプロットし、曜日毎に勝ち負けの分布に差異が無いかを確認しています。

[source,python]
----
# 描画領域を定義
fig, axes = plt.subplots(
    len(results), 1, figsize=(20, 4 * len(results)), sharex=True, sharey=True
)

# 描画用データ保存用
dfs_plot = {}

# 戦略毎に処理
for i, k in enumerate(results.keys()):
    # 描画位置を指定
    ax = axes[i]
    # 列を行に変換
    dfs_plot[k] = (
        results[k]
        .set_index("date")
        .loc[
            :,
            [
                "day_1_return",
                "day_2_return",
                "day_3_return",
                "day_4_return",
                "day_5_return",
            ],
        ]
        .stack()
        .to_frame()
        .reset_index()
        .rename(columns={0: "return"})
    )
    # 作業用に変数設定
    df_plot = dfs_plot[k]
    # 曜日毎のreturnをプロット
    df_plot.groupby(["level_1", "date"]).first().unstack().plot(ax=ax, legend=False)
    # タイトルを設定
    ax.set_title(f"{k}: Daily returns")
    # リターンが0の位置に基準線を描画
    ax.axhline(y=0, color="black")
    # グリッドを表示
    ax.grid(True)
# 描画
plt.show()
----

image::simple/simple14.png[処理結果, width="70%"]

===== 5. 曜日別分析のためのバイオリンプロット
　上のグラフでは何が起きているかわかりにくいので、seabornのバイオリンプロット(link:https://seaborn.pydata.org/generated/seaborn.violinplot.html[リンク])を利用します。バイオリンプロットはデータの密度分布を確認できるグラフであり、統計的な差異がありそうな箇所を発見するために便利です。バイオリンの形状はカーネル密度推定による確率密度関数を表しており、バイオリンの中心部分に平均、中央値、25%タイル、75%タイルを示す箱が表示されています。

[source,python]
----
# 描画領域を定義
fig, axes = plt.subplots(len(results), 1, figsize=(15, 4 * len(results)), sharex=True, sharey=True)

# 戦略毎に処理
for i, k in enumerate(results.keys()):
    # 描画位置を指定
    ax = axes[i]
    # 箱が見やすいように横方向を指定してプロット
    sns.violinplot(x="return", y="level_1", data=dfs_plot[k], ax=ax, orient="h")
    # タイトルを設定
    ax.set_title(f"{k}: daily return")
    # グリッドを表示
    ax.grid(True)
# 文字が重なって読みにくいので間隔調整
fig.tight_layout(pad=2.0)
# 描画
plt.show()
----

image::simple/simple15.png[処理結果, width="100%"]

　reversal/trendは木・金曜日に負ける傾向がありそうです。trendはプラス側の裾野が若干広いように思われます。これはトレンドフォローを行うと大きな勝ちが取れている可能性が示唆されます。

　取引戦略によっては月曜日に大きく勝つモデルや金曜日に大きく負けるなど曜日によって強さが異なることも多く、このような曜日ごとのプロットはその銘柄の特性を知る上で、確認する価値があります。特に曜日や月などの周期でチェックする場合、負けている方(このグラフでいうととマイナス側の分布)に注目することが重要です。周期性を狙って収益を意図的に取得することは難易度の高いテクニックですが、負けの場合は理由をしっかりと分析すると防げる可能性があるためです。例えば、よくあるのが金曜日特有の週末に発生するクローズオーダーなど、機関投資家のルールにより発生する取引です。

===== 6.収益率の時系列を累積プロット

　次にいよいよ取得した収益率の時系列を累積プロットします。まず、比較対象として取引対象の全銘柄の平均週次リターンを計算します。

[source,python]
----
# 変数名を調整します。
# backtest_priceはユニバースで絞り込み済みです
df_price = backtest_price
----

[source,python]
----
# 週毎に始値と終値を取得
df_wp = (
    # start_dt以降の日付のみ計算
    df_price.loc[df_price.index >= start_dt].sort_values(["Local Code", "EndOfDayQuote Date"])
    # 銘柄コード毎に処理
    .groupby("Local Code")
    # 月曜日スタートで週にリサンプル
    .resample("W-MON", label="left", closed="left")
    # 始値は最初のレコード、終値は最後のレコードを取得
    .agg({"EndOfDayQuote Open": "first", "EndOfDayQuote ExchangeOfficialClose": "last"})
    # マルチインデックスを解除
    .reset_index(level=[0])
)
# Open が 0.0 の銘柄は値段が付かなかった銘柄で、バックテストでは購入対象外であるため除外する
df_wp = df_wp.loc[df_wp.loc[:, "EndOfDayQuote Open"] != 0.0]
# 銘柄毎の週次リターンを計算
df_wp.loc[:, "universe"] = (
    (
        (
            df_wp.loc[:, "EndOfDayQuote ExchangeOfficialClose"]
            / df_wp.loc[:, "EndOfDayQuote Open"]
        )
        - 1
    )
    * 100
)
# ユニバースの週毎のリターンを計算します。
df_universe_return = df_wp.groupby(df_wp.index)["universe"].mean().to_frame()
----

　対比軸である取引対象の全銘柄の平均週次リターンが準備できたら、今回の取引戦略の結果と一緒にプロットしてみます。

[source,python]
----
# 描画領域を定義
fig, axes = plt.subplots(1, 1, figsize=(20, 8), sharex=True, sharey=True)

# 戦略毎に処理
for k in results.keys():
    # 描画位置を指定
    ax = axes
    # 戦略別の累積リターンを描画
    results[k].set_index("date").loc[:, ["week_return"]].rename(
        columns={"week_return": f"{k}: week_return"}
    ).cumsum().plot(ax=ax)

# ユニバースの週次リターンの累積をプロット
df_universe_return.cumsum().plot(ax=ax, color="black", label="universe")

# 表示を調整
ax.set_title("Cumulative week_return")
# グリッドを表示
ax.grid(True)
# 描画
plt.show()
----

image::simple/simple16.png[処理結果, width="90%"]

　このプロットは興味深いことがわかります。

　まずreversalは、3月に発生した負けが非常に大きいことがわかります。ベンチマークはおよそ-20%の負けとなっているのに対し、reversalでは-40%に到達しています。これはリターン・リバーサル戦略を採用した場合、マーケット暴落時に負けが積み重なる現象として知られています。一方、その後に0%近辺まで戻しているので3月後半から5月末にかけて、取得できたリターンは非常に大きいこともわかります。ただ、対比軸である取引対象の全銘柄の平均週次リターンには到達していません。その後、reversal戦略のユニバースに対する有意性は6月以降はあまり観測できず、12月までユニバースの平均に勝てないまま最終的に負けています。

　次に、trendは3月上旬の負けがユニバースの平均と比較すると小さいことがわかります。一方、reversal戦略の場合に観測された大きな収益性は観測できず、6月にユニバースの平均と同一の水準になると、以降はreversalと同等に12月までユニバースの平均に勝てないまま最終的に負けています。

===== 7.ユニバースとリターンの散布図をプロット

　ユニバースとリターンの散布図は、マーケットの動きに対してポートフォリオの運用実績がどのように分布するかを確認するために利用します。

[source,python]
----
# 戦略毎に処理
for k in results.keys():
    # 散布図をプロット
    p = sns.jointplot(
        x=df_universe_return.iloc[:, 0],
        y=results[k].loc[:, "week_return"],
        kind="reg",
        height=5,
        stat_func=stats.pearsonr,
    )
    # タイトルを設定
    p.fig.suptitle(f"{k}: week_return vs universe")
    # タイトル表示用に位置を調整
    p.fig.subplots_adjust(top=0.95)
    # 描画
    plt.show()
----


image::simple/simple17.png[処理結果, width="100%"]


　reversalはユニバースに対して、全体が下がった時に負けが大きく、ユニバースが上がった時に勝ちがわずかにユニバースを上回ることが観測できます。trendは分布が広がっており、ユニバースの影響をあまり受けていないことがわかります。また、0でリターンを稼いでいる週が多く観測出来、マーケットが動いていない時に細かく勝てている可能性が示唆されます。

===== 8.ベータ値の算出

　上記の傾向はベータを計算すると一目瞭然です。

[source,python]
----
# 結合用に保存
buff = []
# 戦略毎に処理
for k in results.keys():
    # ベータを計算
    res = stats.linregress(df_universe_return.iloc[:,0], results[k].loc[:, "week_return"])
    # 一覧表示用にデータフレームを作成
    df_beta = pd.DataFrame([res.slope], index=[k], columns=["beta"])
    # インデックス名を設定
    df_beta.index.name = "storategy_id"
    # 保存
    buff.append(df_beta)
# 結合して表示
pd.concat(buff)
----

image::simple/simple19.png[処理結果, width="30%"]

　reversalはベータがおよそ1.6となっており、ユニバースが10%変動すると16%程度の変動が発生する取引戦略であることがわかります。ユニバースが-20%変動したときは-32%変動しますので、暴落時の大きな負けもこのベータ値の高さで説明ができます。3月のようにドローダウンが深くなる現象は、高いベータを持つ取引戦略にはよく観測されます。trendはベータが0.8近辺となっており、reversalと比較すると低いベータとなっています。

===== 銘柄毎に分析するための準備

　最後に銘柄毎のデータを使用して分析していきます。

*各銘柄毎のデータ*

1. 銘柄毎の運用実績の分布をプロット
2. 銘柄毎のreturnの分布をプロット
3. 週毎の勝ち銘柄率をプロット、統計量の算出

銘柄毎のデータを使用して分析するために必要な計算を実施します。


[source,python]
----
# 分析用データ保存用
dfs_analyze = {}
# 戦略毎に処理
for i in stocks.keys():
    # 分析用にデータをコピー
    df_analyze = stocks[i].copy()
    # day5に必ず値が存在するように調整します
    df_analyze.loc[:, ["day_1", "day_2", "day_3", "day_4", "day_5"]] = (
        df_analyze.loc[:, ["day_1", "day_2", "day_3", "day_4", "day_5"]]
        .replace(0.0, np.nan)
        .ffill(axis=1)
    )
    # 終値とエントリーの差分を計算
    df_analyze.loc[:, "diff"] = df_analyze.loc[:, ["entry", "day_5"]].diff(axis=1)[
        "day_5"
    ]
    # 損益を計算します
    df_analyze.loc[:, "pl"] = df_analyze.loc[:, "diff"] * df_analyze.loc[:, "actual"]
    # リターンを計算します
    df_analyze.loc[:, "return"] = (
        (df_analyze.loc[:, "day_5"] / df_analyze.loc[:, "entry"]) - 1
    ) * 100
    # infを0.0に変換
    df_analyze = df_analyze.replace(np.inf, 0.0)
    # 処理結果を保存
    dfs_analyze[i] = df_analyze
----

===== 1.銘柄毎の運用実績の分布をプロット

分析用データを表示して確認します。

[source,python]
----
dfs_analyze["reversal"].head(2)
----

image::simple/simple20.png[処理結果, width="80%"]

===== 2.銘柄毎のリターンの分布をヒストグラムでプロット

銘柄毎の各週のデータを確認します。銘柄毎のリターンの分布をヒストグラムでまずはプロットします。

[source,python]
----
# 描画領域を定義
fig, axes = plt.subplots(1, len(dfs_analyze), figsize=(8 * len(dfs_analyze), 8), sharex=True, sharey=True)

# 戦略毎に処理
for i, k in enumerate(dfs_analyze.keys()):
    # 描画位置を指定
    ax = axes[i]
    # ヒストグラムをプロット
    dfs_analyze[k].groupby(["date", "Local Code"])["return"].sum().hist(bins=50, log=True, ax=ax)
    # タイトルを設定
    ax.set_title(f"{k}: Weekly PL distribution")
# 描画
plt.show()
----

image::simple/simple21.png[処理結果, width=70%"]

　銘柄毎リターンの分布をプロットしてみましたが、ここから何か知見は得ることは難しそうです。上のプロットからもこれといった知見をえることはできません。あえて言うなら、reversalが若干trendと比較すると裾野が広い程度です。ただし、リターンを大きな勝ちに依存している戦略や、負けの裾野が非常に広い戦略などもこのプロットで観測できるため、リターンの分布は常に確認することをおすすめします。

===== 3.週毎に勝ち銘柄率を算出、統計量の算出

最後に週毎の銘柄の勝率を確認します。

[source,python]
----
# 描画領域を定義
fig, ax = plt.subplots(1, 1, figsize=(20, 8), sharex=True, sharey=True)

# 統計量表示用
buff = []
# 戦略毎に処理
for k in dfs_analyze.keys():
    # 週毎の勝ち銘柄率を計算
    win_ratio = (
        dfs_analyze[k]
        .set_index("date")
        .groupby("date")
        .apply(lambda x: (x.pl > 0).sum() / x.shape[0])
        .to_frame()
        .rename(columns={0: f"{k}: win_ratio"})
    )
    # プロット
    win_ratio.plot(ax=ax)
    # 統計量を保存
    buff.append(win_ratio.describe().T)
# ユニバースの勝ち銘柄率をプロット
df_wp.groupby(df_wp.index).apply(lambda x: (x.universe > 0).sum() / x.shape[0]).rename(
    "universe"
).to_frame().plot(ax=ax, color="black")
# タイトルを設定
ax.set_title("win ratio of selected stocks")
# グリッド表示
ax.grid(True)
# 0.5に基準線を描画
ax.axhline(y=0.5, color="red")
#  描画
plt.show()
# 週毎の勝ち銘柄率の統計量
display(pd.concat(buff))
----

image::simple/simple22.png[処理結果, width="90%"]

　このプロットは黒線のuniverseに対してどの時期に銘柄単位で勝率が低く、どの時期に勝率が高かったを確認しています。trendは暴落後にあまり収益を得ることができませんでしたが、revesalやunivereとの差異が観測できます。 +
もし、銘柄単位の勝率で大きな差異が発生していないのに、収益に差異が出ている場合はその時の銘柄の勝ち幅が大きい可能性があります。また、9月以降にもみ合いになってしまった時期はこの勝率でも50%近辺で揉み合っており、取引戦略の優位性が発揮できていない時期であることがわかります。

===== 考察

　ここまででいろいろな観点からreversalとtrendの評価を実施してきました。3月末以降のリターン・リバーサル戦略が有効に働いている時期もありましたし、トレンドフォロー戦略でベータ値を低く抑える可能性があることがわかりました。

　基本的にリターン・リバーサル戦略とトレンドフォロー戦略は安定して勝てる手法ではなく、リバーサル・モメンタムという代表的なファクターと密接な関係があり、マーケットの局面ごとに有効な戦略が変わっていることが知られています。ここまでの結果から、適切にリターン・リバーサル戦略とトレンドフォロー戦略をモデルで切り替えることが実現できれば、高い収益が実現できるポテンシャルがありそうです。そのような機械学習モデルの構築を検討する価値はあるでしょう。

　このように様々な可視化を通して、取引戦略を評価することで、取引戦略を発展させていくことができます。

=== 投稿用パッケージの作成

==== 投稿用パッケージのディレクトリの作成

　ここまで、モデルの作成及び評価をしてきました。ここからは、投稿用のパッケージを作成します。ランタイム環境用のモデルは以下の構成である必要がありますので、まずは必要なディレクトリを作成していきます。

[source]
----
.
├── model                  必須 学習済モデルを置くディレクトリ
│   └── ...
├── src                    必須 Python のプログラムを置くディレクトリ
│   ├── predictor.py       必須 最初にプログラムが呼び出すファイル
│   └── ...                その他のファイル (ディレクトリも作成可能)
└── requirements.txt       任意

----


[source,python]
----
# 作業用のディレクトリを設定
if 'google.colab' in sys.modules:
    working_dir = "/content/drive/MyDrive/JPX_competition/Chapter03"
else:
    working_dir = "."
# パッケージのrootディレクトリ
package_dir = f"{working_dir}/archive"
----


[source,python]
----
# 必要なディレクトリを作成します
os.makedirs(f"{package_dir}/model", exist_ok=True)
os.makedirs(f"{package_dir}/src", exist_ok=True)
# 今回はmodelディレクトリに保存するファイルがないため空ファイルを作成します
open(f"{package_dir}/model/dummy.txt", mode="a").close()
----

==== ランタイム実行用クラスの作成

　notebookの各セルで実行する内容をファイルに書き出すために、jupyter notebookにマジックコマンドを追加します。

[source,python]
----
# jupyter notebookにマジックコマンドを追加します
# セル実行と同時にセル内の記載内容をファイルに書き込みます
@register_cell_magic
def writerun(line, cell):
    # 書き込み先ファイルパスを取得
    file_path = line.split()[-1]
    # 親ディレクトリ名を取得
    p_dir = os.path.dirname(file_path)
    # 親ディレクトリが存在する場合は
    if p_dir != "":
        # ディレクトリ作成
        os.makedirs(p_dir, exist_ok=True)
    # cellの内容をファイルに書き込み
    with open(file_path, mode="w") as f:
        f.write(cell)
    # cellを実行
    get_ipython().run_cell(cell)
----

　以下のコードは、ここまでに一行ずつ作成したコードを投稿用の `ScoringService` としてまとめて実装したものになります。また、今回は学習済モデルのパラメーターなどをファイルに書き出していないため、`get_model` メソッドでは何もせずにTrueを返しています。そして、`predict` メソッドには上記で実行したコードをコピーして貼り付けています。表示用のコードおよび作業用データのコピーについてはランタイム環境では実行する必要がないため削除しています。

　jupyter notebookのセルに `ScoringService` クラスを作成している理由は、`ScoringService` クラスの出力するポートフォリオがこれまで検証してきたポートフォリオと同一であることの検証が容易であるためです。慣れている方は直接`predictor.py` ファイル上で作業することを好まれるかもしれません。

　次のセルでは、セルの先頭で `writerun` マジックコマンドを指定することで、実行時にセルの内容が `$package_dir/src/predictor.py` ファイルに書き込まれます。すでにファイルが存在している場合は上書きされるためご注意ください。

[source,python]
----
%%writerun $package_dir/src/predictor.py
include::../../../../handson/Chapter03/archive/src/predictor.py[]
----

ファイルが書き込まれていることを確認します

[source]
----
! ls -l $package_dir/src/
----

出力

[source]
----
total 3
-rw-r--r-- 1 root root 4450 Feb 27 09:40 predictor.py
----

　作成したクラスが適切に動くかを動作確認します。 上記で1行ずつ実行した場合と同一の出力であれば良いことになります。

[source,python]
----
# 動作確認します
str_ret = ScoringService.predict(inputs, start_dt=pd.Timestamp("2020-01-01"))
----

[source,python]
----
# 出力を確認
print("\n".join(str_ret.split("\n")[:10]))
----

　出力させます。

[source]
----
date,Local Code,budget
2020-01-06,4592,50000
2020-01-06,3254,50000
2020-01-06,6875,50000
2020-01-06,4571,50000
2020-01-06,7956,50000
2020-01-06,6049,50000
2020-01-06,7744,50000
2020-01-06,2395,50000
2020-01-06,3660,50000
----

[source,python]
----
# 出力を保存
with open("chapter03-tutorial-01-class.csv", mode="w") as f:
    f.write(str_ret)
----

　出力が一致していることを確認します。

[source,python]
----
assert outputs["reversal"] == str_ret
----


==== 提出用パッケージの作成と提出

　3.7.10で作成した `ScoringService` を `predictor.py` に書き込み、Zip形式で圧縮します。今回はシンプルな特徴量を使用してポートフォリオを作成しているため、学習済みのモデルファイルは存在しません。ただし、modelディレクトリは必須であるため、zipファイルには該当のディレクトリを含める必要があります。そのためには、modelファイルには何らかのダミーファイルを作成してzipファイルを作成すると良いでしょう。

以下、Zipファイル作成例になります。

[source,bash]
----
$ mkdir model src
$ touch model/dummy.txt
$ ls
model src
$ zip -v submit.zip src/predictor.py model/dummy.txt
----


[source,python]
----
# 提出用パッケージ名
package_file = "chapter03-model.zip"
# パッケージファイルパス
package_path = f"{working_dir}/{package_file}"

# zipファイルを作成
with zipfile.ZipFile(package_path, "w") as f:
    # model/dummy.txtを追加
    print(f"[+] add {package_dir}/model/dummy.txt to model/dummy.txt")
    f.write(f"{package_dir}/model/dummy.txt", "model/dummy.txt")
    # src/predictor.py を追加
    print(f"[+] add {package_dir}/src/predictor.py to src/predictor.py")
    f.write(f"{package_dir}/src/predictor.py", "src/predictor.py")

print(f"[+] please check {package_path}")
----

　以上で投稿用のモデルパッケージは完成です。`chapter03-model.zip` ファイルをコンペティションページから投稿してリーダーボードに掲載されることを確認しましょう。

　リーダーボードに掲載されたことを確認したら、例えば、特徴量を変更してみたり、複数の特徴量それぞれから10銘柄ずつ選択するロジックを実装してみるのはどうでしょうか。いくつかの特徴量については「<<anchor-2.7,2.7. 特徴量の生成>>」で説明していますのでご参照ください。特に、「2.7.2. 定常性を意識した特徴量設計」は重要な概念ですのでご一読ください。

次章では、2章で作成した機械学習モデルを使用してポートフォリオを組成する方法について記載しています。
